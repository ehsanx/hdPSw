[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hdPS and its machine learning extensions in residual confounding control",
    "section": "",
    "text": "The use of retrospective health care claims datasets is frequently criticized for lacking complete information on potential confounders. Ultimately, the treatment effects estimated utilizing such data sources may be subject to residual confounding. Digital electronic administrative records routinely collect a large volume of health-related information; and many of whom are usually not considered in conventional pharmacoepidemiological studies."
  },
  {
    "objectID": "index.html#proposal-to-reduce-residual-confounding-bias",
    "href": "index.html#proposal-to-reduce-residual-confounding-bias",
    "title": "hdPS and its machine learning extensions in residual confounding control",
    "section": "Proposal to reduce residual confounding bias",
    "text": "Proposal to reduce residual confounding bias\nIn 2009, a high-dimensional propensity score (hdPS) algorithm was proposed that utilizes such information as surrogates or proxies for mismeasured and unobserved confounders in an effort to reduce residual confounding bias. Since then, many machine learning and semi-parametric extensions of this algorithm have been proposed to exploit the wealth of high-dimensional proxy information properly.\n\n\nSchneeweiss et al. (2009)"
  },
  {
    "objectID": "index.html#purpose-of-the-workshop",
    "href": "index.html#purpose-of-the-workshop",
    "title": "hdPS and its machine learning extensions in residual confounding control",
    "section": "Purpose of the workshop",
    "text": "Purpose of the workshop\nThis workshop will\n\ndemonstrate logic, steps and implementation guidelines of hdPS utilizing an open data source as an example (using reproducible R codes),\nfamiliarize participants with the difference between propensity score vs. hdPS,\nexplain the rationale for using the machine learning extensions of hdPS, and their statistical properties, and\ndiscuss advantages, controversies, and hdPS reporting guidelines while writing a manuscript."
  },
  {
    "objectID": "index.html#workshop-prerequisite",
    "href": "index.html#workshop-prerequisite",
    "title": "hdPS and its machine learning extensions in residual confounding control",
    "section": "Workshop prerequisite",
    "text": "Workshop prerequisite\nAttendees should have prerequisite knowledge of multiple regression analysis and working knowledge in R (e.g., basic data manipulation and regression fitting).\n\nR Codes\nR Codes for data creation and hdPS analysis can be found on the GitHub repo (codes directory).\n\n\nVersion history\nDifferent versions and updates of the materials were presented in the following sessions\n\nR/Medicine Conference 2023, Virtual, June 5, 2023\n2023 Society of Epidemiologic Research Workshops, Virtual, May 4, 2023\n\nAdditional relevant talks (selected):\n\nStatistical issues in administrative data, Banff International Research Station, Banff, Feb 2019.\nStatistics Conference in Genomics, Pharmaceutical Science, and Health Data Science, August 15-17, 2022 University of Victoria, Victoria, BC\nWork in Progress Seminar, CHEOS, St. Paul’s Hospital (Hurlburt Auditorium), Dec 14th, 2022.\nStatistics and Biostatistics seminar series, at the Department of Statistics and Actuarial Science, University of Waterloo, April 26, 2023.\nConference on Statistics and Data Science with Applications in Biology, Genetics, Public Health, and Finance, Thompson Rivers University, Kamloops, August 21-24, 2023 (upcoming/scheduled).\n\n\n\nCitation\n\n\n\n\n\n\nHow to cite\n\n\n\nKarim, ME. (2023). High-dimensional propensity score and its machine learning extensions in residual confounding control in pharmacoepidemiologic studies. Zenodo. DOI: 10.5281/zenodo.7894083.\n\n\n\n\nComments\nFor any comments regarding this document, reach out to me.\n\n\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn, Helen Mogun, and M Alan Brookhart. 2009. “High-Dimensional Propensity Score Adjustment in Studies of Treatment Effects Using Health Care Claims Data.” Epidemiology (Cambridge, Mass.) 20 (4): 512."
  },
  {
    "objectID": "motivating.html#literature",
    "href": "motivating.html#literature",
    "title": "Motivating example",
    "section": "Literature",
    "text": "Literature\nType 2 diabetes is a metabolic disorder that is characterized by high blood sugar levels and insulin resistance. There is a growing body of evidence that, for type 2 diabetes, obesity is a well-established risk factor. Possible mechanism includes excess body fat leading to insulin resistance, while impairing the body’s ability to regulate blood sugar levels.\n\n\n\n\n\n\n\n\n\n(Klein et al. 2022)"
  },
  {
    "objectID": "motivating.html#research-question",
    "href": "motivating.html#research-question",
    "title": "Motivating example",
    "section": "Research question",
    "text": "Research question\n“Does obesity increase the risk of developing diabetes?”\n\n\nObesity is often considered a challenging exposure variable to define precisely in research studies (Hernán and Taubman 2008). In this case, we are using it as an illustrative example to explain the methods and not attempting to make any clinical statements about this topic.\n\n\n\n\n\nflowchart LR\n  A[Obesity] --> Y(Diabetes)\n\n\n\n\n\n\n\n\n\n\n\nExposure: Being obese\n\nOutcome: Developing diabetes\n\n\n\n\n\n\n\nTip\n\n\n\nThe primary goal of the research is not to answer a clinical question or to draw conclusions about the relationship between obesity and diabetes in the general population, but rather to use the relationship as a motivating example for conducting simulations that compares different statistical methods.\n\n\n\n\n\n\nHernán, Miguel A, and Sarah L Taubman. 2008. “Does Obesity Shorten Life? The Importance of Well-Defined Interventions to Answer Causal Questions.” International Journal of Obesity 32 (3): S8–14.\n\n\nKlein, Samuel, Amalia Gastaldelli, Hannele Yki-Järvinen, and Philipp E Scherer. 2022. “Why Does Obesity Cause Diabetes?” Cell Metabolism 34 (1): 11–20."
  },
  {
    "objectID": "data.html#choose-a-u.s.-data-source",
    "href": "data.html#choose-a-u.s.-data-source",
    "title": "1  Data to Analyze",
    "section": "1.1 Choose a U.S. data source",
    "text": "1.1 Choose a U.S. data source\n\n\n\n\n\n\n\n\nData source: National Health and Nutrition Examination Survey (NHANES) (Disease Control and Prevention 2021)\n\n2013-2014,\n2015-2016,\n2017-2018\n\nAvailability: NHANES is a publicly available dataset that can be downloaded for free from the CDC website.\nDesign: Observational cross-sectional data. Hence, inferring causality is not a possibility or our objective here."
  },
  {
    "objectID": "data.html#confounder-identification",
    "href": "data.html#confounder-identification",
    "title": "1  Data to Analyze",
    "section": "1.2 Confounder identification",
    "text": "1.2 Confounder identification\nDirected acyclic graph (DAG)\n\n\n(Greenland, Pearl, and Robins 1999)\n\n\n\n\n\nflowchart TB\n  A[Obesity A] --> Y(Diabetes Y)\n  L[Confounders C] --> Y\n  L --> A\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesized Directed acyclic graph drawn based on analyst’s best understanding of the literature\n\n\n\n\n\n\nExposure: Being obese\n\nOutcome: Developing diabetes\n\nConfounders: Demographic and lab variables"
  },
  {
    "objectID": "data.html#structure-of-the-data",
    "href": "data.html#structure-of-the-data",
    "title": "1  Data to Analyze",
    "section": "1.3 Structure of the data",
    "text": "1.3 Structure of the data\n\n\n\n\n\nflowchart LR\n  D[NHANES 2013-14] --> demo[Demographic \\nVariables \\nand \\nSample \\nWeights]\n  demo --> Age\n  demo --> Sex\n  demo --> Education\n  demo --> r[Race or \\nethnicity]\n  demo --> m[Marital \\nstatus]\n  demo --> Income\n  demo --> b[Birth place]\n  demo --> sf[Survey \\nfeatures: \\nsampling \\nweights, \\nstrata, \\ncluster]\n  D --> bmi[Body \\nMeasures]\n  bmi --> Obesity\n  D --> diq[Diabetes]\n  diq --> Diabetes\n  diq --> f[Family \\nhistory of \\ndiabetes]\n  D --> smq[Smoking - \\nCigarette Use]\n  smq --> Smoking\n  D --> dbq[Diet \\nBehavior & \\nNutrition]\n  dbq --> Diet\n  D --> paq[Physical \\nActivity]\n  paq --> p[Physical \\nactivities]\n  D --> huq[Hospital \\nUtilization & \\nAccess \\nto Care]\n  huq --> mm[Medical \\naccess]\n  D --> bpx[Blood \\nPressure]\n  bpx --> sbp[Systolic \\nBlood \\nPressure]\n  bpx --> dbp[Diastolic \\nBlood \\nPressure]\n  D --> bpq[Blood \\nPressure & \\nCholesterol]\n  bpq --> hc[High \\ncholesterol]\n  D --> slq[Sleep \\nDisorders]\n  slq --> Sleep\n  D --> biopro[Standard\\n Biochemistry \\nProfile]\n  biopro --> u[Uric \\nacid]\n  biopro --> Protein\n  biopro --> Bilirubin\n  biopro --> Phosphorus\n  biopro --> Sodium\n  biopro --> Potassium\n  biopro --> Globulin\n  biopro --> Calcium\n  D --> rxq[Prescription\\n Medications -  \\nICD-10-CM \\ncodes]\n  style D fill:#FFA500;\n  style rxq fill:#00FF00;\n  style biopro fill:#00FF00;\n  style slq fill:#00FF00;\n  style bpq fill:#00FF00;\n  style bpx fill:#00FF00;\n  style huq fill:#00FF00;\n  style paq fill:#00FF00;\n  style dbq fill:#00FF00;\n  style smq fill:#00FF00;\n  style diq fill:#00FF00;\n  style bmi fill:#00FF00;\n  style demo fill:#00FF00;\n\n\n\n\n\n\n\n\n\n\n\nWe do the same for the following cycles:\n\nNHANES 2015-16\nNHANES 2017-18"
  },
  {
    "objectID": "data.html#identify-measured-and-unmeasured-variables-in-the-data",
    "href": "data.html#identify-measured-and-unmeasured-variables-in-the-data",
    "title": "1  Data to Analyze",
    "section": "1.4 Identify measured and unmeasured variables in the data",
    "text": "1.4 Identify measured and unmeasured variables in the data\nFind variables capturing the following concepts in the data based on a hypothesized DAG.\n\n\n\n\n\nRole\nData Component\nVariables considered based on DAG\n\n\n\n\nOutcome\nDIQ\nHave diabetes1\n\n\nExposure\nBMX\nObese; BMI >= 30\n\n\nConfounder\n(demographic) DEMO\nAge, Sex, Education, Race/ethnicity, Marital status, Annual household income, County of birth, Survey cycle year\n\n\n\n(behaviour) SMQ, PAQ, SLQ, DBQ\nSmoking2, Vigorous work activity, Sleep3, Diet4\n\n\n\n(health history / access) DIQ, HUQ\nDiabetes family history, Access to care5\n\n\n\n(lab) BPX, BPQ, BIOPRO\nBlood pressure (systolic, diastolic6), Cholesterol, Uric acid, Total Protein, Total Bilirubin, Phosphorus, Sodium, Potassium, Globulin, Total Calcium\n\n\n\n\n\n\n\n\n14 demographic, behavioral, health history related variables\n\nMostly categorical\n\n11 lab variables\n\nMostly continuous\n\n\n\n\nDisease Control, Centers for, and Prevention. 2021. “National Health and Nutrition Examination Survey (NHANES).” National Center for Health Statistics.\n\n\nGreenland, Sander, Judea Pearl, and James M Robins. 1999. “Causal Diagrams for Epidemiologic Research.” Epidemiology, 37–48."
  },
  {
    "objectID": "proxy.html#measuring-comorbidity-burden",
    "href": "proxy.html#measuring-comorbidity-burden",
    "title": "2  Reducing residual confounding",
    "section": "2.1 Measuring comorbidity burden",
    "text": "2.1 Measuring comorbidity burden\nIn health research, the overall health status/ Disease burden could be a potential confounding factor. In the original DAG, we had comorbidity as a known confounder.\n\n\n\n\n\nflowchart TB\n  A[Obesity] --> Y(Diabete)\n  L[Comorbidity measure unobserved] --> Y\n  L --> A\n  style A fill:#90EE90;\n  style Y fill:#ADD8E6;\n  style L fill:#FF0000;\n\n\n\n\n\n\n\n\n\n\nCharlson Comorbidity Index (CCI) is a measure that quantifies the burden of comorbidities or pre-existing medical conditions in patients (takes into account 17 comorbidities), which can impact their health outcomes and overall survival.\nElixhauser Comorbidity Index (ECI) is a measure of the burden of comorbidities, based on 30 different comorbid conditions.\nChronic Disease Score (CDS) is a weighted score of the number and severity of chronic diseases, calculated using self-reported data on diagnosed conditions (considers the presence of 21 chronic conditions).\n\n\n\n\n(Charlson et al. 1987; Elixhauser et al. 1998; Von Korff, Wagner, and Saunders 1992)\nNHANES does not include information on all of the comorbidities included in theses scores / indices.\n\n\n\n\n\n\n\nResidual confounding\n\n\n\nComorbidity scores are widely used as a measure of comorbidity burden, and their calculation often relies on data that may not be available in certain contexts, such as in NHANES or Canadian health administrative databases. In such cases, when comorbidity burden is a known confounder, researchers may use proxy information to approximate and mimic the information. Not being able to adjust for such variable can introduce bias and residual confounding in the treatment effect estimation.\n\n\n\n\n\n(Schneeweiss and Maclure 2000; L. Lix et al. 2011; L. M. Lix et al. 2013)"
  },
  {
    "objectID": "proxy.html#proxy-adjustment-empirical-criterion",
    "href": "proxy.html#proxy-adjustment-empirical-criterion",
    "title": "2  Reducing residual confounding",
    "section": "2.2 Proxy Adjustment Empirical criterion",
    "text": "2.2 Proxy Adjustment Empirical criterion\nEmpirical criterion: Modified disjunctive cause criterion\nVanderWeele et al. 2019 European Journal of Epidemiology: CC BY license\n\n\n\n\n\nHypothesized Directed acyclic graph with comorbidity measure being unmeasured, and approximated by the simple count measures based on the ICD codes\n\n\n\n\n\n\nAdjust for variables that are (a) causes of exposure or outcome or both, (b) discard: known instrument, (c) including good proxies for unmeasured common causes (VanderWeele 2019)"
  },
  {
    "objectID": "proxy.html#additional-information-icd-10-cm",
    "href": "proxy.html#additional-information-icd-10-cm",
    "title": "2  Reducing residual confounding",
    "section": "2.3 Additional information: ICD-10-CM",
    "text": "2.3 Additional information: ICD-10-CM\n\n\nThe International Classification of Diseases 10th Revision (ICD-10) is a standardized system of codes for the classification of diseases, disorders, and injuries.\n\n\n\nRole\nData Source\nVariables considered\n\n\n\n\nRole unclear as they may not directly relate to the research question\nRXQ_RX\nPrescription medication ICD-10-CM code\n\n\n\n\n\nRXQ_RX questionnaire (a) collects information on prescription medications taken in the past 30 days, (b) conducted by trained interviewers, and (c) with some quality control efforts.\n\n\n\n\n\nExamples of ICD-10-CM codes (3-7 characters, 1st character being alpha, 2-end are numberic, often with a dot) assigned to reasons for using medication (see Appendix in NHANES RXQ_RX component)\n\n\n\n\n\n\nWe have a lot of information through these ICD-10-CM codes, but for most of these information, it is unclear what role they play within the context of our research questions.\nCount of prescriptions is often used to measure comorbidity burden. This is not a perfect measure. But could serve as a proxy for our purpose.\n\n\n\n\nPrescription medication (ICD-10-CM codes from all 3 cycles) data was liked with the initial data.\n\n\nCharlson, Mary E, Peter Pompei, Kathy L Ales, and C Ronald MacKenzie. 1987. “A New Method of Classifying Prognostic Comorbidity in Longitudinal Studies: Development and Validation.” Journal of Chronic Diseases 40 (5): 373–83.\n\n\nElixhauser, Anne, Claudia Steiner, D Robert Harris, and Rosanna M Coffey. 1998. “Comorbidity Measures for Use with Administrative Data.” Medical Care, 8–27.\n\n\nLix, Lisa M, Jacqueline Quail, Opeyemi Fadahunsi, and Gary F Teare. 2013. “Predictive Performance of Comorbidity Measures in Administrative Databases for Diabetes Cohorts.” BMC Health Services Research 13: 1–12.\n\n\nLix, LM, J Quail, G Teare, and B Acan. 2011. “Performance of Comorbidity Measures for Predicting Outcomes in Population-Based Osteoporosis Cohorts.” Osteoporosis International 22: 2633–43.\n\n\nSchneeweiss, Sebastian, and Malcolm Maclure. 2000. “Use of Comorbidity Scores for Control of Confounding in Studies Using Administrative Databases.” International Journal of Epidemiology 29 (5): 891–98.\n\n\nVanderWeele, Tyler J. 2019. “Principles of Confounder Selection.” European Journal of Epidemiology 34: 211–19.\n\n\nVon Korff, Michael, Edward H Wagner, and Kathleen Saunders. 1992. “A Chronic Disease Score from Automated Pharmacy Data.” Journal of Clinical Epidemiology 45 (2): 197–203."
  },
  {
    "objectID": "hdps.html#origin",
    "href": "hdps.html#origin",
    "title": "High-dimensional Propensity score",
    "section": "Origin",
    "text": "Origin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Schneeweiss et al. 2009)"
  },
  {
    "objectID": "hdps.html#key-idea",
    "href": "hdps.html#key-idea",
    "title": "High-dimensional Propensity score",
    "section": "Key idea",
    "text": "Key idea\nSchneeweiss et al. 2009 extended to a variety of classifications to code diagnoses (ICD), procedure (CPT), medications (eg, NDC, AHFS, ATCC), or others (PCP, LOINC).\n\n\n\n\n\n\n\n\n\n\n\nCPT-4 (Current Procedural Terminology, 4th edition), ICD-9 (International Classification of Diseases, 9th edition), PCP visits (Primary Care Physician visits), NDC (National Drug Code), and ATC (Anatomical Therapeutic Chemical classification) are all codes or measures commonly used in healthcare and medical research.\nSchneeweiss et al. 2018 Clinical Epidemiology: CC BY license\n\n\n(Schneeweiss 2018)\n\n\n\n\n\n\nAdjust useful proxies\n\n\n\nIn administrative data sources, the main idea of hdPS (high-dimensional propensity score) is to adjust for proxies that are empirically associated with the outcome of interest, which may not be directly measured in the data.\n\n\n\n\nWith hdPS, users do not need to know which unmeasured confounders are being adjusted for by proxy information.\n\nAdjusting for something that may not be interpretable directly with the context of the research question.\nLogic: measures from same subject should be correlated = has relevant proxy information\n\n\n\n\n\nSchneeweiss, Sebastian. 2018. “Automated Data-Adaptive Analytics for Electronic Healthcare Data to Study Causal Treatment Effects.” Clinical Epidemiology, 771–88.\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn, Helen Mogun, and M Alan Brookhart. 2009. “High-Dimensional Propensity Score Adjustment in Studies of Treatment Effects Using Health Care Claims Data.” Epidemiology (Cambridge, Mass.) 20 (4): 512."
  },
  {
    "objectID": "step0.html#creating-analytic-data",
    "href": "step0.html#creating-analytic-data",
    "title": "3  Step 0: Analytic data",
    "section": "3.1 Creating Analytic data",
    "text": "3.1 Creating Analytic data\n3 cycles of NHANES datasets were - downloaded from the US CDC website - recoded for consistency, and - merged together to make an analytic data.\nDetails of data download process, and recoding and merging are discussed in Appendix.\n\n\n\n\nflowchart LR\n  A[NHANES] --> C1(2013-2014 cycle) --> ss1(10,175 \\nparticipants)\n  A --> C2(2015-2016 cycle) --> ss2(9,971 \\nparticipants)\n  A --> C3(2017-2018 cycle) --> ss3(9,254 \\nparticipants)\n  ss1 --> ss(7,585 \\nafter \\nimposing \\neligibility \\ncriteria)\n  ss2 --> ss\n  ss3 --> ss\n  style A fill:#FFA500;\n  style C1 fill:#FFA500;\n  style C2 fill:#FFA500;\n  style C3 fill:#FFA500;\n  style ss1 fill:#FFA500;\n  style ss2 fill:#FFA500;\n  style ss3 fill:#FFA500;\n  style ss fill:#FFA500;\n\n\n\n\n\n\n\n\n\n\nOur study population was restricted to the U.S. population who were\n\n20 years or older and\nnot pregnant at the time of survey data collection, and\nwho had available International Classification of Diseases (ICD) codes to ensure we can extract sufficient proxy information for the analysis (discussed in step 1).\n\nTo simplify the analysis, we only considered complete case data."
  },
  {
    "objectID": "step0.html#ps-model-no-proxies",
    "href": "step0.html#ps-model-no-proxies",
    "title": "3  Step 0: Analytic data",
    "section": "3.2 PS model (no proxies)",
    "text": "3.2 PS model (no proxies)\nWe build the propensity score model in this data using the investigator-specified covariates.\n\n\n\n\n\n\n\n\n\n\n\nC = investigator-specified covariates.\nThen the PS can be used as matching, weighting, stratifying variables, or as covariates (usually in deciles) in outcome model.\n\n\nIf you are somewhat unfamiliar with propensity score paradigm, look at tutorials dedicated towards that topic. There are additional tutorials also talking about propensity score weighting.\nIn our current context, we only talk about inverse probability weighting.\n\n\n\n\n\n\nTip\n\n\n\nResults from propensity score analysis (through inverse probability weighting) with only investigator-specified covariates are shown in Propensity Score section."
  },
  {
    "objectID": "step1.html#data-with-investigator-specified-variables",
    "href": "step1.html#data-with-investigator-specified-variables",
    "title": "4  Step 1: Proxy sources",
    "section": "4.1 Data with investigator-specified variables",
    "text": "4.1 Data with investigator-specified variables\n\n\n\n\n\n\nData: part 1\n\n\n\nWe will work with the data.complete data for the investigator-specified information.\n\n\n\nanalytic <- data.complete\nidx <- analytic$id\noutcome <- as.numeric(analytic$diabetes == \"Yes\") \nexposure <- as.numeric(analytic$obese == \"Yes\")\ndomain <- \"dx\"\nanalytic.dfx <- as.data.frame(cbind(idx, exposure, outcome, domain))\n\n\n\nWe prepare the minimal analytic data only with the following 4 information:\n\nidentifying information (idx)\nexposure (obese)\noutcome (diabetes)\ndomain of the codes (dx). In this example we only have prescription domain (1 domain dx)"
  },
  {
    "objectID": "step1.html#proxy-data",
    "href": "step1.html#proxy-data",
    "title": "4  Step 1: Proxy sources",
    "section": "4.2 Proxy data",
    "text": "4.2 Proxy data\n\n4.2.1 Identify the data dimensions (proxy sources)\nIn this example we only have prescription domain (1 domain dx of ICD-10-CM code). Hence \\(p = 1\\) in this exercise.\n\n\nNHANES Questionnaire collects information on: (a) dietary supplements, (b) nonprescription antacids, (c) prescription medications, and (d) preventive aspirin use.\n\n\n4.2.2 Define a covariate assessment period (CAP)\n\n\n\n\n\n\n\n\n\n\n\n(Connolly et al. 2019; Schneeweiss et al. 2009)\nWe only collect proxy information from a well-defined CAP. In our case, it was \\(30\\) days.\n\n\nNHANES asked “In the past 30 days, have you used or taken medication for which a prescription is needed? Do not include prescription vitamins or minerals you may have already told me about.”\n\n\n\n\n\n\nData: part 2\n\n\n\nWe will work with the merge proxy data (ICD-10 codes) from 3 cycles: dat.proxy.long.\n\n\n\n\n4.2.3 Omit duplicated information\n\n\nWe need to delete codes that could be close proxies of exposure and/or outcome, or other investigator specified covariates we have already selected in step0.\n\n\n\n\n\n\n\n\n\n\ndat.proxy.long <- subset(dat.proxy.long, \n                         icd10 != \"E66\") # Overweight and obesity\ndat.proxy.long <- subset(dat.proxy.long, \n                         icd10 != \"O24\") # Gestational diabetes mellitus\ndat.proxy.long <- subset(dat.proxy.long, \n                         icd10 != \"E10\") # Type 1 diabetes mellitus\ndat.proxy.long <- subset(dat.proxy.long, \n                         icd10 != \"E11\") # Type 2 diabetes mellitus\n\n\n\n\nWe delete codes associated with exposure and outcome.\nSame should be done for any other proxies that may have duplicating information compared to the investigator-specified covariates.\n\n\n\n4.2.4 Long format proxy data\n\n\n\n\n\nHere is an example of 3 digit codes for 1 patient with subject ID “100001”. We create the same for all patients.\n\n\n\n\n \n  \n    ID \n    ICD 10 codes (3 digit) \n    Description \n  \n \n\n  \n    100001 \n    F33 \n    Major depressive disorder, recurrent \n  \n  \n    100001 \n    I10 \n    Hypertension \n  \n  \n    100001 \n    M62 \n    Muscle spasm \n  \n  \n    100001 \n    F32 \n    Major depressive disorder, single episode \n  \n  \n    100001 \n    M25 \n    Joint disorder/pain \n  \n  \n    100001 \n    K21 \n    Gastro-esophageal reflux disease \n  \n  \n    100001 \n    M79 \n    musculoskeletal pain conditions \n  \n  \n    100001 \n    R12 \n    Heartburn"
  },
  {
    "objectID": "step1.html#merge-proxy-data-with-analytic-data",
    "href": "step1.html#merge-proxy-data-with-analytic-data",
    "title": "4  Step 1: Proxy sources",
    "section": "4.3 Merge Proxy data with Analytic data",
    "text": "4.3 Merge Proxy data with Analytic data\n\n\n\n\n\n\nMerged Data: parts 1 and 2\n\n\n\n\nWe will work with the merge proxy data with analytic data.\nThat will provide us with the IDs (idx) of the subject that have proxy (ICD-10) information associated with them.\n\n\n\n\nrequire(dplyr) \ndfx <- merge(analytic.dfx, proxy.var.long, by = \"idx\")\nhead(dfx)\n\n\n\n  \n\n\nbasetable <- dfx %>% select(idx, exposure, outcome) %>% distinct()\npatientIds <- basetable$idx\nlength(patientIds)\n#> [1] 7585\n\n\n\n\n\n\n\n\nConnolly, John G, Sebastian Schneeweiss, Robert J Glynn, and Joshua J Gagne. 2019. “Quantifying Bias Reduction with Fixed-Duration Versus All-Available Covariate Assessment Periods.” Pharmacoepidemiology and Drug Safety 28 (5): 665–70.\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn, Helen Mogun, and M Alan Brookhart. 2009. “High-Dimensional Propensity Score Adjustment in Studies of Treatment Effects Using Health Care Claims Data.” Epidemiology (Cambridge, Mass.) 20 (4): 512."
  },
  {
    "objectID": "step2.html#sort-by-prevalence",
    "href": "step2.html#sort-by-prevalence",
    "title": "5  Step 2: Empirical",
    "section": "5.1 Sort by prevalence",
    "text": "5.1 Sort by prevalence\nCheck out the frequency of each codes:\n\nlibrary(dplyr)\ndf <- data.frame(\n  icd10 = names(sort(table(dfx$icd10), decreasing = TRUE)),\n  count = sort(table(dfx$icd10), decreasing = TRUE)\n)\n\n\n\n\n\nICD10 Code Frequencies\n \n  \n    ICD10 Code \n    Count \n  \n \n\n  \n    I10 \n    5742 \n  \n  \n    E78 \n    2965 \n  \n  \n    F32 \n    1135 \n  \n  \n    F41 \n    1090 \n  \n  \n    K21 \n    911 \n  \n  \n    M79 \n    870 \n  \n  \n    E03 \n    807 \n  \n  \n    M54 \n    772 \n  \n  \n    G47 \n    697 \n  \n  \n    J45 \n    626 \n  \n\n\n\n\n\n\n\nOnly top 10 prevalent codes are shown.\nHowever, some may be associated with lower counts (e.g., less than 20).\n\n\n\n\n\n\nRestrictions\n\n\n\nCandidate empirical covariates list is constrained by\n\ntheir prevalence of codes. Only top n covariates with highest prevalence would be chosen.\nanalysts absolutely need to get rid of the codes that have zero variance (e.g., everyone has the code, or nobody has it).\ncodes associated with very low prevalence are also numerically problematic for further analyses.\n\n\n\n\n\nWe choose n = 200 [for (1)] as it was proposed in the original algorithm (Schneeweiss et al. 2009). In reality, this is not necessary to be so restrictive (Schuster, Pang, and Platt 2015). Parts (2) and (3) are more likely and addressed by the following restriction: At least min_num_patients number of patients need to have that code to be selected in the list.\nIf there were more dimensions, separate list of candidate empirical covariates would be identified."
  },
  {
    "objectID": "step2.html#choose-granularity",
    "href": "step2.html#choose-granularity",
    "title": "5  Step 2: Empirical",
    "section": "5.2 Choose Granularity",
    "text": "5.2 Choose Granularity\nOne important point here is that we have chosen granularity to be 3 digits in the ICD-10 code.\n\n\nWe have already truncated the codes at 3 digit level while preparing the data."
  },
  {
    "objectID": "step2.html#retain-top-n-empirical-covariates",
    "href": "step2.html#retain-top-n-empirical-covariates",
    "title": "5  Step 2: Empirical",
    "section": "5.3 Retain top n empirical covariates",
    "text": "5.3 Retain top n empirical covariates\n\nrequire(autoCovariateSelection)\nstep1 <- get_candidate_covariates(df = dfx,  \n                                  domainVarname = \"domain\",\n                                  eventCodeVarname = \"icd10\", \n                                  patientIdVarname = \"idx\",\n                                  patientIdVector = patientIds,\n                                  n = 200, \n                                  min_num_patients = 20)\n\n\n\nYou can use autoCovariateSelection package to implement these restrictions (Robert 2020).\n\n5.3.1 Long format data\n\nout1 <- step1$covars_data\nhead(out1)\n\n\n\n  \n\n\n\n\n\n5.3.2 Updated frequency data\n\ndf2 <- data.frame(\n  icd10 = names(table(out1$icd10)),\n  count = as.numeric(table(out1$icd10))\n)\n\n\n\n\n\n \n  \n    ICD10 Code \n    Count \n  \n \n\n  \n    dx_A49 \n    44 \n  \n  \n    dx_B00 \n    35 \n  \n  \n    dx_B20 \n    44 \n  \n  \n    dx_B35 \n    41 \n  \n  \n    dx_B37 \n    20 \n  \n  \n    dx_B96 \n    30 \n  \n\n\n\n\n\n\n\nOnly first few code frequencies are shown (alphabetic order), that were selected based on the restrictions n = 200 and min_num_patients = 20.\n\n\n\n\n \n  \n      \n    ICD10 Code \n    Count \n  \n \n\n  \n    121 \n    dx_R73 \n    431 \n  \n  \n    122 \n    dx_T14 \n    167 \n  \n  \n    123 \n    dx_T78 \n    201 \n  \n  \n    124 \n    dx_T88 \n    23 \n  \n  \n    125 \n    dx_Z79 \n    489 \n  \n  \n    126 \n    dx_Z95 \n    26 \n  \n\n\n\n\n\n\n\nOnly last few code frequencies are shown (alphabetic order).\n\n\n5.3.3 Total number of codes retained\n\nnrow(df2)\n#> [1] 126\n\n\n\n\n\n\n\n\nRobert, Dennis. 2020. autoCovariateSelection: Automatic Covariate Selection. https://CRAN.R-project.org/package=autoCovariateSelection.\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn, Helen Mogun, and M Alan Brookhart. 2009. “High-Dimensional Propensity Score Adjustment in Studies of Treatment Effects Using Health Care Claims Data.” Epidemiology (Cambridge, Mass.) 20 (4): 512.\n\n\nSchuster, Tibor, Menglan Pang, and Robert W Platt. 2015. “On the Role of Marginal Confounder Prevalence–Implications for the High-Dimensional Propensity Score Algorithm.” Pharmacoepidemiology and Drug Safety 24 (9): 1004–7."
  },
  {
    "objectID": "step3.html#genrate-recurrence-covariates",
    "href": "step3.html#genrate-recurrence-covariates",
    "title": "6  Step 3: Recurrence",
    "section": "6.1 Genrate recurrence covariates",
    "text": "6.1 Genrate recurrence covariates\n\n\n\n\n\n(Schneeweiss et al. 2009)\nIn this step, we generate 3 binary recurrence covariates for each of the candidate empirical covariates identified in the previous step:\n\noccurred at least once\noccurred sporadically (at least more than the median)\noccurred frequently (at least more than the 75th percentile)\n\n\nstep2 <- get_recurrence_covariates(df = out1, \n                                   patientIdVarname = \"idx\",\n                                   eventCodeVarname = \"icd10\", \n                                   patientIdVector = patientIds)"
  },
  {
    "objectID": "step3.html#example-of-recurrence-covariates",
    "href": "step3.html#example-of-recurrence-covariates",
    "title": "6  Step 3: Recurrence",
    "section": "6.2 Example of recurrence covariates",
    "text": "6.2 Example of recurrence covariates\n\n\n\n\n\n\n\n\n\n\n\nICD-10-CM code (dimension 1)\ncode appeared at least once\ncode appeared at least more than the median\ncode appeared at least more than the 75th percentile\n\n\n\n\nD64.9 Anemia\nrec_dx_D64_once\nrec_dx_D64_sporadic\nrec_dx_D64_frequent\n\n\nD75.9P Blood clots\nrec_dx_D75_once\nrec_dx_D75_sporadic\nrec_dx_D75_frequent\n\n\nD89.9 Immune disorder\nrec_dx_D89_once\nrec_dx_D89_sporadic\nrec_dx_D89_frequent\n\n\n\\(\\ldots\\)\n\\(\\ldots\\)\n\\(\\ldots\\)\n\\(\\ldots\\)\n\n\nE07.9 Disorder of thyroid\nrec_dx_E07_once\nrec_dx_E07_sporadic\nrec_dx_E07_frequent\n\n\n\n\n\nExample of 3 binary covariates (hypothetical) created based on the candidate empirical covariates."
  },
  {
    "objectID": "step3.html#recurrence-covariates-in-the-data",
    "href": "step3.html#recurrence-covariates-in-the-data",
    "title": "6  Step 3: Recurrence",
    "section": "6.3 Recurrence covariates in the data",
    "text": "6.3 Recurrence covariates in the data\n\nout2 <- step2$recurrence_data\nncol(out2)\n#> [1] 143\n\n\n\n\n\n  \n\n\n\n\n\nHere we show binary recurrence covariates for only 2 columns"
  },
  {
    "objectID": "step3.html#refined-recurrence-covariates",
    "href": "step3.html#refined-recurrence-covariates",
    "title": "6  Step 3: Recurrence",
    "section": "6.4 Refined recurrence covariates",
    "text": "6.4 Refined recurrence covariates\nBelow you can click to see a list of all recurrence covariates obtained in our data.\n\n\nShow/Hide Table\n\n\n\n\nICD-10 Recurrence Data\n\n\n1\nrec_dx_A49_once\nrec_dx_B00_once\nrec_dx_B20_once\n\n\n2\nrec_dx_B20_frequent\nrec_dx_B35_once\nrec_dx_B37_once\n\n\n3\nrec_dx_B96_once\nrec_dx_C50_once\nrec_dx_D75_once\n\n\n4\nrec_dx_E03_once\nrec_dx_E04_once\nrec_dx_E05_once\n\n\n5\nrec_dx_E07_once\nrec_dx_E28_once\nrec_dx_E28_frequent\n\n\n6\nrec_dx_E29_once\nrec_dx_E78_once\nrec_dx_E79_once\n\n\n7\nrec_dx_E87_once\nrec_dx_F17_once\nrec_dx_F20_once\n\n\n8\nrec_dx_F20_frequent\nrec_dx_F29_once\nrec_dx_F31_once\n\n\n9\nrec_dx_F31_frequent\nrec_dx_F32_once\nrec_dx_F39_once\n\n\n10\nrec_dx_F41_once\nrec_dx_F43_once\nrec_dx_F90_once\n\n\n11\nrec_dx_G20_once\nrec_dx_G20_frequent\nrec_dx_G25_once\n\n\n12\nrec_dx_G30_once\nrec_dx_G30_frequent\nrec_dx_G31_once\n\n\n13\nrec_dx_G40_once\nrec_dx_G43_once\nrec_dx_G47_once\n\n\n14\nrec_dx_G89_once\nrec_dx_H04_once\nrec_dx_H10_once\n\n\n15\nrec_dx_H40_once\nrec_dx_H40_frequent\nrec_dx_H57_once\n\n\n16\nrec_dx_H57_frequent\nrec_dx_H66_once\nrec_dx_I10_once\n\n\n17\nrec_dx_I10_frequent\nrec_dx_I20_once\nrec_dx_I21_once\n\n\n18\nrec_dx_I48_once\nrec_dx_I48_frequent\nrec_dx_I49_once\n\n\n19\nrec_dx_I50_once\nrec_dx_I50_frequent\nrec_dx_I51_once\n\n\n20\nrec_dx_I63_once\nrec_dx_I70_once\nrec_dx_I80_once\n\n\n21\nrec_dx_I99_once\nrec_dx_J01_once\nrec_dx_J02_once\n\n\n22\nrec_dx_J20_once\nrec_dx_J30_once\nrec_dx_J40_once\n\n\n23\nrec_dx_J42_once\nrec_dx_J43_once\nrec_dx_J43_frequent\n\n\n24\nrec_dx_J44_once\nrec_dx_J44_frequent\nrec_dx_J45_once\n\n\n25\nrec_dx_J45_frequent\nrec_dx_J98_once\nrec_dx_K04_once\n\n\n26\nrec_dx_K08_once\nrec_dx_K21_once\nrec_dx_K25_once\n\n\n27\nrec_dx_K27_once\nrec_dx_K30_once\nrec_dx_K31_once\n\n\n28\nrec_dx_K58_once\nrec_dx_K59_once\nrec_dx_K76_once\n\n\n29\nrec_dx_K92_once\nrec_dx_L08_once\nrec_dx_L20_once\n\n\n30\nrec_dx_L23_once\nrec_dx_L29_once\nrec_dx_L40_once\n\n\n31\nrec_dx_L70_once\nrec_dx_L93_once\nrec_dx_L93_frequent\n\n\n32\nrec_dx_M06_once\nrec_dx_M06_frequent\nrec_dx_M10_once\n\n\n33\nrec_dx_M13_once\nrec_dx_M19_once\nrec_dx_M1A_once\n\n\n34\nrec_dx_M25_once\nrec_dx_M54_once\nrec_dx_M62_once\n\n\n35\nrec_dx_M79_once\nrec_dx_M81_once\nrec_dx_N19_once\n\n\n36\nrec_dx_N20_once\nrec_dx_N28_once\nrec_dx_N30_once\n\n\n37\nrec_dx_N32_once\nrec_dx_N39_once\nrec_dx_N40_once\n\n\n38\nrec_dx_N42_once\nrec_dx_N52_once\nrec_dx_N92_once\n\n\n39\nrec_dx_N94_once\nrec_dx_N95_once\nrec_dx_R00_once\n\n\n40\nrec_dx_R05_once\nrec_dx_R06_once\nrec_dx_R07_once\n\n\n41\nrec_dx_R09_once\nrec_dx_R10_once\nrec_dx_R11_once\n\n\n42\nrec_dx_R12_once\nrec_dx_R19_once\nrec_dx_R21_once\n\n\n43\nrec_dx_R25_once\nrec_dx_R32_once\nrec_dx_R35_once\n\n\n44\nrec_dx_R39_once\nrec_dx_R41_once\nrec_dx_R42_once\n\n\n45\nrec_dx_R45_once\nrec_dx_R51_once\nrec_dx_R52_once\n\n\n46\nrec_dx_R60_once\nrec_dx_R73_once\nrec_dx_T14_once\n\n\n47\nrec_dx_T78_once\nrec_dx_T88_once\nrec_dx_Z79_once\n\n\n48\nrec_dx_Z95_once\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nGiven that we had one dimension of proxy data, \\(p=1\\), at most \\(n=200\\) most prevalent codes (with the restriction that minimum number of patients in each code = 20), and \\(3\\) intensity, we could theoretically have at most \\(p \\times n \\times 3 = 1 \\times 200 \\times \\ 3 = 600\\) recurrence covariates.\n\n\n\n\nBased on all of the restrictions, we created 143 distinct recurrence covariates.\nThe merged data (analytic and proxies) size is now 7,585.\n\n\n\n\nIf 2 or all 3 recurrence covariates are identical, only one distinct recurrence covariate is returned. This is why you do not see any sporadic recurrence covariate here.\nRecurrence covariate creation is for each patient, and it is possible to have same code occur multiple time because we are working with a 3 digit granularity (possible to have medications from other codes within same ICD-10 3 digit granularity).\n\n\n\n\n\n\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn, Helen Mogun, and M Alan Brookhart. 2009. “High-Dimensional Propensity Score Adjustment in Studies of Treatment Effects Using Health Care Claims Data.” Epidemiology (Cambridge, Mass.) 20 (4): 512."
  },
  {
    "objectID": "step4.html#bross-formula",
    "href": "step4.html#bross-formula",
    "title": "7  Step 4: Prioritize",
    "section": "7.1 Bross formula",
    "text": "7.1 Bross formula\nWe need to make an educated guess about 3 components (i.e., make an assumption), that are used in the calculation of bias contributed by not adjusting for a covariate based on Bross (1966) formula:\n\n\nBross formula (Bross 1966; Schneeweiss 2006) for the Bias Multiplier considers both the imbalance in the prevalence of the unmeasured confounder between the exposure groups and the association between the confounder and the outcome to assess the potential bias.\n\nprevalence of a binary unmeasured confounder (\\(U\\)) among exposed (\\(P_{UA_1}\\))\nprevalence of that binary unmeasured confounder among unexposed (\\(P_{UA_0}\\))\nassociation between that binary unmeasured confounder and the outcome (\\(RR_{UY} = \\frac{P_{UY_1}}{P_{UY_1}}\\))\n\nThe above components can help us calculate \\(bias\\) amount (known as ‘Bias Multiplier’) using the Bross formula when we omit adjusting for \\(U\\):\n\\[\\text{Bias}_U = \\frac{P_{UA_1} (RR_{UY} - 1) + 1}{P_{UA_0} (RR_{UY} - 1) + 1}\\]\n\n\nThese are the ingredients of the Bross formula. This formula is helpful for understanding the impact of unmeasured confounding of a binary variable. We have to put assumed prevalence and risk ratio associated with an unmeasured confounder."
  },
  {
    "objectID": "step4.html#calculating-bias-from-a-recurrence-covariate",
    "href": "step4.html#calculating-bias-from-a-recurrence-covariate",
    "title": "7  Step 4: Prioritize",
    "section": "7.2 Calculating bias from a recurrence covariate",
    "text": "7.2 Calculating bias from a recurrence covariate\nFor recurrence covariates (\\(R\\)), we do not need to assume, we just plug-in \\(R\\) instead of \\(U\\) in the following calculations:\n\nprevalence of a binary recurrence variable among exposed (\\(P_{RA_1}\\))\nprevalence of that binary recurrence variable among unexposed (\\(P_{RA_0}\\))\nassociation between that binary recurrence variable and the outcome (\\(RR_{RY} = \\frac{P_{RY_1}}{P_{RY_1}}\\))\n\nThese components can help us empirically calculate \\(bias\\) amount:\n\\[\\text{Bias}_R = \\frac{P_{RA_1} (RR_{RY} - 1) + 1}{P_{RA_0} (RR_{RY} - 1) + 1}\\]\nHere, \\(RR_{RY}\\) is the crude risk ratio between the recurrence covariate and the outcome, \\(Y\\) is the outcome, \\(A\\) is the exposure, and \\(R\\) is a recurrence covariate.\n\n\nFor recurrence covariates, we do not need to assume, we can basically calculate these numbers (\\(log-absolute-bias\\)) for all of the recurrence covariates (Schneeweiss et al. 2009). For each data dimension, we can rank each of the recurrence covariates based on the amount of bias (confounding or imbalance) it could likely adjust."
  },
  {
    "objectID": "step4.html#calculating-bias-from-all-recurrence-covariates",
    "href": "step4.html#calculating-bias-from-all-recurrence-covariates",
    "title": "7  Step 4: Prioritize",
    "section": "7.3 Calculating bias from all recurrence covariates",
    "text": "7.3 Calculating bias from all recurrence covariates\nIn our example, we simply plug-in each recurrence covariates one-by-one to calculate \\(log-absolute-bias\\):\n\n\n\n\n\nR=rec_dx_D64_once\n\n\nR=rec_dx_D75_sporadic\n\n\n…\n\n\nR=rec_dx_E07_frequent"
  },
  {
    "objectID": "step4.html#obtain-log-of-absolute-bias",
    "href": "step4.html#obtain-log-of-absolute-bias",
    "title": "7  Step 4: Prioritize",
    "section": "7.4 Obtain log of absolute-bias",
    "text": "7.4 Obtain log of absolute-bias\nWe calculate \\(log-absolute-bias\\) for all recurrence covariates.\n\n\nAbsolute log of the Bias Multiplier, \\(log-absolute-bias\\), is a symmetric measure of the potential bias introduced by the recurrence covariate, making it easier to compare and rank recurrence covariates.\n\nout3 <- get_prioritised_covariates(df = out2,\n                                   patientIdVarname = \"idx\", \n                                   exposureVector = basetable$exposure,\n                                   outcomeVector = basetable$outcome,\n                                   patientIdVector = patientIds, \n                                   k = 100)\nsorted_values <- sort(out3$multiplicative_bias, \n                      decreasing = TRUE)\n\nThis would return absolute log of the multiplicative bias for each recurrence covariate (by univariate Bross formula). We can use this information to prioritize recurrence covariates in the next step."
  },
  {
    "objectID": "step4.html#convert-to-absolute-log-of-multiplicative-bias",
    "href": "step4.html#convert-to-absolute-log-of-multiplicative-bias",
    "title": "7  Step 4: Prioritize",
    "section": "7.5 Convert to Absolute log of multiplicative bias",
    "text": "7.5 Convert to Absolute log of multiplicative bias\nHere are the few covariates and associated Absolute log of the multiplicative bias:\n\n\n\n\n\nrec_dx_I10_once : 0.115\n\n\nrec_dx_R73_once : 0.088\n\n\nrec_dx_I10_frequent : 0.068\n\n\nrec_dx_R60_once : 0.054\n\n\nrec_dx_E78_once : 0.038\n\n\nrec_dx_M79_once : 0.017\n\n\nrec_dx_E87_once : 0.015\n\n\nrec_dx_I51_once : 0.013\n\n\nrec_dx_I50_once : 0.011\n\n\n\n\n\nAnd here are translated table with description:\n\n\n\n\n\nHypertension : 0.115\n\n\nElevated blood glucose level : 0.088\n\n\nHypertension : 0.068\n\n\nEdema : 0.054\n\n\nPure hypercholesterolemia : 0.038\n\n\nmusculoskeletal pain : 0.017\n\n\nHypokalemia : 0.015\n\n\nHeart disease : 0.013\n\n\nHeart failure : 0.011\n\n\n\n\n\n\n\nSome of the empirical covariates with top Absolute log of the multiplicative bias are actually relevant to the outcome (diabetes): Hypertension, Elevated blood glucose level , etc. (Choi and Shi 2001)\n\n\n\n\n\n\n\n\n\nSMD vs Bias multiplier\n\n\n\nStandardized mean difference (SMD) is useful for assessing the balance in the propensity score literature. However, Bross formula incorporates outcome information. In the investigation of empirical covariates or recurrence covariates where interpretations of these covariates are unknown, it may seem more safe to use the multiplicative bias term from the Bross formula to identify proxy covariates that are helpful in predicting the outcome.\n\n\n\n\n(Stuart, Lee, and Leacy 2013)\n\n\n\n\n\n\n\nBross, Irwin DJ. 1966. “Spurious Effects from an Extraneous Variable.” Journal of Chronic Diseases 19 (6): 637–47.\n\n\nChoi, BCK, and F Shi. 2001. “Risk Factors for Diabetes Mellitus by Age and Sex: Results of the National Population Health Survey.” Diabetologia 44: 1221–31.\n\n\nSchneeweiss, Sebastian. 2006. “Sensitivity Analysis and External Adjustment for Unmeasured Confounders in Epidemiologic Database Studies of Therapeutics.” Pharmacoepidemiology and Drug Safety 15 (5): 291–303.\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn, Helen Mogun, and M Alan Brookhart. 2009. “High-Dimensional Propensity Score Adjustment in Studies of Treatment Effects Using Health Care Claims Data.” Epidemiology (Cambridge, Mass.) 20 (4): 512.\n\n\nStuart, Elizabeth A, Brian K Lee, and Finbarr P Leacy. 2013. “Prognostic Score–Based Balance Measures Can Be a Useful Diagnostic for Propensity Score Methods in Comparative Effectiveness Research.” Journal of Clinical Epidemiology 66 (8): S84–90."
  },
  {
    "objectID": "step5.html#ideal-number-of-prioritised-covariates",
    "href": "step5.html#ideal-number-of-prioritised-covariates",
    "title": "8  Step 5: Covariates",
    "section": "8.1 Ideal number of prioritised covariates",
    "text": "8.1 Ideal number of prioritised covariates\nBased on calculated \\(log-absolute-bias\\), we select top k recurrence covariates to be used in the hdPS analyses later. Below is a plot of all of the absolute log of the Bias Multiplier:\n\n\n\n\n\n\n\n\n\nWe used \\(k = 100\\) covariates selected by the hdPS algorithm (we call them ‘hdPS covariates’). What should be the cutpoint?\n\n\n\nAbsolute log of the Bias Multiplier has a null value of 0. Anything above 0 is an indication of confounding bias adjusted by the adjustment of the associated recurrent covariate.\nFor large proxy data sources, \\(k = 500\\) is suggested (Schneeweiss et al. 2009).\nSee Sensitivity Analysis section for an understanding of how to choose a value based on an ad-hoc process."
  },
  {
    "objectID": "step5.html#selected-hdps-variables-proxies",
    "href": "step5.html#selected-hdps-variables-proxies",
    "title": "8  Step 5: Covariates",
    "section": "8.2 Selected hdPS variables (proxies)",
    "text": "8.2 Selected hdPS variables (proxies)\n\nhdps.dim <- out3$autoselected_covariate_df\ndim(hdps.dim) # id + k\n#> [1] 7585  101\nhead(hdps.dim)[,1:3]\n\n\n\n  \n\n\nhdps.dim$id <- hdps.dim$idx\nhdps.dim$idx <- NULL"
  },
  {
    "objectID": "step5.html#investigator-specified-covariates",
    "href": "step5.html#investigator-specified-covariates",
    "title": "8  Step 5: Covariates",
    "section": "8.3 Investigator-specified covariates",
    "text": "8.3 Investigator-specified covariates\n\\(25\\) investigator-specified covariates are selected based on variables in the DAG that are available in the data set.\n\n\nWe should also add necessary interactions of these investigator-specified covariates, or add other useful model-specifications (e.g., polynomials).\n\n\n\n\n\nHypothesized Directed acyclic graph drawn based on analyst’s best understanding of the literature\n\n\n\n\n\n\n\n14 demographic, behavioral, health history related variables/access\n\nMostly categorical\n\n11 lab variables\n\nMostly continuous\n\n\n\nexposure <- \"obese\"\noutcome <- \"diabetes\" \ninvestigator.specified.covariates <- \n  c(# Demographic\n  \"age.cat\", \"sex\", \"education\", \"race\", \n  \"marital\", \"income\", \"born\", \"year\",\n  \n  # health history related variables/access\n  \"diabetes.family.history\", \"medical.access\",\n  \n  # behavioral\n  \"smoking\", \"diet.healthy\", \"physical.activity\", \"sleep\",\n  \n  # Laboratory \n  \"uric.acid\", \"protein.total\", \"bilirubin.total\", \"phosphorus\",\n  \"sodium\", \"potassium\", \"globulin\", \"calcium.total\", \n  \"systolicBP\", \"diastolicBP\", \"high.cholesterol\"\n)\nlength(investigator.specified.covariates)\n#> [1] 25"
  },
  {
    "objectID": "step5.html#merged-data",
    "href": "step5.html#merged-data",
    "title": "8  Step 5: Covariates",
    "section": "8.4 Merged data",
    "text": "8.4 Merged data\n\nload(\"data/analytic3cycles.RData\")\nhdps.data <- merge(data.complete[,c(\"id\",\n                                    outcome, \n                                    exposure, \n                                    investigator.specified.covariates)], \n                       hdps.dim, by = \"id\")\ndim(hdps.data)\n#> [1] 7585  128\n\n\n\nVariable count (128)\n\n1 ID variable\n1 exposure\n1 outcome\n25 investigator-specified covariates\n100 hdPS variables\n\n\n\n\n\n\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn, Helen Mogun, and M Alan Brookhart. 2009. “High-Dimensional Propensity Score Adjustment in Studies of Treatment Effects Using Health Care Claims Data.” Epidemiology (Cambridge, Mass.) 20 (4): 512."
  },
  {
    "objectID": "step6.html#hdps-model",
    "href": "step6.html#hdps-model",
    "title": "9  Step 6: Propensity",
    "section": "9.1 hdPS model",
    "text": "9.1 hdPS model\n\n\n\n\n\n\n\n\n\n\n\nC = investigator-specified covariates and EC = hdPS covariates (Schneeweiss et al. 2009)\nThen the hdPS can be used as matching, weighting, stratifying variables, or as covariates (usually in deciles) in outcome model.\n\n\n(Wyss et al. 2022)\n\n9.1.1 Create propensity score formula\n\nhdps.data$exposure <- as.numeric(I(hdps.data$obese=='Yes'))\nhdps.data$outcome <- as.numeric(I(hdps.data$diabetes=='Yes'))\nproxy.list.sel <- names(out3$autoselected_covariate_df[,-1])\nproxyform <- paste0(proxy.list.sel, collapse = \"+\")\ncovform <- paste0(investigator.specified.covariates, collapse = \"+\")\n\n\nrhsformula <- paste0(c(covform, proxyform), collapse = \"+\")\nps.formula <- as.formula(paste0(\"exposure\", \"~\", rhsformula))\nps.formula\n#> exposure ~ age.cat + sex + education + race + marital + income + \n#>     born + year + diabetes.family.history + medical.access + \n#>     smoking + diet.healthy + physical.activity + sleep + uric.acid + \n#>     protein.total + bilirubin.total + phosphorus + sodium + potassium + \n#>     globulin + calcium.total + systolicBP + diastolicBP + high.cholesterol + \n#>     rec_dx_I10_once + rec_dx_R73_once + rec_dx_I10_frequent + \n#>     rec_dx_R60_once + rec_dx_E78_once + rec_dx_M79_once + rec_dx_E87_once + \n#>     rec_dx_I51_once + rec_dx_I50_once + rec_dx_D75_once + rec_dx_K21_once + \n#>     rec_dx_Z79_once + rec_dx_N28_once + rec_dx_F41_once + rec_dx_E79_once + \n#>     rec_dx_M10_once + rec_dx_M1A_once + rec_dx_L70_once + rec_dx_F32_once + \n#>     rec_dx_I80_once + rec_dx_F90_once + rec_dx_F43_once + rec_dx_I50_frequent + \n#>     rec_dx_B00_once + rec_dx_R12_once + rec_dx_N19_once + rec_dx_B20_once + \n#>     rec_dx_E28_frequent + rec_dx_R07_once + rec_dx_M19_once + \n#>     rec_dx_I48_once + rec_dx_G47_once + rec_dx_T14_once + rec_dx_I70_once + \n#>     rec_dx_M06_once + rec_dx_N40_once + rec_dx_R51_once + rec_dx_R21_once + \n#>     rec_dx_J45_once + rec_dx_H40_once + rec_dx_K31_once + rec_dx_I63_once + \n#>     rec_dx_H10_once + rec_dx_R52_once + rec_dx_R19_once + rec_dx_N95_once + \n#>     rec_dx_K92_once + rec_dx_J20_once + rec_dx_E28_once + rec_dx_R32_once + \n#>     rec_dx_M13_once + rec_dx_G30_once + rec_dx_J98_once + rec_dx_N52_once + \n#>     rec_dx_I49_once + rec_dx_N30_once + rec_dx_N42_once + rec_dx_R39_once + \n#>     rec_dx_N92_once + rec_dx_B96_once + rec_dx_H40_frequent + \n#>     rec_dx_M54_once + rec_dx_K59_once + rec_dx_G25_once + rec_dx_F31_once + \n#>     rec_dx_T78_once + rec_dx_R41_once + rec_dx_G31_once + rec_dx_I99_once + \n#>     rec_dx_G30_frequent + rec_dx_C50_once + rec_dx_F39_once + \n#>     rec_dx_R06_once + rec_dx_F17_once + rec_dx_K30_once + rec_dx_R45_once + \n#>     rec_dx_K25_once + rec_dx_B37_once + rec_dx_L08_once + rec_dx_T88_once + \n#>     rec_dx_J42_once + rec_dx_R10_once + rec_dx_M06_frequent + \n#>     rec_dx_G20_once + rec_dx_E07_once + rec_dx_K58_once + rec_dx_R35_once + \n#>     rec_dx_K04_once + rec_dx_R05_once + rec_dx_K08_once + rec_dx_I20_once + \n#>     rec_dx_H57_once + rec_dx_R25_once + rec_dx_H66_once + rec_dx_R42_once + \n#>     rec_dx_B20_frequent + rec_dx_I48_frequent + rec_dx_M62_once + \n#>     rec_dx_G43_once + rec_dx_F29_once\n\n\n\nThis is an overly simplistic scenario where we are adding only the main effects in the non-transformed form.\n\n\n9.1.2 Fit PS model\n\nrequire(WeightIt)\nW.out <- weightit(ps.formula, \n                    data = hdps.data, \n                    estimand = \"ATE\",\n                    method = \"ps\")\n\n\n\n9.1.3 Obtain PS\n\nhdps.data$ps <- W.out$ps\n\n\n\n\n\n\n\n\nAlways a good idea to check propensity score overlap in both exposure groups\n\n\n9.1.4 Obtain Weights\n\nhdps.data$w <- W.out$weights\n\n\n\n\n\n\n\n\nAlways a good idea to check the summary statistics of the weights to assess whether there are extreme weights\n\n\n9.1.5 Assessing balance\n\n\n\n\n\n\n\nAlways a good idea to assess balance. Here we are measuring against SMD 0.1. Use love.plot function from the cobalt package. See more descriptions of balanced diagnostics elsewhere for a propensity score context.\n\n\n\n\n\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn, Helen Mogun, and M Alan Brookhart. 2009. “High-Dimensional Propensity Score Adjustment in Studies of Treatment Effects Using Health Care Claims Data.” Epidemiology (Cambridge, Mass.) 20 (4): 512.\n\n\nWyss, Richard, Chen Yanover, Tal El-Hay, Dimitri Bennett, Robert W Platt, Andrew R Zullo, Grammati Sari, et al. 2022. “Machine Learning for Improving High-Dimensional Proxy Confounder Adjustment in Healthcare Database Studies: An Overview of the Current Literature.” Pharmacoepidemiology and Drug Safety 31 (9): 932–43."
  },
  {
    "objectID": "step7.html",
    "href": "step7.html",
    "title": "10  Step 7: Association",
    "section": "",
    "text": "10.0.1 Set outcome formula\n\nout.formula <- as.formula(paste0(\"outcome\", \"~\", \"exposure\"))\nout.formula\n#> outcome ~ exposure\n\n\n\n10.0.2 Obtain OR from unadjusted model\n\nfit <- glm(out.formula,\n            data = hdps.data,\n            weights = W.out$weights,\n            family= binomial(link = \"logit\"))\nfit.summary <- summary(fit)$coef[\"exposure\",\n                                 c(\"Estimate\", \n                                   \"Std. Error\", \n                                   \"Pr(>|z|)\")]\nfit.ci <- confint(fit, level = 0.95)[\"exposure\", ]\nfit.summary_with_ci <- c(fit.summary, fit.ci)\nround(fit.summary_with_ci,2) \n#>   Estimate Std. Error   Pr(>|z|)      2.5 %     97.5 % \n#>       0.42       0.04       0.00       0.35       0.49\n\n\n\n\nWe are using a crude outcome model here.\nSomewhat controversial to adjust for all (investigator-specified and all 100 proxies) covariates.\n\n\n\n10.0.3 Obtain RD from unadjusted model\n\nfit <- glm(out.formula,\n            data = hdps.data,\n            weights = W.out$weights,\n            family= gaussian(link = \"identity\"))\nfit.summary <- summary(fit)$coef[\"exposure\",\n                                 c(\"Estimate\", \n                                   \"Std. Error\", \n                                   \"Pr(>|t|)\")]\nfit.summary[2] <- sqrt(sandwich::sandwich(fit)[2,2])\nrequire(lmtest)\nconf.int <- confint(fit, \"exposure\", level = 0.95, method = \"hc1\")\nfit.summary <- c(fit.summary, conf.int)\nround(fit.summary, 2) \n#>   Estimate Std. Error   Pr(>|t|)      2.5 %     97.5 % \n#>       0.08       0.01       0.00       0.06       0.10\n\n\n\n(Naimi and Whitcomb 2020)\n\n\n\n\n\n\n\nNaimi, Ashley I, and Brian W Whitcomb. 2020. “Estimating Risk Ratios and Risk Differences Using Regression.” American Journal of Epidemiology 189 (6): 508–10."
  },
  {
    "objectID": "ps.html#fitting-ps-model-to-obtain-or",
    "href": "ps.html#fitting-ps-model-to-obtain-or",
    "title": "11  Propensity score",
    "section": "11.1 Fitting PS model to obtain OR",
    "text": "11.1 Fitting PS model to obtain OR\n\n11.1.1 Create propensity score formula\n\ncovform <- paste0(investigator.specified.covariates, collapse = \"+\")\nps.formula <- as.formula(paste0(\"exposure\", \"~\", covform))\nps.formula\n#> exposure ~ age.cat + sex + education + race + marital + income + \n#>     born + year + diabetes.family.history + medical.access + \n#>     smoking + diet.healthy + physical.activity + sleep + uric.acid + \n#>     protein.total + bilirubin.total + phosphorus + sodium + potassium + \n#>     globulin + calcium.total + systolicBP + diastolicBP + high.cholesterol\n\n\n\n\nOnly use investigator specified covariates to build the formula.\nDuring the construction of the propensity score model, researchers should consider incorporating additional model specifications, such as interactions and polynomials, if they are deemed necessary.\n\n\n\n\n\n\n\nOverfitting\n\n\n\nPropensity score models typically involve a greater number of covariates and incorporate other functional specifications (interactions) (Ho et al. 2007). In this context, overfitting is generally considered to be less worrisome as long as satisfactory overlap and balance diagnostics are achieved. This is because, some researchers suggest that propensity score model is meant to be descriptive of the data in hand, and should not be generalizabled (Judkins et al. 2007). However, there is a limit to it. Recent literature did suggest that propensity score model overfitting can lead to inflated variance of estimated treatment effect estimate (Schuster, Lowe, and Platt 2016). While the standard errors of the beta coefficients from the propensity score model are not typically a primary concern, it is generally advisable to examine them as part of the diagnostics process for assessing the propensity score model (Platt et al. 2019). Specifically, techniques like double cross-fitting have demonstrated promise in minimizing the aforementioned impact (Karim 2023).\n\n\n\n\n11.1.2 Fit PS model\n\nrequire(WeightIt)\nW.out <- weightit(ps.formula, \n                    data = hdps.data, \n                    estimand = \"ATE\",\n                    method = \"ps\")\n\n\n\n\nUse that formula to estimate propensity scores.\nIn this demonstration, we did not use stabilize = TRUE. However, stabilized propensity score weights often reduce the variance of treatment effect estimates.\n\n\n\n11.1.3 Obtain PS\n\n\n\n\n\n\n\nCheck propensity score overlap in both exposure groups. Similar as before?\n\n\n11.1.4 Obtain Weights\n\n\n\n\n\n\n\n\nCheck the summary statistics of the weights to assess whether there are extreme weights. Less extreme weights now?\n\n\n\n11.1.5 Assessing balance\n\n\n\n\n\n\n\n\nAssess balance against SMD 0.1. Still balanced?\nPredictive measures such as c-statistics are not helpful in this context (Westreich et al. 2011): “use of the c-statistic as a guide in constructing propensity scores may result in less overlap in propensity scores between treated and untreated subjects”!\n\n\n\n11.1.6 Set outcome formula\n\nout.formula <- as.formula(paste0(\"outcome\", \"~\", \"exposure\"))\nout.formula\n#> outcome ~ exposure\n\n\n\nWe are again using a crude weighted outcome model here.\n\n\n11.1.7 Obtain OR from unadjusted model\n\nfit <- glm(out.formula,\n            data = hdps.data,\n            weights = W.out$weights,\n            family= binomial(link = \"logit\"))\nfit.summary <- summary(fit)$coef[\"exposure\",\n                                 c(\"Estimate\", \n                                   \"Std. Error\", \n                                   \"Pr(>|z|)\")]\nfit.ci <- confint(fit, level = 0.95)[\"exposure\", ]\nfit.summary_with_ci.ps <- c(fit.summary, fit.ci)\nround(fit.summary_with_ci.ps,2) \n#>   Estimate Std. Error   Pr(>|z|)      2.5 %     97.5 % \n#>       0.68       0.04       0.00       0.61       0.76"
  },
  {
    "objectID": "ps.html#fitting-crude-model-to-obtain-or",
    "href": "ps.html#fitting-crude-model-to-obtain-or",
    "title": "11  Propensity score",
    "section": "11.2 Fitting crude model to obtain OR",
    "text": "11.2 Fitting crude model to obtain OR\n\n\n\n\n\n\nCrude association\n\n\n\nHere we estimate the crude association between the exposure and the outcome.\n\n\n\nfit <- glm(out.formula,\n            data = hdps.data,\n            family= binomial(link = \"logit\"))\nfit.summary <- summary(fit)$coef[\"exposure\",\n                                 c(\"Estimate\", \n                                   \"Std. Error\", \n                                   \"Pr(>|z|)\")]\nfit.ci <- confint(fit, level = 0.95)[\"exposure\", ]\nfit.summary_with_ci.crude <- c(fit.summary, fit.ci)\nround(fit.summary_with_ci.crude,2) \n#>   Estimate Std. Error   Pr(>|z|)      2.5 %     97.5 % \n#>       0.73       0.05       0.00       0.63       0.84\n\n\n\nNo adjustment at all!\n\n\n\n\n\n\n\nHo, Daniel E, Kosuke Imai, Gary King, and Elizabeth A Stuart. 2007. “Matching as Nonparametric Preprocessing for Reducing Model Dependence in Parametric Causal Inference.” Political Analysis 15 (3): 199–236.\n\n\nJudkins, David R, David Morganstein, Paul Zador, Andrea Piesse, Brandon Barrett, and Pushpal Mukhopadhyay. 2007. “Variable Selection and Raking in Propensity Scoring.” Statistics in Medicine 26 (5): 1022–33.\n\n\nKarim, ME. 2023. “Rethinking Residual Confounding Bias Reduction: Why Vanilla hdPS Alone Is No Longer Enough.” https://doi.org/10.5281/zenodo.7877767.\n\n\nPlatt, Robert W, Mohammad Ehsanul Karim, Thomas PA Debray, Massimiliano Copetti, Georgios Tsivgoulis, Emmanuelle Waubant, and Hans-Peter Hartung. 2019. “Comparison of Fingolimod, Dimethyl Fumarate and Teriflunomide for Multiple Sclerosis: When Methodology Does Not Hold the Promise.” Journal of Neurology, Neurosurgery, and Psychiatry 90 (4): 458.responses. https://jnnp.bmj.com/content/90/4/458.responses.\n\n\nSchuster, Tibor, Wilfrid Kouokam Lowe, and Robert W Platt. 2016. “Propensity Score Model Overfitting Led to Inflated Variance of Estimated Odds Ratios.” Journal of Clinical Epidemiology 80: 97–106.\n\n\nWestreich, Daniel, Stephen R Cole, Michele Jonsson Funk, M Alan Brookhart, and Til Stürmer. 2011. “The Role of the c-Statistic in Variable Selection for Propensity Score Models.” Pharmacoepidemiology and Drug Safety 20 (3): 317–20."
  },
  {
    "objectID": "sens.html#sensitivity-analysis-for-k",
    "href": "sens.html#sensitivity-analysis-for-k",
    "title": "12  Sensitivity",
    "section": "12.1 Sensitivity analysis for k",
    "text": "12.1 Sensitivity analysis for k\n\n12.1.1 Create propensity score formula\n\n\n\n\n\n\n\nHence we iterate the process (change k parameter in get_prioritised_covariates function in step 4) and obtain odds ratio (exponentiation of log-OR) for each k. We varied k from 10 to 140.\n\n\n\n\n\n\nTip\n\n\n\nOR estimates stabilizes around 1.5, shows variability below k = 50 and above 110"
  },
  {
    "objectID": "sens.html#sensitivity-analysis-for-n",
    "href": "sens.html#sensitivity-analysis-for-n",
    "title": "12  Sensitivity",
    "section": "12.2 Sensitivity analysis for n",
    "text": "12.2 Sensitivity analysis for n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe varied n from 10 to 120, remaining everything else constant (e.g., k = 100).\n\n\n\n\n\n\n\n\n\nHence we iterate the process (change n parameter in get_candidate_covariates function step 2) and obtain odds ratio (exponentiation of log-OR) for each n. We varied n from 10 to 120.\n\n\n\n\n\n\nTip\n\n\n\nOR estimates stabilizes around 1.5 for above n = 60.\n\n\n\n\nLiterature suggested that this restriction of n can be detrimental (Schuster, Pang, and Platt 2015). Hence in the original analysis we chose n such that that is larger than available empirical covariates.\n\n\n\n\n\n\n\nSchuster, Tibor, Menglan Pang, and Robert W Platt. 2015. “On the Role of Marginal Confounder Prevalence–Implications for the High-Dimensional Propensity Score Algorithm.” Pharmacoepidemiology and Drug Safety 24 (9): 1004–7."
  },
  {
    "objectID": "extension.html#issues-with-hdps",
    "href": "extension.html#issues-with-hdps",
    "title": "Challenges",
    "section": "Issues with hdPS",
    "text": "Issues with hdPS\n\n\n\n\n\n\nUnivariate selection of many proxies\n\n\n\n\nRecurrent covariates selected separately / univariately\ncan be correlated (coming from same patient) and cause multicollinearity\nmay inflate variance\nGeneral overfitting problem. Too many adjustment variables?\n\n\n\n\n\n(Franklin et al. 2015; Schuster, Lowe, and Platt 2016; Karim, Pang, and Platt 2018)"
  },
  {
    "objectID": "extension.html#potential-ways-to-improve",
    "href": "extension.html#potential-ways-to-improve",
    "title": "Challenges",
    "section": "Potential ways to improve",
    "text": "Potential ways to improve\n\nMultiple recurrent covariates could provide same information, may not be useful anymore in the presence of others. Multivariate structure could be good to consider in a single model.\nMachine learning variable selection methods could be useful to combat multicollinearity.\nSample splitting methods could be useful in combating overfitting in high dimensions.\n\n\n\nCross-validation is embedded within super (ensemble) learning."
  },
  {
    "objectID": "extension.html#controversy",
    "href": "extension.html#controversy",
    "title": "Challenges",
    "section": "Controversy",
    "text": "Controversy\nResearchers argue that the PS model, which does not allow for data-driven selection of variables, is a more principled approach to adjusting for confounding in observational studies, without introducing any bias in the analysis .\nOther researchers argue that the hdPS approach can improve the precision of effect estimates by including additional variables that are empirically associated with both the exposure and the outcome, which may reduce residual confounding.\n\n\nMachine learning alternatives have the same criticism as some of them depend on association with the outcome.\n\n\n\n\n\n\nTip\n\n\n\nhdPS can only control for observed confounding, and cannot guarantee the direction or magnitude of residual confounding that may still exist. This is why sensitivity analyses and model diagnostics are important in assessing the robustness of hdPS results.\n\n\n\n\n\n\n(VanderWeele 2019)\n\n\nFranklin, Jessica M, Wesley Eddings, Robert J Glynn, and Sebastian Schneeweiss. 2015. “Regularized Regression Versus the High-Dimensional Propensity Score for Confounding Adjustment in Secondary Database Analyses.” American Journal of Epidemiology 182 (7): 651–59.\n\n\nKarim, Mohammad Ehsanul, Menglan Pang, and Robert W Platt. 2018. “Can We Train Machine Learning Methods to Outperform the High-Dimensional Propensity Score Algorithm?” Epidemiology 29 (2): 191–98.\n\n\nSchuster, Tibor, Wilfrid Kouokam Lowe, and Robert W Platt. 2016. “Propensity Score Model Overfitting Led to Inflated Variance of Estimated Odds Ratios.” Journal of Clinical Epidemiology 80: 97–106.\n\n\nVanderWeele, Tyler J. 2019. “Principles of Confounder Selection.” European Journal of Epidemiology 34: 211–19."
  },
  {
    "objectID": "pubmed.html#pubmed",
    "href": "pubmed.html#pubmed",
    "title": "13  Literature",
    "section": "13.1 PubMed",
    "text": "13.1 PubMed\nCombination of plasmode, simulation, high-dimensional propensity provides 7 papers (searched in April 23, 2023):\n\n\n\n\n\nflowchart LR\n  A[PubMed] --> p4(Karim et al. 2018 \\nEpidemiology)\n  p4 --> ml1\n  p4 --> ml0[Hybrid]\n  A[PubMed] --> p2(Tian et al. 2018 \\nInt J Epidemiol.)\n  p2 --> ml1[Pure LASSO]\n  A[PubMed] --> p5(Wyss et al. 2018 \\nEpidemiology)\n  p5 --> sl1[vary k,\\nk=25,100:500\\nSuper \\nLearner]\n  p5 --> ct1\n  A[PubMed] --> p1(Benasseur et al. 2022 \\nPharmacoepidemiol Drug Saf. )\n  p1 --> ml2[Low k,\\nk = 10]\n  p1 --> ct1[cTMLE]\n  A[PubMed] --> p7(Neugebauer et al. 2015 \\nStat Med.)\n  p7 --> O2[time-varying \\ninterventions]\n  A[PubMed] --> p6(Franklin et al. 2015 \\nAm J Epidemiol.)\n  p6 --> ml1\n  p6 --> ml0\n  A[PubMed] --> p3(Schneeweiss et al. 2018 \\nClin Epidemiol.)\n  p3 --> O1[Review]\n  style p1 fill:#f44,stroke-width:2px,stroke:#f00,color:#fff;\n  style p3 fill:#f44,stroke-width:2px,stroke:#f00,color:#fff;\n  style p7 fill:#f44,stroke-width:2px,stroke:#f00,color:#fff;\n  style p5 fill:#ffff00,stroke-width:2px,stroke:#ffcc00,color:#000;\n  style p2 fill:#9f9,stroke-width:2px,stroke:#090,color:#000;\n  style p4 fill:#9f9,stroke-width:2px,stroke:#090,color:#000;\n  style p6 fill:#9f9,stroke-width:2px,stroke:#090,color:#000;\n\n\n\n\n\n\n\n\n\n\n\n(Benasseur et al. 2022; Tian, Schuemie, and Suchard 2018; Franklin et al. 2015; Neugebauer et al. 2015; Wyss et al. 2018; Karim, Pang, and Platt 2018; Schneeweiss 2018)"
  },
  {
    "objectID": "pubmed.html#outside-of-pubmed",
    "href": "pubmed.html#outside-of-pubmed",
    "title": "13  Literature",
    "section": "13.2 Outside of PubMed",
    "text": "13.2 Outside of PubMed\n\n\n\n\n\nflowchart LR\n  S[Simulations] --> p0(Pang et al. 2016 \\nInt. J Biostat.)\n  p0 --> t1[TMLE, \\nNo \\nsuper \\nlearner]\n  D--> p00(Pang et al. 2016 \\nEpidemiology)\n  p00 --> t1\n  D[Data \\nanalysis] --> p1(Ju et al. 2019 \\nJ App Stat.)\n  p1 --> sl1[Super \\nlearner, \\nNo TMLE, \\n bias not \\nused as a\\nperformance \\nmeasure]\n  D --> p3(Schneeweiss et al. 2017 \\nEpidemiology)\n  p3 --> ml1[LASSO]\n  S --> p4(Weberpals et al. 2021 \\nEpidemiology)\n  p4 --> ml1[LASSO]\n  p4 --> ml2[Autoencoder]\n  S --> p5(Ju et al. 2019 \\nStat Meth Med Res.)\n  p5 --> t1\n  p5 --> t2[cTMLE, \\nmore about \\ntime \\ncomplexity]\n  S --> p6(Low et al. 2015 \\nJ Comp Eff Res.)\n  p6 --> ml1\n  \n  style p1 fill:#ffff00,stroke-width:2px,stroke:#ffcc00,color:#000;\n  style p4 fill:#9f9,stroke-width:2px,stroke:#090,color:#000;\n  style p3 fill:#9f9,stroke-width:2px,stroke:#090,color:#000;\n  style p6 fill:#9f9,stroke-width:2px,stroke:#090,color:#000;\n  style p5 fill:#44f,stroke-width:2px,stroke:#00f,color:#fff;\n  style p0 fill:#44f,stroke-width:2px,stroke:#00f,color:#fff;\n  style p00 fill:#44f,stroke-width:2px,stroke:#00f,color:#fff;\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Pang, Schuster, Filion, Eberg, et al. 2016; Pang, Schuster, Filion, Schnitzer, et al. 2016; Ju, Gruber, et al. 2019; Ju, Combs, et al. 2019; Schneeweiss et al. 2017; Weberpals et al. 2021; Low, Gallego, and Shah 2016)\n\n\nBenasseur, Imane, Denis Talbot, Madeleine Durand, Anne Holbrook, Alexis Matteau, Brian J Potter, Christel Renoux, Mireille E Schnitzer, Jean-Éric Tarride, and Jason R Guertin. 2022. “A Comparison of Confounder Selection and Adjustment Methods for Estimating Causal Effects Using Large Healthcare Databases.” Pharmacoepidemiology and Drug Safety 31 (4): 424–33.\n\n\nFranklin, Jessica M, Wesley Eddings, Robert J Glynn, and Sebastian Schneeweiss. 2015. “Regularized Regression Versus the High-Dimensional Propensity Score for Confounding Adjustment in Secondary Database Analyses.” American Journal of Epidemiology 182 (7): 651–59.\n\n\nJu, Cheng, Mary Combs, Samuel D Lendle, Jessica M Franklin, Richard Wyss, Sebastian Schneeweiss, and Mark J van der Laan. 2019. “Propensity Score Prediction for Electronic Healthcare Databases Using Super Learner and High-Dimensional Propensity Score Methods.” Journal of Applied Statistics 46 (12): 2216–36.\n\n\nJu, Cheng, Susan Gruber, Samuel D Lendle, Antoine Chambaz, Jessica M Franklin, Richard Wyss, Sebastian Schneeweiss, and Mark J van Der Laan. 2019. “Scalable Collaborative Targeted Learning for High-Dimensional Data.” Statistical Methods in Medical Research 28 (2): 532–54.\n\n\nKarim, Mohammad Ehsanul, Menglan Pang, and Robert W Platt. 2018. “Can We Train Machine Learning Methods to Outperform the High-Dimensional Propensity Score Algorithm?” Epidemiology 29 (2): 191–98.\n\n\nLow, Yen Sia, Blanca Gallego, and Nigam Haresh Shah. 2016. “Comparing High-Dimensional Confounder Control Methods for Rapid Cohort Studies from Electronic Health Records.” Journal of Comparative Effectiveness Research 5 (2): 179–92.\n\n\nNeugebauer, Romain, Julie A Schmittdiel, Zheng Zhu, Jeremy A Rassen, John D Seeger, and Sebastian Schneeweiss. 2015. “High-Dimensional Propensity Score Algorithm in Comparative Effectiveness Research with Time-Varying Interventions.” Statistics in Medicine 34 (5): 753–81.\n\n\nPang, Menglan, Tibor Schuster, Kristian B Filion, Maria Eberg, and Robert W Platt. 2016. “Targeted Maximum Likelihood Estimation for Pharmacoepidemiologic Research.” Epidemiology (Cambridge, Mass.) 27 (4): 570.\n\n\nPang, Menglan, Tibor Schuster, Kristian B Filion, Mireille E Schnitzer, Maria Eberg, and Robert W Platt. 2016. “Effect Estimation in Point-Exposure Studies with Binary Outcomes and High-Dimensional Covariate Data–a Comparison of Targeted Maximum Likelihood Estimation and Inverse Probability of Treatment Weighting.” The International Journal of Biostatistics 12 (2).\n\n\nSchneeweiss, Sebastian. 2018. “Automated Data-Adaptive Analytics for Electronic Healthcare Data to Study Causal Treatment Effects.” Clinical Epidemiology, 771–88.\n\n\nSchneeweiss, Sebastian, Wesley Eddings, Robert J Glynn, Elisabetta Patorno, Jeremy Rassen, and Jessica M Franklin. 2017. “Variable Selection for Confounding Adjustment in High-Dimensional Covariate Spaces When Analyzing Healthcare Databases.” Epidemiology 28 (2): 237–48.\n\n\nTian, Yuxi, Martijn J Schuemie, and Marc A Suchard. 2018. “Evaluating Large-Scale Propensity Score Performance Through Real-World and Synthetic Data Experiments.” International Journal of Epidemiology 47 (6): 2005–14.\n\n\nWeberpals, Janick, Tim Becker, Jessica Davies, Fabian Schmich, Dominik Rüttinger, Fabian J Theis, and Anna Bauer-Mehren. 2021. “Deep Learning-Based Propensity Scores for Confounding Control in Comparative Effectiveness Research: A Large-Scale, Real-World Data Study.” Epidemiology 32 (3): 378–88.\n\n\nWyss, Richard, Sebastian Schneeweiss, Mark Van Der Laan, Samuel D Lendle, Cheng Ju, and Jessica M Franklin. 2018. “Using Super Learner Prediction Modeling to Improve High-Dimensional Propensity Score Estimation.” Epidemiology 29 (1): 96–106."
  },
  {
    "objectID": "mllogic.html#understanding-variables-role",
    "href": "mllogic.html#understanding-variables-role",
    "title": "Machine learning",
    "section": "Understanding variable’s role",
    "text": "Understanding variable’s role\n\n\n\n\n\n\n\n\n\n\n\n(Rubin and Thomas 1996; Rubin 1997; Brookhart et al. 2006)\n\nConfounders\n\n\n\n\nflowchart LR\n  C --> A\n  C --> Y\n  A --> Y\n  style A fill:#90EE90;\n  style Y fill:#ADD8E6;\n  style C fill:#FF0000;\n\n\n\n\n\n\n\n\n\n\nAdjusting Confounders help reduce bias\n\n\n(Near) instruments\n\n\n\n\nflowchart LR\n  C --> A\n  A --> Y\n  style A fill:#90EE90;\n  style Y fill:#ADD8E6;\n  style C fill:#FF0000;\n\n\n\n\n\n\n\n\n\n\nAdjusting for covariates strongly associated with the exposure: Adjusting for these variables can potentially amplify bias in the treatment effect estimate and increase standard error (SE).\n\n\nPrecision variables\n\n\n\n\nflowchart LR\n  C --> Y\n  A --> Y\n  style A fill:#90EE90;\n  style Y fill:#ADD8E6;\n  style C fill:#FF0000;\n\n\n\n\n\n\n\n\n\n\nAdjusting for covariates strongly associated with the outcome: Adjusting for these variables can lead to decrease of the SE of the treatment effect estimate.\n\n\nNoise variables\n\n\n\n\nflowchart LR\n  C\n  A --> Y\n  style A fill:#90EE90;\n  style Y fill:#ADD8E6;\n  style C fill:#FF0000;\n\n\n\n\n\n\n\n\n\n\nAdjusting for covariates that are neither associated with the outcome or the exposure can increase the SE of the treatment effect estimate."
  },
  {
    "objectID": "mllogic.html#overall-picture",
    "href": "mllogic.html#overall-picture",
    "title": "Machine learning",
    "section": "Overall picture",
    "text": "Overall picture\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoose variables associated with the outcome in general, as long as they are not mediator, collider or effect of the outcome. In hdPS, we chose proxies in the covariate assessment period (before exposure occurs), reducing the possibility of those proxies to be mediator, collider or effect of the outcome.\n\n\nBrookhart, M Alan, Sebastian Schneeweiss, Kenneth J Rothman, Robert J Glynn, Jerry Avorn, and Til Stürmer. 2006. “Variable Selection for Propensity Score Models.” American Journal of Epidemiology 163 (12): 1149–56.\n\n\nRubin, Donald B. 1997. “Estimating Causal Effects from Large Data Sets Using Propensity Scores.” Annals of Internal Medicine 127 (8_Part_2): 757–63.\n\n\nRubin, Donald B, and Neal Thomas. 1996. “Matching Using Estimated Propensity Scores: Relating Theory to Practice.” Biometrics, 249–64."
  },
  {
    "objectID": "mllasso.html",
    "href": "mllasso.html",
    "title": "14  Pure ML",
    "section": "",
    "text": "15 Pure ML approach (LASSO)\nStart with all recurrence variables (EC in the following equation)\nSay, 100 proxies (associated with outcome) were selected by LASSO approach (ML-hdPS)"
  },
  {
    "objectID": "mllasso.html#choose-variables-associated-with-outcome",
    "href": "mllasso.html#choose-variables-associated-with-outcome",
    "title": "14  Pure ML",
    "section": "15.1 Choose variables associated with outcome",
    "text": "15.1 Choose variables associated with outcome\n\nproxy.list <- names(out3$autoselected_covariate_df[,-1])\ncovarsTfull <- c(investigator.specified.covariates, proxy.list)\nY.form <- as.formula(paste0(c(\"outcome~ exposure\", \n                              covarsTfull), collapse = \"+\") )\ncovar.mat <- model.matrix(Y.form, data = hdps.data)[,-1]\nlasso.fit<-glmnet::cv.glmnet(y = hdps.data$outcome, \n                             x = covar.mat, \n                             type.measure='mse',\n                             family=\"binomial\",\n                             alpha = 1, \n                             nfolds = 5)\ncoef.fit<-coef(lasso.fit,s='lambda.min',exact=TRUE)\nsel.variables<-row.names(coef.fit)[which(as.numeric(coef.fit)!=0)]\nproxy.list.sel.ml <- proxy.list[proxy.list %in% sel.variables]\nlength(proxy.list.sel.ml)\n#> [1] 61\n\n\n\n\nFrom all proxies, we try to identify proxies that are empirically associated with the outcome based on a multivariate LASSO (outcome with all proxies in one model).\nNote that LASSO model is choosing variables based on association with the outcome conditional on the ’exposure`.\nVariable selection is only happening for proxy variables.\nInvestigator specified variables are not being subject to variable selection."
  },
  {
    "objectID": "mllasso.html#build-model-formula-based-on-selected-variables",
    "href": "mllasso.html#build-model-formula-based-on-selected-variables",
    "title": "14  Pure ML",
    "section": "15.2 Build model formula based on selected variables",
    "text": "15.2 Build model formula based on selected variables\n\ncovform <- paste0(investigator.specified.covariates, collapse = \"+\")\nproxyform <- paste0(proxy.list.sel.ml, collapse = \"+\")\nrhsformula <- paste0(c(covform, proxyform), collapse = \"+\")\nps.formula <- as.formula(paste0(\"exposure\", \"~\", rhsformula))\n\n\n\nBuild propensity score model based on selected variables based on LASSO."
  },
  {
    "objectID": "mllasso.html#fit-the-ps-model",
    "href": "mllasso.html#fit-the-ps-model",
    "title": "14  Pure ML",
    "section": "15.3 Fit the PS model",
    "text": "15.3 Fit the PS model\n\nrequire(WeightIt)\nW.out <- weightit(ps.formula, \n                    data = hdps.data, \n                    estimand = \"ATE\",\n                    method = \"ps\")\n\n\n\nPropensity score model fit to be able to calculate the inverse probability weights."
  },
  {
    "objectID": "mllasso.html#obtain-log-or-from-unadjusted-outcome-model",
    "href": "mllasso.html#obtain-log-or-from-unadjusted-outcome-model",
    "title": "14  Pure ML",
    "section": "15.4 Obtain log-OR from unadjusted outcome model",
    "text": "15.4 Obtain log-OR from unadjusted outcome model\n\nout.formula <- as.formula(paste0(\"outcome\", \"~\", \"exposure\"))\nfit <- glm(out.formula,\n            data = hdps.data,\n            weights = W.out$weights,\n            family= binomial(link = \"logit\"))\nfit.summary <- summary(fit)$coef[\"exposure\",\n                                 c(\"Estimate\", \n                                   \"Std. Error\", \n                                   \"Pr(>|z|)\")]\nfit.ci <- confint(fit, level = 0.95)[\"exposure\", ]\nfit.summary_with_ci <- c(fit.summary, fit.ci)\nround(fit.summary_with_ci,2) \n#>   Estimate Std. Error   Pr(>|z|)      2.5 %     97.5 % \n#>       0.41       0.04       0.00       0.34       0.49\n\n\n\nSummary of results (log-OR).\n\n\n\n\n\n\n\nFranklin, Jessica M, Wesley Eddings, Robert J Glynn, and Sebastian Schneeweiss. 2015. “Regularized Regression Versus the High-Dimensional Propensity Score for Confounding Adjustment in Secondary Database Analyses.” American Journal of Epidemiology 182 (7): 651–59.\n\n\nKarim, Mohammad Ehsanul, Menglan Pang, and Robert W Platt. 2018. “Can We Train Machine Learning Methods to Outperform the High-Dimensional Propensity Score Algorithm?” Epidemiology 29 (2): 191–98."
  },
  {
    "objectID": "mlhybrid.html#build-model-formula-based-on-selected-variables",
    "href": "mlhybrid.html#build-model-formula-based-on-selected-variables",
    "title": "15  Hybrid ML",
    "section": "15.1 Build model formula based on selected variables",
    "text": "15.1 Build model formula based on selected variables\n\n\nFrom hdPS variables, we try to identify proxies that are empirically associated with the outcome based on a multivariate LASSO (outcome with all proxies in one model).\n\nlength(proxy.list.sel)\n#> [1] 100\nproxy.list <- names(out3$autoselected_covariate_df[,-1])\ncovarsTfull <- c(investigator.specified.covariates, proxy.list)\nY.form <- as.formula(paste0(c(\"outcome~ exposure\", \n                              covarsTfull), collapse = \"+\") )\ncovar.mat <- model.matrix(Y.form, data = hdps.data)[,-1]\nlasso.fit<-glmnet::cv.glmnet(y = hdps.data$outcome, \n                             x = covar.mat, \n                             type.measure='mse',\n                             family=\"binomial\",\n                             alpha = 1, \n                             nfolds = 5)\ncoef.fit<-coef(lasso.fit,s='lambda.min',exact=TRUE)\nsel.variables<-row.names(coef.fit)[which(as.numeric(coef.fit)!=0)]\nproxy.list.sel.hybrid <- proxy.list[proxy.list %in% sel.variables]\nlength(proxy.list.sel.hybrid)\n#> [1] 52\nproxyform <- paste0(proxy.list.sel.hybrid, collapse = \"+\")\nrhsformula <- paste0(c(covform, proxyform), collapse = \"+\")\nps.formula <- as.formula(paste0(\"exposure\", \"~\", rhsformula))\n\n\n\nBuild propensity score model based on selected variables based on LASSO.\n\n15.1.1 Fit the PS model\n\nW.out <- weightit(ps.formula, \n                    data = hdps.data, \n                    estimand = \"ATE\",\n                    method = \"ps\")\n\n\n\nPropensity score model fit to be able to calculate the inverse probability weights.\n\n\n15.1.2 Obtain log-OR from unadjusted outcome model\n\nout.formula <- as.formula(paste0(\"outcome\", \"~\", \"exposure\"))\nfit <- glm(out.formula,\n            data = hdps.data,\n            weights = W.out$weights,\n            family= binomial(link = \"logit\"))\nfit.summary <- summary(fit)$coef[\"exposure\",\n                                 c(\"Estimate\", \n                                   \"Std. Error\", \n                                   \"Pr(>|z|)\")]\nfit.ci <- confint(fit, level = 0.95)[\"exposure\", ]\nfit.summary_with_ci.h <- c(fit.summary, fit.ci)\nround(fit.summary_with_ci.h,2) \n#>   Estimate Std. Error   Pr(>|z|)      2.5 %     97.5 % \n#>       0.44       0.04       0.00       0.36       0.51\n\n\n\nSummary of results (log-OR).\n\n\n\n\n\n\nAlternative process\n\n\n\nIt is also possible to start with ML selection, and then applying Bross’s formula on top of it (Schneeweiss et al. 2017).\n\n\n\n\n\n\n\n\n\nFranklin, Jessica M, Wesley Eddings, Robert J Glynn, and Sebastian Schneeweiss. 2015. “Regularized Regression Versus the High-Dimensional Propensity Score for Confounding Adjustment in Secondary Database Analyses.” American Journal of Epidemiology 182 (7): 651–59.\n\n\nKarim, Mohammad Ehsanul, Menglan Pang, and Robert W Platt. 2018. “Can We Train Machine Learning Methods to Outperform the High-Dimensional Propensity Score Algorithm?” Epidemiology 29 (2): 191–98.\n\n\nSchneeweiss, Sebastian, Wesley Eddings, Robert J Glynn, Elisabetta Patorno, Jeremy Rassen, and Jessica M Franklin. 2017. “Variable Selection for Confounding Adjustment in High-Dimensional Covariate Spaces When Analyzing Healthcare Databases.” Epidemiology 28 (2): 237–48."
  },
  {
    "objectID": "sl.html#build-model-formula-based-on-all-variables",
    "href": "sl.html#build-model-formula-based-on-all-variables",
    "title": "16  Ensemble",
    "section": "16.1 Build model formula based on all variables",
    "text": "16.1 Build model formula based on all variables\n\nproxy.list <- names(out3$autoselected_covariate_df[,-1])\nlength(proxy.list)\n#> [1] 100\ncovform <- paste0(investigator.specified.covariates, collapse = \"+\")\nproxyform <- paste0(proxy.list, collapse = \"+\")\nrhsformula <- paste0(c(covform, proxyform), collapse = \"+\")\nps.formula <- as.formula(paste0(\"exposure\", \"~\", rhsformula))\n\n\n\nWe work with all proxies"
  },
  {
    "objectID": "sl.html#fit-the-ps-model-with-super-learner",
    "href": "sl.html#fit-the-ps-model-with-super-learner",
    "title": "16  Ensemble",
    "section": "16.2 Fit the PS model with super learner",
    "text": "16.2 Fit the PS model with super learner\n\nrequire(WeightIt)\nW.out <- weightit(ps.formula, \n                  data = hdps.data, \n                  estimand = \"ATE\",\n                  method = \"super\",\n                  SL.library = c(\"SL.glm\", \n                                 \"SL.glmnet\",\n                                 \"SL.earth\"))\n#> Loading required namespace: glmnet\n#> Loading required namespace: earth\n\n\n\nPropensity score model fit based on super learning algorithm to be able to calculate the inverse probability weights."
  },
  {
    "objectID": "sl.html#obtain-log-or-from-unadjusted-outcome-model",
    "href": "sl.html#obtain-log-or-from-unadjusted-outcome-model",
    "title": "16  Ensemble",
    "section": "16.3 Obtain log-OR from unadjusted outcome model",
    "text": "16.3 Obtain log-OR from unadjusted outcome model\n\nsummary(W.out$ps)\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.006352 0.250715 0.430072 0.448121 0.628521 0.982149\nout.formula <- as.formula(paste0(\"outcome\", \"~\", \"exposure\"))\nfit <- glm(out.formula,\n            data = hdps.data,\n            weights = W.out$weights,\n            family= binomial(link = \"logit\"))\nfit.summary <- summary(fit)$coef[\"exposure\",\n                                 c(\"Estimate\", \n                                   \"Std. Error\", \n                                   \"Pr(>|z|)\")]\nfit.ci <- confint(fit, level = 0.95)[\"exposure\", ]\nfit.summary_with_ci.sl <- c(fit.summary, fit.ci)\nround(fit.summary_with_ci.sl,2) \n#>   Estimate Std. Error   Pr(>|z|)      2.5 %     97.5 % \n#>       0.47       0.04       0.00       0.39       0.54\n\n\n\nSummary of results (log-OR)."
  },
  {
    "objectID": "tmle.html#obtain-or-with-superlearner",
    "href": "tmle.html#obtain-or-with-superlearner",
    "title": "17  TMLE",
    "section": "17.1 Obtain OR with superlearner",
    "text": "17.1 Obtain OR with superlearner\n\nsummary(W.out$ps)\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> 0.006352 0.250715 0.430072 0.448121 0.628521 0.982149\nSL.library = c(\"SL.glm\", \"SL.glmnet\",\"SL.earth\")\nproxy.list <- names(out3$autoselected_covariate_df[,-1])\nObsData.noYA <- hdps.data[,c(investigator.specified.covariates, \n                             proxy.list)]\n\n\n\nWe use the same propensity score model that was fitted based on super learning algorithm.\n\ntmle.fit <- tmle::tmle(Y = hdps.data$outcome,\n                       A = hdps.data$exposure, \n                       W = ObsData.noYA, \n                       family = \"binomial\",\n                       V = 3,\n                       Q.SL.library = SL.library,\n                       g1W = W.out$ps)\n\n\n\nIf you want to know more about TMLE, look at other tutorials.\n\nestOR.tmle <- tmle.fit$estimates$OR\nestOR.tmle\n#> $psi\n#> [1] 1.565526\n#> \n#> $CI\n#> [1] 1.407437 1.741373\n#> \n#> $pvalue\n#> [1] 1.548568e-16\n#> \n#> $log.psi\n#> [1] 0.4482221\n#> \n#> $var.log.psi\n#> [1] 0.002949814\n\n\n\nSummary of results (OR)."
  },
  {
    "objectID": "tmle.html#obtain-or-without-superlearner",
    "href": "tmle.html#obtain-or-without-superlearner",
    "title": "17  TMLE",
    "section": "17.2 Obtain OR without superlearner",
    "text": "17.2 Obtain OR without superlearner\n\nsummary(W.out0$ps)\n#>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#> 0.0000003 0.2445201 0.4308397 0.4481213 0.6321148 0.9975438\nSL.library = c(\"SL.glm\")\nproxy.list <- names(out3$autoselected_covariate_df[,-1])\nObsData.noYA <- hdps.data[,c(investigator.specified.covariates, \n                             proxy.list)]\n\n\n\nWe use the same propensity score model that was fitted based on hdPS variables via logistic regression (no other learners).\n\ntmle.fit0 <- tmle::tmle(Y = hdps.data$outcome,\n                       A = hdps.data$exposure, \n                       W = ObsData.noYA, \n                       family = \"binomial\",\n                       V = 3,\n                       Q.SL.library = SL.library,\n                       g1W = W.out$ps)\n\n\nestOR.tmle0 <- tmle.fit0$estimates$OR\nestOR.tmle0\n#> $psi\n#> [1] 1.549417\n#> \n#> $CI\n#> [1] 1.390011 1.727104\n#> \n#> $pvalue\n#> [1] 2.675267e-15\n#> \n#> $log.psi\n#> [1] 0.4378789\n#> \n#> $var.log.psi\n#> [1] 0.003068198\n\n\n\nSummary of results (OR)."
  },
  {
    "objectID": "mlcompare.html",
    "href": "mlcompare.html",
    "title": "18  Compare results",
    "section": "",
    "text": "Summary of model results\n \n  \n      \n    OR \n    Beta-coef \n    coef-SE \n    CI (2.5 %) \n    CI (97.5 %) \n    p-value \n  \n \n\n  \n    Crude (no adjustment) \n    2.08 \n    0.73 \n    0.05 \n    0.63 \n    0.84 \n    < 2e-16 \n  \n  \n    PS (no proxies) \n    1.98 \n    0.68 \n    0.04 \n    0.61 \n    0.76 \n    < 2e-16 \n  \n  \n    hdPS \n    1.52 \n    0.42 \n    0.04 \n    0.35 \n    0.49 \n    < 2e-16 \n  \n  \n    Pure LASSO \n    1.51 \n    0.41 \n    0.04 \n    0.34 \n    0.49 \n    < 2e-16 \n  \n  \n    Hybrid (hdPS, then LASSO) \n    1.55 \n    0.44 \n    0.04 \n    0.36 \n    0.51 \n    < 2e-16 \n  \n  \n    Super learner (GLM, LASSO, MARS) \n    1.60 \n    0.47 \n    0.04 \n    0.39 \n    0.54 \n    < 2e-16 \n  \n  \n    TMLE (GLM, LASSO, MARS in SL) \n    1.57 \n    0.45 \n    0.05 \n    0.34 \n    0.55 \n    < 2e-16 \n  \n  \n    TMLE (only GLM in SL) \n    1.55 \n    0.44 \n    0.06 \n    0.33 \n    0.55 \n    2.7e-15 \n  \n\n\n\n\n\n\n\n\nPS is the result from the propensity score approach that did not include any proxies.\nResults from this approach is somewhat different than other approaches.\nMore detailed results from simulations are available elsewhere (Karim 2023).\n\n\n\n\n\n\n\n\nMost hdPS, ML extensions (pure, SL or TMLE) and hybrids perform similarly (ORs between 1.52-1.6)\n\n\n\n\n\n\n\nKarim, ME. 2023. “Rethinking Residual Confounding Bias Reduction: Why Vanilla hdPS Alone Is No Longer Enough.” https://doi.org/10.5281/zenodo.7877767."
  },
  {
    "objectID": "guideline.html#reviews-and-guidelines",
    "href": "guideline.html#reviews-and-guidelines",
    "title": "Guideline",
    "section": "Reviews and Guidelines",
    "text": "Reviews and Guidelines\n\n\n\n\n\nflowchart LR\n  r[Reviews] --> p1(Wyss et al. 2022 \\nPharmacoepidemiol Drug Saf.)\n  r --> p0(Schneeweiss et al. 2018 \\nClin Epidemiol.)\n  g[Guideline] --> p2(Rassen et al. 2022 \\nPharmacoepidemiol Drug Saf.)\n  g --> p3(Tazare et al. 2022 \\nPharmacoepidemiol Drug Saf.)\n  style p0 fill:#f44,stroke-width:2px,stroke:#f00,color:#fff;\n  style p1 fill:#f44,stroke-width:2px,stroke:#f00,color:#fff;\n  style p2 fill:#b03,stroke-width:2px,stroke:#600,color:#fff;\n  style p3 fill:#b03,stroke-width:2px,stroke:#600,color:#fff;\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Wyss et al. 2022; Rassen et al. 2023; Tazare et al. 2022; Schneeweiss 2018)\n\n\nRassen, Jeremy A, Patrick Blin, Sebastian Kloss, Romain S Neugebauer, Robert W Platt, Anton Pottegård, Sebastian Schneeweiss, and Sengwee Toh. 2023. “High-Dimensional Propensity Scores for Empirical Covariate Selection in Secondary Database Studies: Planning, Implementation, and Reporting.” Pharmacoepidemiology and Drug Safety 32 (2): 93–106.\n\n\nSchneeweiss, Sebastian. 2018. “Automated Data-Adaptive Analytics for Electronic Healthcare Data to Study Causal Treatment Effects.” Clinical Epidemiology, 771–88.\n\n\nTazare, John, Richard Wyss, Jessica M Franklin, Liam Smeeth, Stephen JW Evans, Shirley V Wang, Sebastian Schneeweiss, Ian J Douglas, Joshua J Gagne, and Elizabeth J Williamson. 2022. “Transparency of High-Dimensional Propensity Score Analyses: Guidance for Diagnostics and Reporting.” Pharmacoepidemiology and Drug Safety 31 (4): 411–23.\n\n\nWyss, Richard, Chen Yanover, Tal El-Hay, Dimitri Bennett, Robert W Platt, Andrew R Zullo, Grammati Sari, et al. 2022. “Machine Learning for Improving High-Dimensional Proxy Confounder Adjustment in Healthcare Database Studies: An Overview of the Current Literature.” Pharmacoepidemiology and Drug Safety 31 (9): 932–43."
  },
  {
    "objectID": "report.html#analysis-information",
    "href": "report.html#analysis-information",
    "title": "19  Reporting",
    "section": "19.1 Analysis Information",
    "text": "19.1 Analysis Information\nMany reporting guideline already exists about what to report in a propensity score analysis. Most of the reporting guideline should be applicable in the hdPS context as well. On top of those, we also need to consider reporting the following information about hdPS for transparency.\n\n\n(Karim et al. 2022; Simoneau et al. 2022)"
  },
  {
    "objectID": "report.html#information-about-hdps",
    "href": "report.html#information-about-hdps",
    "title": "19  Reporting",
    "section": "19.2 Information about hdPS",
    "text": "19.2 Information about hdPS\n\n\n\n\n\nInformation\nDescription\nOur example\n\n\n\n\nProxy data dimensions\nThe number of data dimensions (p) used.\np = 1 as only one proxy data dimension (dx) was available from medication usage\n\n\nWhat was done to remove proxies that are problematic\nUsually proxies of outcome, exposure as well as those identified as IV or mediator or collider are discarded.\nobesity and diabetes related codes removed\n\n\nProxy feature parameters\nThe parameters used to select proxy features, including granularity [g], prevalence filter [n], and the minimum number of patients [m]\ng = 3, n = 200, m = 20. This resulted in 126 empirical covariates.\n\n\nRecurrence parameters\nHow many recurrence variables per code [r] and the covariate assessment period [CAP]\nr = 3, CAP = 30 days. This resulted in 143 distinct recurrence covariates.\n\n\nPrioritization process\nThe process used to prioritize proxy features, such as machine learning (ML), Bross, or hybrid methods\nWe used all of these, but used Bross formula for hdPS to calculate absolute log of the multiplicative bias, and then ranked based on magnitude to select / prioritize recurrence covariates.\n\n\nSelected proxies\nThe number of proxies selected (k) for the model\nk = 100 for the hdPS\n\n\nSoftware\nThe software used to perform the analysis\nR: autoCovariateSelection package\n\n\n\n\n\n(Rassen et al. 2023; Tazare et al. 2022)"
  },
  {
    "objectID": "report.html#diagnostics",
    "href": "report.html#diagnostics",
    "title": "19  Reporting",
    "section": "19.3 Diagnostics",
    "text": "19.3 Diagnostics\n\n\n\nInformation\nDescription\nOur example\n\n\n\n\nDiagnostics used to assess the model\nStandardized mean differences (SMD)\nWithin 0.1 in hdPS analysis\n\n\n\nWeight (IPW) summary assessment\nSomewhat reasonable (maximum approximately 54) within hdPS analysis\n\n\n\nComparison of propensity score distributions between each exposure group\nOverlapping (common support) does not seem to be an issue.\n\n\n\nAssess distribution of absolute log bias\nMost bias multiplier values are close to null (0), only a few values seem to deviate from null.\n\n\n\nComparison with regular propensity score\nEstimates slightly towards null"
  },
  {
    "objectID": "report.html#sensitivity-analysis",
    "href": "report.html#sensitivity-analysis",
    "title": "19  Reporting",
    "section": "19.4 Sensitivity analysis",
    "text": "19.4 Sensitivity analysis\n\n\n\nInformation\nDescription\nOur example\n\n\n\n\nSensitivity analysis\nVarying the number of selected proxies [k].\nOR estimates stabilizes around 1.5, shows variability below k = 50 and above 110\n\n\nSensitivity analysis\nVarying the prevalence filter [n].\nOR estimates stabilizes around 1.5 for above n = 60.\n\n\n\n\n\n\n\nKarim, Mohammad Ehsanul, Fabio Pellegrini, Robert W Platt, Gabrielle Simoneau, Julie Rouette, and Carl de Moor. 2022. “The Use and Quality of Reporting of Propensity Score Methods in Multiple Sclerosis Literature: A Review.” Multiple Sclerosis Journal 28 (9): 1317–23.\n\n\nRassen, Jeremy A, Patrick Blin, Sebastian Kloss, Romain S Neugebauer, Robert W Platt, Anton Pottegård, Sebastian Schneeweiss, and Sengwee Toh. 2023. “High-Dimensional Propensity Scores for Empirical Covariate Selection in Secondary Database Studies: Planning, Implementation, and Reporting.” Pharmacoepidemiology and Drug Safety 32 (2): 93–106.\n\n\nSimoneau, Gabrielle, Fabio Pellegrini, Thomas PA Debray, Julie Rouette, Johanna Muñoz, Robert W Platt, John Petkau, et al. 2022. “Recommendations for the Use of Propensity Score Methods in Multiple Sclerosis Research.” Multiple Sclerosis Journal 28 (9): 1467–80.\n\n\nTazare, John, Richard Wyss, Jessica M Franklin, Liam Smeeth, Stephen JW Evans, Shirley V Wang, Sebastian Schneeweiss, Ian J Douglas, Joshua J Gagne, and Elizabeth J Williamson. 2022. “Transparency of High-Dimensional Propensity Score Analyses: Guidance for Diagnostics and Reporting.” Pharmacoepidemiology and Drug Safety 31 (4): 411–23."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Benasseur, Imane, Denis Talbot, Madeleine Durand, Anne Holbrook, Alexis\nMatteau, Brian J Potter, Christel Renoux, Mireille E Schnitzer,\nJean-Éric Tarride, and Jason R Guertin. 2022. “A Comparison of\nConfounder Selection and Adjustment Methods for Estimating Causal\nEffects Using Large Healthcare Databases.”\nPharmacoepidemiology and Drug Safety 31 (4): 424–33.\n\n\nBrookhart, M Alan, Sebastian Schneeweiss, Kenneth J Rothman, Robert J\nGlynn, Jerry Avorn, and Til Stürmer. 2006. “Variable Selection for\nPropensity Score Models.” American Journal of\nEpidemiology 163 (12): 1149–56.\n\n\nBross, Irwin DJ. 1966. “Spurious Effects from an Extraneous\nVariable.” Journal of Chronic Diseases 19 (6): 637–47.\n\n\nCharlson, Mary E, Peter Pompei, Kathy L Ales, and C Ronald MacKenzie.\n1987. “A New Method of Classifying Prognostic Comorbidity in\nLongitudinal Studies: Development and Validation.” Journal of\nChronic Diseases 40 (5): 373–83.\n\n\nChoi, BCK, and F Shi. 2001. “Risk Factors for Diabetes Mellitus by\nAge and Sex: Results of the National Population Health Survey.”\nDiabetologia 44: 1221–31.\n\n\nConnolly, John G, Sebastian Schneeweiss, Robert J Glynn, and Joshua J\nGagne. 2019. “Quantifying Bias Reduction with Fixed-Duration\nVersus All-Available Covariate Assessment Periods.”\nPharmacoepidemiology and Drug Safety 28 (5): 665–70.\n\n\nDisease Control, Centers for, and Prevention. 2021. “National\nHealth and Nutrition Examination Survey (NHANES).” National\nCenter for Health Statistics.\n\n\nElixhauser, Anne, Claudia Steiner, D Robert Harris, and Rosanna M\nCoffey. 1998. “Comorbidity Measures for Use with Administrative\nData.” Medical Care, 8–27.\n\n\nFranklin, Jessica M, Wesley Eddings, Robert J Glynn, and Sebastian\nSchneeweiss. 2015. “Regularized Regression Versus the\nHigh-Dimensional Propensity Score for Confounding Adjustment in\nSecondary Database Analyses.” American Journal of\nEpidemiology 182 (7): 651–59.\n\n\nGreenland, Sander, Judea Pearl, and James M Robins. 1999. “Causal\nDiagrams for Epidemiologic Research.” Epidemiology,\n37–48.\n\n\nHernán, Miguel A, and Sarah L Taubman. 2008. “Does Obesity Shorten\nLife? The Importance of Well-Defined Interventions to Answer Causal\nQuestions.” International Journal of Obesity 32 (3):\nS8–14.\n\n\nHo, Daniel E, Kosuke Imai, Gary King, and Elizabeth A Stuart. 2007.\n“Matching as Nonparametric Preprocessing for Reducing Model\nDependence in Parametric Causal Inference.” Political\nAnalysis 15 (3): 199–236.\n\n\nJu, Cheng, Mary Combs, Samuel D Lendle, Jessica M Franklin, Richard\nWyss, Sebastian Schneeweiss, and Mark J van der Laan. 2019.\n“Propensity Score Prediction for Electronic Healthcare Databases\nUsing Super Learner and High-Dimensional Propensity Score\nMethods.” Journal of Applied Statistics 46 (12):\n2216–36.\n\n\nJu, Cheng, Susan Gruber, Samuel D Lendle, Antoine Chambaz, Jessica M\nFranklin, Richard Wyss, Sebastian Schneeweiss, and Mark J van Der Laan.\n2019. “Scalable Collaborative Targeted Learning for\nHigh-Dimensional Data.” Statistical Methods in Medical\nResearch 28 (2): 532–54.\n\n\nJudkins, David R, David Morganstein, Paul Zador, Andrea Piesse, Brandon\nBarrett, and Pushpal Mukhopadhyay. 2007. “Variable Selection and\nRaking in Propensity Scoring.” Statistics in Medicine 26\n(5): 1022–33.\n\n\nKarim, ME. 2023. “Rethinking Residual Confounding Bias Reduction:\nWhy Vanilla hdPS Alone Is No Longer Enough.” https://doi.org/10.5281/zenodo.7877767.\n\n\nKarim, Mohammad Ehsanul, Menglan Pang, and Robert W Platt. 2018.\n“Can We Train Machine Learning Methods to Outperform the\nHigh-Dimensional Propensity Score Algorithm?”\nEpidemiology 29 (2): 191–98.\n\n\nKarim, Mohammad Ehsanul, Fabio Pellegrini, Robert W Platt, Gabrielle\nSimoneau, Julie Rouette, and Carl de Moor. 2022. “The Use and\nQuality of Reporting of Propensity Score Methods in Multiple Sclerosis\nLiterature: A Review.” Multiple Sclerosis Journal 28\n(9): 1317–23.\n\n\nKlein, Samuel, Amalia Gastaldelli, Hannele Yki-Järvinen, and Philipp E\nScherer. 2022. “Why Does Obesity Cause Diabetes?” Cell\nMetabolism 34 (1): 11–20.\n\n\nLix, Lisa M, Jacqueline Quail, Opeyemi Fadahunsi, and Gary F Teare.\n2013. “Predictive Performance of Comorbidity Measures in\nAdministrative Databases for Diabetes Cohorts.” BMC Health\nServices Research 13: 1–12.\n\n\nLix, LM, J Quail, G Teare, and B Acan. 2011. “Performance of\nComorbidity Measures for Predicting Outcomes in Population-Based\nOsteoporosis Cohorts.” Osteoporosis International 22:\n2633–43.\n\n\nLow, Yen Sia, Blanca Gallego, and Nigam Haresh Shah. 2016.\n“Comparing High-Dimensional Confounder Control Methods for Rapid\nCohort Studies from Electronic Health Records.” Journal of\nComparative Effectiveness Research 5 (2): 179–92.\n\n\nNaimi, Ashley I, and Brian W Whitcomb. 2020. “Estimating Risk\nRatios and Risk Differences Using Regression.” American\nJournal of Epidemiology 189 (6): 508–10.\n\n\nNeugebauer, Romain, Julie A Schmittdiel, Zheng Zhu, Jeremy A Rassen,\nJohn D Seeger, and Sebastian Schneeweiss. 2015. “High-Dimensional\nPropensity Score Algorithm in Comparative Effectiveness Research with\nTime-Varying Interventions.” Statistics in Medicine 34\n(5): 753–81.\n\n\nPang, Menglan, Tibor Schuster, Kristian B Filion, Maria Eberg, and\nRobert W Platt. 2016. “Targeted Maximum Likelihood Estimation for\nPharmacoepidemiologic Research.” Epidemiology (Cambridge,\nMass.) 27 (4): 570.\n\n\nPang, Menglan, Tibor Schuster, Kristian B Filion, Mireille E Schnitzer,\nMaria Eberg, and Robert W Platt. 2016. “Effect Estimation in\nPoint-Exposure Studies with Binary Outcomes and High-Dimensional\nCovariate Data–a Comparison of Targeted Maximum Likelihood Estimation\nand Inverse Probability of Treatment Weighting.” The\nInternational Journal of Biostatistics 12 (2).\n\n\nPlatt, Robert W, Mohammad Ehsanul Karim, Thomas PA Debray, Massimiliano\nCopetti, Georgios Tsivgoulis, Emmanuelle Waubant, and Hans-Peter\nHartung. 2019. “Comparison of Fingolimod, Dimethyl Fumarate and\nTeriflunomide for Multiple Sclerosis: When Methodology Does Not Hold the\nPromise.” Journal of Neurology, Neurosurgery, and\nPsychiatry 90 (4): 458.responses. https://jnnp.bmj.com/content/90/4/458.responses.\n\n\nRassen, Jeremy A, Patrick Blin, Sebastian Kloss, Romain S Neugebauer,\nRobert W Platt, Anton Pottegård, Sebastian Schneeweiss, and Sengwee Toh.\n2023. “High-Dimensional Propensity Scores for Empirical Covariate\nSelection in Secondary Database Studies: Planning, Implementation, and\nReporting.” Pharmacoepidemiology and Drug Safety 32 (2):\n93–106.\n\n\nRobert, Dennis. 2020. autoCovariateSelection: Automatic Covariate\nSelection. https://CRAN.R-project.org/package=autoCovariateSelection.\n\n\nRubin, Donald B. 1997. “Estimating Causal Effects from Large Data\nSets Using Propensity Scores.” Annals of Internal\nMedicine 127 (8_Part_2): 757–63.\n\n\nRubin, Donald B, and Neal Thomas. 1996. “Matching Using Estimated\nPropensity Scores: Relating Theory to Practice.”\nBiometrics, 249–64.\n\n\nSchneeweiss, Sebastian. 2006. “Sensitivity Analysis and External\nAdjustment for Unmeasured Confounders in Epidemiologic Database Studies\nof Therapeutics.” Pharmacoepidemiology and Drug Safety\n15 (5): 291–303.\n\n\n———. 2018. “Automated Data-Adaptive Analytics for Electronic\nHealthcare Data to Study Causal Treatment Effects.” Clinical\nEpidemiology, 771–88.\n\n\nSchneeweiss, Sebastian, Wesley Eddings, Robert J Glynn, Elisabetta\nPatorno, Jeremy Rassen, and Jessica M Franklin. 2017. “Variable\nSelection for Confounding Adjustment in High-Dimensional Covariate\nSpaces When Analyzing Healthcare Databases.”\nEpidemiology 28 (2): 237–48.\n\n\nSchneeweiss, Sebastian, and Malcolm Maclure. 2000. “Use of\nComorbidity Scores for Control of Confounding in Studies Using\nAdministrative Databases.” International Journal of\nEpidemiology 29 (5): 891–98.\n\n\nSchneeweiss, Sebastian, Jeremy A Rassen, Robert J Glynn, Jerry Avorn,\nHelen Mogun, and M Alan Brookhart. 2009. “High-Dimensional\nPropensity Score Adjustment in Studies of Treatment Effects Using Health\nCare Claims Data.” Epidemiology (Cambridge, Mass.) 20\n(4): 512.\n\n\nSchuster, Tibor, Wilfrid Kouokam Lowe, and Robert W Platt. 2016.\n“Propensity Score Model Overfitting Led to Inflated Variance of\nEstimated Odds Ratios.” Journal of Clinical Epidemiology\n80: 97–106.\n\n\nSchuster, Tibor, Menglan Pang, and Robert W Platt. 2015. “On the\nRole of Marginal Confounder Prevalence–Implications for the\nHigh-Dimensional Propensity Score Algorithm.”\nPharmacoepidemiology and Drug Safety 24 (9): 1004–7.\n\n\nSimoneau, Gabrielle, Fabio Pellegrini, Thomas PA Debray, Julie Rouette,\nJohanna Muñoz, Robert W Platt, John Petkau, et al. 2022.\n“Recommendations for the Use of Propensity Score Methods in\nMultiple Sclerosis Research.” Multiple Sclerosis Journal\n28 (9): 1467–80.\n\n\nStuart, Elizabeth A, Brian K Lee, and Finbarr P Leacy. 2013.\n“Prognostic Score–Based Balance Measures Can Be a Useful\nDiagnostic for Propensity Score Methods in Comparative Effectiveness\nResearch.” Journal of Clinical Epidemiology 66 (8):\nS84–90.\n\n\nTazare, John, Richard Wyss, Jessica M Franklin, Liam Smeeth, Stephen JW\nEvans, Shirley V Wang, Sebastian Schneeweiss, Ian J Douglas, Joshua J\nGagne, and Elizabeth J Williamson. 2022. “Transparency of\nHigh-Dimensional Propensity Score Analyses: Guidance for Diagnostics and\nReporting.” Pharmacoepidemiology and Drug Safety 31 (4):\n411–23.\n\n\nTian, Yuxi, Martijn J Schuemie, and Marc A Suchard. 2018.\n“Evaluating Large-Scale Propensity Score Performance Through\nReal-World and Synthetic Data Experiments.” International\nJournal of Epidemiology 47 (6): 2005–14.\n\n\nVanderWeele, Tyler J. 2019. “Principles of Confounder\nSelection.” European Journal of Epidemiology 34: 211–19.\n\n\nVon Korff, Michael, Edward H Wagner, and Kathleen Saunders. 1992.\n“A Chronic Disease Score from Automated Pharmacy Data.”\nJournal of Clinical Epidemiology 45 (2): 197–203.\n\n\nWeberpals, Janick, Tim Becker, Jessica Davies, Fabian Schmich, Dominik\nRüttinger, Fabian J Theis, and Anna Bauer-Mehren. 2021. “Deep\nLearning-Based Propensity Scores for Confounding Control in Comparative\nEffectiveness Research: A Large-Scale, Real-World Data Study.”\nEpidemiology 32 (3): 378–88.\n\n\nWestreich, Daniel, Stephen R Cole, Michele Jonsson Funk, M Alan\nBrookhart, and Til Stürmer. 2011. “The Role of the c-Statistic in\nVariable Selection for Propensity Score Models.”\nPharmacoepidemiology and Drug Safety 20 (3): 317–20.\n\n\nWyss, Richard, Sebastian Schneeweiss, Mark Van Der Laan, Samuel D\nLendle, Cheng Ju, and Jessica M Franklin. 2018. “Using Super\nLearner Prediction Modeling to Improve High-Dimensional Propensity Score\nEstimation.” Epidemiology 29 (1): 96–106.\n\n\nWyss, Richard, Chen Yanover, Tal El-Hay, Dimitri Bennett, Robert W\nPlatt, Andrew R Zullo, Grammati Sari, et al. 2022. “Machine\nLearning for Improving High-Dimensional Proxy Confounder Adjustment in\nHealthcare Database Studies: An Overview of the Current\nLiterature.” Pharmacoepidemiology and Drug Safety 31\n(9): 932–43."
  },
  {
    "objectID": "NHANES.html#nhanes",
    "href": "NHANES.html#nhanes",
    "title": "Appendix: NHANES",
    "section": "NHANES",
    "text": "NHANES\nThe National Health and Nutrition Examination Survey (NHANES) is a cross-sectional survey that is designed to provide nationally representative data on the health and nutritional status of the non-institutionalized, civilian US population. This survey is conducted by the National Center for Health Statistics, that provides valuable data on a wide range of health issues, such as diabetes, obesity, as well as nutrition, physical activity, and environmental exposures."
  },
  {
    "objectID": "NHANES.html#components",
    "href": "NHANES.html#components",
    "title": "Appendix: NHANES",
    "section": "Components",
    "text": "Components\nNHANES combines interviews, physical examinations, and laboratory tests to gather comprehensive health and nutrition information about participants."
  },
  {
    "objectID": "NHANES.html#design",
    "href": "NHANES.html#design",
    "title": "Appendix: NHANES",
    "section": "Design",
    "text": "Design\nThe survey selects participants using a complex sampling design, which allows researchers to make inferences about the overall health of the US population. The survey uses a complex, multistage probability sampling design to select participants from US households."
  },
  {
    "objectID": "NHANES.html#usefulness",
    "href": "NHANES.html#usefulness",
    "title": "Appendix: NHANES",
    "section": "Usefulness",
    "text": "Usefulness\nThe collected data is used by many researchers, policymakers, and public health officials to identify emerging health issues, monitor trends in health and inform public health policies and initiatives to improve health-related outcomes in the US."
  },
  {
    "objectID": "NHANES.html#cylces-we-used",
    "href": "NHANES.html#cylces-we-used",
    "title": "Appendix: NHANES",
    "section": "Cylces we used",
    "text": "Cylces we used\nNHANES is an ongoing annual survey, continuous NHANES cycles started from 1999-2000 (cycle 1; see SDDSRVYR variable in the Demographic Variables & Sample Weights component). NHANES cycles 8, 9, and 10 (that we used in this tutorial) refer to the 8th, 9th, and 10th rounds of surveys, which took place from 2013-2014, 2015-2016, and 2017-2018, respectively. Each cycle involves a nationally representative sample of the US population."
  },
  {
    "objectID": "index13.html#download-and-subsetting-to-retain-only-the-useful-variables",
    "href": "index13.html#download-and-subsetting-to-retain-only-the-useful-variables",
    "title": "20  Download cycle 8",
    "section": "20.1 Download and Subsetting to retain only the useful variables",
    "text": "20.1 Download and Subsetting to retain only the useful variables\n\n20.1.1 Demographic\nDemographic Variables and Sample Weights (DEMO_H): The 2-year sample weights (WTINT2YR, WTMEC2YR) should be used. 15 masked variance strata and 30 masked primary sampling units (PSUs) are included in the demographics file. Each stratum has 2 PSUs.\n\ndemo <- nhanes('DEMO_H')    # Both males and females 0 YEARS - 150 YEARS\ndemo1 <- demo[c(\"SEQN\",     # Respondent sequence number\n                \"RIDAGEYR\", # Age in years at screening\n                \"RIAGENDR\", # gender\n                \"DMDEDUC2\", # Education level - Adults 20+\n                \"RIDRETH1\", # race/ethnicity\n                \"DMDMARTL\", # marital status    \n                \"INDHHIN2\", # Annual household income\n                \"DMDBORN4\", # where born\n                \"RIDEXPRG\", # Pregnancy status at exam (released for 20-44 yrs)\n                \"SDDSRVYR\", # survey cycle\n                \"WTINT2YR\", # Full sample 2 year weights\n                \"WTMEC2YR\", # Full sample 2 year MEC exam weight\n                \"SDMVPSU\",  # Masked variance pseudo-PSU\n                \"SDMVSTRA\")]# Masked variance pseudo-stratum\ndemo_vars <- names(demo1) \ndemo2 <- nhanesTranslate('DEMO_H', demo_vars, data = demo1)\n#> No translation table is available for SEQN\n#> Translated columns: RIAGENDR DMDEDUC2 RIDRETH1 DMDMARTL INDHHIN2 DMDBORN4 RIDEXPRG\nsaveRDS(demo2, file = \"data/components/demo13.RData\")\n\n\n\n20.1.2 BMI\nBody Measures (BMX_H): The NHANES examination sample weights should be used to analyze the body measurement data. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\nbmx <- nhanes('BMX_H')\nbmx1 <- bmx[c(\"SEQN\", # Respondent sequence number\n              \"BMXBMI\")] # Body Mass Index (kg/m**2): 2 YEARS - 150 YEARS\nbmx_vars <- names(bmx1)\nbmx2 <- nhanesTranslate('BMX_H', bmx_vars, data = bmx1)\n#> No translation table is available for SEQN\n#> Warning in nhanesTranslate(\"BMX_H\", bmx_vars, data = bmx1): No columns were\n#> translated\nsaveRDS(bmx2, file = \"data/components/bmx13.RData\")\n\n\n\n20.1.3 Diabetes\nDiabetes (DIQ_H): diabetes questionnaire data must be conducted using the appropriate survey design variables, sample weights, and the basic demographic variables. Interview weights should only be used if questionnaire data are analyzed by themselves. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\ndiq <- nhanes('DIQ_H')\ndiq1 <- diq[c(\"SEQN\", # Respondent sequence number\n              \"DIQ010\", # Doctor told you have diabetes\n              \"DIQ050\", # Taking insulin now\n              \"DIQ070\", # Take diabetic pills to lower blood sugar\n              \"DIQ175A\")] # Family history\ndiq_vars <- names(diq1)\ndiq2 <- nhanesTranslate('DIQ_H', diq_vars, data = diq1)\n#> No translation table is available for SEQN\n#> Translated columns: DIQ010 DIQ050 DIQ070 DIQ175A\nsaveRDS(diq2, file = \"data/components/diq13.RData\")\n\n\n\n20.1.4 Smoking\nSmoking - Cigarette Use (SMQ_H): Interview weights should only be used if questionnaire data are analyzed by themselves. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\nsmq <- nhanes('SMQ_H')\nsmq1 <- smq[c(\"SEQN\", # Respondent sequence number\n              \"SMQ020\", # Smoked at least 100 cigarettes in life\n              \"SMQ040\")] # Do you now smoke cigarettes?: 18 YEARS - 150 YEARS\nsmq_vars <- names(smq1)\nsmq2 <- nhanesTranslate('SMQ_H', smq_vars, data = smq1)\n#> No translation table is available for SEQN\n#> Translated columns: SMQ020 SMQ040\nsaveRDS(smq2, file = \"data/components/smq13.RData\")\n\n\n\n20.1.5 Diet\nDiet Behavior & Nutrition (DBQ_H): interview sample weights may be used in their analysis. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\ndbq <- nhanes('DBQ_H')\ndbq1 <- dbq[c(\"SEQN\", # Respondent sequence number\n              \"DBQ700\")] # How healthy is the diet: 16 YEARS - 150 YEARS\ndbq_vars <- names(dbq1)\ndbq2 <- nhanesTranslate('DBQ_H', dbq_vars, data = dbq1)\n#> No translation table is available for SEQN\n#> Translated columns: DBQ700\nsaveRDS(dbq2, file = \"data/components/dbq13.RData\")\n\n\n\n20.1.6 Physical activity\nPhysical Activity (PAQ_H): the interview sample weights should be used in their analysis. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\npaq <- nhanes('PAQ_H')\npaq1 <- paq[c(\"SEQN\", # Respondent sequence number\n              \"PAQ605\")] # Vigorous work activity: 18 YEARS150 YEARS\npaq_vars <- names(paq1)\npaq2 <- nhanesTranslate('PAQ_H', paq_vars, data = paq1)\n#> No translation table is available for SEQN\n#> Translated columns: PAQ605\nsaveRDS(paq2, file = \"data/components/paq13.RData\")\n\n\n\n20.1.7 Access to healthcare\nHospital Utilization & Access to Care (HUQ_H): Although these data were collected as part of the household questionnaire, if they are merged with the MEC exam data, exam sample weights should be used for the analyses.\n\nhuq <- nhanes('HUQ_H')\nhuq1 <- huq[c(\"SEQN\", # Respondent sequence number\n              \"HUQ030\")] # Routine place to go for healthcare\nhuq_vars <- names(huq1)\nhuq2 <- nhanesTranslate('HUQ_H', huq_vars, data = huq1)\n#> No translation table is available for SEQN\n#> Translated columns: HUQ030\nsaveRDS(huq2, file = \"data/components/huq13.RData\")\n\n\n\n20.1.8 Blood pressure\nBlood Pressure (BPX_H): Exam sample weights should be used for analyses.\n\nSystolic blood pressure and maximum inflation level cannot be greater than 300 mmHg;\nSystolic and diastolic blood pressure measurements and the maximum inflation level can be even numbers only;\nSystolic blood pressure must be greater than diastolic blood pressure;\nIf there is no systolic blood pressure, there can be no diastolic blood pressure. (There can be a systolic measurement without a diastolic measurement.); and\nDiastolic blood pressure can be zero.\n\n\nbpx <- nhanes('BPX_H')\nbpx1 <- bpx[c(\"SEQN\", # Respondent sequence number\n              \"BPXSY1\", # Systolic Blood pres (1st rdg) mmHg: 8 - 150 YEARS\n              \"BPXSY2\", # Systolic: Blood pres (2nd rdg) mm Hg\n              \"BPXSY3\", # Systolic: Blood pres (3rd rdg) mm Hg\n              \"BPXSY4\", # Systolic: Blood pres (4th rdg) mm Hg\n              \"BPXDI1\", # Diastolic Blood pres (1st rdg) mmHg: 8 - 150 YEARS\n              \"BPXDI2\", # Diastolic: Blood pres (2nd rdg) mm Hg\n              \"BPXDI3\", # Diastolic: Blood pres (3rd rdg) mm Hg\n              \"BPXDI4\")] # Diastolic: Blood pres (4th rdg) mm Hg\nbpx_vars <- names(bpx1)\nbpx2 <- nhanesTranslate('BPX_H', bpx_vars, data = bpx1)\n#> No translation table is available for SEQN\n#> Warning in nhanesTranslate(\"BPX_H\", bpx_vars, data = bpx1): No columns were\n#> translated\nsaveRDS(bpx2, file = \"data/components/bpx13.RData\")\n\nBlood Pressure & Cholesterol (BPQ_H): Although these data were collected as part of the household questionnaire, if they are merged with the MEC exam data, exam sample weights should be used for the analyses.\n\nbpq <- nhanes('BPQ_H')\nbpq1 <- bpq[c(\"SEQN\", # Respondent sequence number\n              \"BPQ080\")] # high cholesterol\nbpq_vars <- names(bpq1)\nbpq2 <- nhanesTranslate('BPQ_H', bpq_vars, data = bpq1)\n#> No translation table is available for SEQN\n#> Translated columns: BPQ080\nsaveRDS(bpq2, file = \"data/components/bpq13.RData\")\n\n\n\n20.1.9 Sleep\nSleep Disorders (SLQ_H):\n\nslq <- nhanes('SLQ_H')\nslq1 <- slq[c(\"SEQN\", # Respondent sequence number\n              \"SLD010H\")] # Sleep hours \nslq_vars <- names(slq1)\nslq2 <- nhanesTranslate('SLQ_H', slq_vars, data = slq1)\n#> No translation table is available for SEQN\n#> Warning in nhanesTranslate(\"SLQ_H\", slq_vars, data = slq1): No columns were\n#> translated\nsaveRDS(slq2, file = \"data/components/slq13.RData\")\n\n\n\n20.1.10 Laboratory data\nStandard Biochemistry Profile (BIOPRO_H): Exam sample weights should be used for analyses.\n\n# Standard Biochemistry Profile\nbiopro <- nhanes('BIOPRO_H') # 12 YEARS - 150 YEARS\nbiopro1 <- biopro[c(\"SEQN\", # Respondent sequence number\n                    #\"LBXSTR\", # Triglycerides, refrigerated (mg/dL)\n                    \"LBXSUA\", # Uric acid (mg/dL)\n                    \"LBXSTP\", # Total protein (g/dL)\n                    \"LBXSTB\", # Total bilirubin (mg/dL)\n                    \"LBXSPH\", # Phosphorus (mg/dL)\n                    \"LBXSNASI\", # Sodium (mmol/L)\n                    \"LBXSKSI\", # Potassium (mmol/L)\n                    \"LBXSGB\", # Globulin (g/dL)\n                    \"LBXSCA\")] # Total Calcium (mg/dL)\nbiopro_vars <- names(biopro1) \nbiopro2 <- nhanesTranslate('BIOPRO_H', biopro_vars, data = biopro1)\n#> No translation table is available for SEQN\n#> Warning in nhanesTranslate(\"BIOPRO_H\", biopro_vars, data = biopro1): No columns\n#> were translated\nsaveRDS(biopro2, file = \"data/components/biopro13.RData\")\n\n\n\n20.1.11 ICD-10-CM codes\nPrescription Medications (RXQ_RX_H): The Prescription Medications subsection provides personal interview data on use of prescription medications during a one-month period prior to the participant’s interview date. During the household SP interview, survey participants are asked if they have taken medications in the past 30 days for which they needed a prescription. Those who answer “yes” are asked to show the interviewer the medication containers of all the products used.\n\nrxq <- nhanes('RXQ_RX_H')\nrxq10 <- rxq[c(\"SEQN\", # Respondent sequence number\n               \"RXDRSC1\")] # ICD-10-CM code 1\nrxq11 <- names(rxq10) \nrxq12 <- nhanesTranslate('RXQ_RX_H', rxq11, data = rxq10)\n#> No translation table is available for SEQN\n#> Translated columns: RXDRSC1\n\nrxq20 <- rxq[c(\"SEQN\", # Respondent sequence number\n               \"RXDRSC2\")] # ICD-10-CM code 2\nrxq21 <- names(rxq20) \nrxq22 <- nhanesTranslate('RXQ_RX_H', rxq21, data = rxq20)\n#> No translation table is available for SEQN\n#> Translated columns: RXDRSC2\n\nrxq30 <- rxq[c(\"SEQN\", # Respondent sequence number\n               \"RXDRSC3\")] # ICD-10-CM code 3\nrxq31 <- names(rxq30) \nrxq32 <- nhanesTranslate('RXQ_RX_H', rxq31, data = rxq30)\n#> No translation table is available for SEQN\n#> Translated columns: RXDRSC3\n\nsaveRDS(rxq12, file = \"data/components/rxq1213.RData\")\nsaveRDS(rxq22, file = \"data/components/rxq2213.RData\")\nsaveRDS(rxq32, file = \"data/components/rxq3213.RData\")"
  },
  {
    "objectID": "index13.html#merging-all-the-datasets---except-for-icd-10-codes",
    "href": "index13.html#merging-all-the-datasets---except-for-icd-10-codes",
    "title": "20  Download cycle 8",
    "section": "20.2 Merging all the datasets - except for ICD-10 codes",
    "text": "20.2 Merging all the datasets - except for ICD-10 codes\n\ndat <- join_all(list(demo2, bmx2, diq2, smq2, dbq2, paq2, \n                     huq2, bpx2, bpq2, slq2, biopro2),\n                by = \"SEQN\", type='full')\nnhanes13 <- dat\n\n\n20.2.1 Save dataset for later use\n\ndim(nhanes13)\n#> [1] 10175    42\nsave(nhanes13, rxq12, rxq22, rxq32, file = \"data/analytic13.RData\")"
  },
  {
    "objectID": "index15.html#download-and-subsetting-to-retain-only-the-useful-variables",
    "href": "index15.html#download-and-subsetting-to-retain-only-the-useful-variables",
    "title": "21  Download cycle 9",
    "section": "21.1 Download and Subsetting to retain only the useful variables",
    "text": "21.1 Download and Subsetting to retain only the useful variables\n\n21.1.1 Demographic\nDemographic Variables and Sample Weights (DEMO_H): The 2-year sample weights (WTINT2YR, WTMEC2YR) should be used. 15 masked variance strata and 30 masked primary sampling units (PSUs) are included in the demographics file. Each stratum has 2 PSUs.\n\ndemo <- nhanes('DEMO_I')    # Both males and females 0 YEARS - 150 YEARS\ndemo1 <- demo[c(\"SEQN\",     # Respondent sequence number\n                \"RIDAGEYR\", # Age in years at screening\n                \"RIAGENDR\", # gender\n                \"DMDEDUC2\", # Education level - Adults 20+\n                \"RIDRETH1\", # race/ethnicity\n                \"DMDMARTL\", # marital status    \n                \"INDHHIN2\", # Annual household income\n                \"DMDBORN4\", # where born\n                \"RIDEXPRG\", # Pregnancy status at exam (released for 20-44 yrs)\n                \"SDDSRVYR\", # survey cycle\n                \"WTINT2YR\", # Full sample 2 year weights\n                \"WTMEC2YR\", # Full sample 2 year MEC exam weight\n                \"SDMVPSU\",  # Masked variance pseudo-PSU\n                \"SDMVSTRA\")]# Masked variance pseudo-stratum\ndemo_vars <- names(demo1) \ndemo2 <- nhanesTranslate('DEMO_I', demo_vars, data = demo1)\n#> No translation table is available for SEQN\n#> Translated columns: RIAGENDR DMDEDUC2 RIDRETH1 DMDMARTL INDHHIN2 DMDBORN4 RIDEXPRG\nsaveRDS(demo2, file = \"data/components/demo15.RData\")\n\n\n\n21.1.2 BMI\nBody Measures (BMX_H): The NHANES examination sample weights should be used to analyze the body measurement data. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\nbmx <- nhanes('BMX_I')\nbmx1 <- bmx[c(\"SEQN\", # Respondent sequence number\n              \"BMXBMI\")] # Body Mass Index (kg/m**2): 2 YEARS - 150 YEARS\nbmx_vars <- names(bmx1)\nbmx2 <- nhanesTranslate('BMX_I', bmx_vars, data = bmx1)\n#> No translation table is available for SEQN\n#> Warning in nhanesTranslate(\"BMX_I\", bmx_vars, data = bmx1): No columns were\n#> translated\nsaveRDS(bmx2, file = \"data/components/bmx15.RData\")\n\n\n\n21.1.3 Diabetes\nDiabetes (DIQ_H): diabetes questionnaire data must be conducted using the appropriate survey design variables, sample weights, and the basic demographic variables. Interview weights should only be used if questionnaire data are analyzed by themselves. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\ndiq <- nhanes('DIQ_I')\ndiq1 <- diq[c(\"SEQN\", # Respondent sequence number\n              \"DIQ010\", # Doctor told you have diabetes \n              \"DIQ050\", # Taking insulin now\n              \"DIQ070\", # Take diabetic pills to lower blood sugar\n              \"DIQ175A\")] # Family history\ndiq_vars <- names(diq1)\ndiq2 <- nhanesTranslate('DIQ_I', diq_vars, data = diq1)\n#> No translation table is available for SEQN\n#> Translated columns: DIQ010 DIQ050 DIQ070\nsaveRDS(diq2, file = \"data/components/diq15.RData\")\n\n\n\n21.1.4 Smoking\nSmoking - Cigarette Use (SMQ_H): Interview weights should only be used if questionnaire data are analyzed by themselves. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\nsmq <- nhanes('SMQ_I')\nsmq1 <- smq[c(\"SEQN\", # Respondent sequence number\n              \"SMQ020\", # Smoked at least 100 cigarettes in life\n              \"SMQ040\")] # Do you now smoke cigarettes?: 18 YEARS - 150 YEARS\nsmq_vars <- names(smq1)\nsmq2 <- nhanesTranslate('SMQ_I', smq_vars, data = smq1)\n#> No translation table is available for SEQN\n#> Translated columns: SMQ020 SMQ040\nsaveRDS(smq2, file = \"data/components/smq15.RData\")\n\n\n\n21.1.5 Diet\nDiet Behavior & Nutrition (DBQ_H): interview sample weights may be used in their analysis. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\ndbq <- nhanes('DBQ_I')\ndbq1 <- dbq[c(\"SEQN\", # Respondent sequence number\n              \"DBQ700\")] # How healthy is the diet: 16 YEARS - 150 YEARS\ndbq_vars <- names(dbq1)\ndbq2 <- nhanesTranslate('DBQ_I', dbq_vars, data = dbq1)\n#> No translation table is available for SEQN\n#> Translated columns: DBQ700\nsaveRDS(dbq2, file = \"data/components/dbq15.RData\")\n\n\n\n21.1.6 Physical activity\nPhysical Activity (PAQ_H): the interview sample weights should be used in their analysis. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\npaq <- nhanes('PAQ_I')\npaq1 <- paq[c(\"SEQN\", # Respondent sequence number\n              \"PAQ605\")] # Vigorous work activity: 18 YEARS150 YEARS\npaq_vars <- names(paq1)\npaq2 <- nhanesTranslate('PAQ_I', paq_vars, data = paq1)\n#> No translation table is available for SEQN\n#> Translated columns: PAQ605\nsaveRDS(paq2, file = \"data/components/paq15.RData\")\n\n\n\n21.1.7 Access to healthcare\nHospital Utilization & Access to Care (HUQ_H): Although these data were collected as part of the household questionnaire, if they are merged with the MEC exam data, exam sample weights should be used for the analyses.\n\nhuq <- nhanes('HUQ_I')\nhuq1 <- huq[c(\"SEQN\", # Respondent sequence number\n              \"HUQ030\")] # Routine place to go for healthcare\nhuq_vars <- names(huq1)\nhuq2 <- nhanesTranslate('HUQ_I', huq_vars, data = huq1)\n#> No translation table is available for SEQN\n#> Translated columns: HUQ030\nsaveRDS(huq2, file = \"data/components/huq15.RData\")\n\n\n\n21.1.8 Blood pressure\nBlood Pressure (BPX_H): Exam sample weights should be used for analyses.\n\nSystolic blood pressure and maximum inflation level cannot be greater than 300 mmHg;\nSystolic and diastolic blood pressure measurements and the maximum inflation level can be even numbers only;\nSystolic blood pressure must be greater than diastolic blood pressure;\nIf there is no systolic blood pressure, there can be no diastolic blood pressure. (There can be a systolic measurement without a diastolic measurement.); and\nDiastolic blood pressure can be zero.\n\n\nbpx <- nhanes('BPX_I')\nbpx1 <- bpx[c(\"SEQN\", # Respondent sequence number\n              \"BPXSY1\", # Systolic Blood pres (1st rdg) mmHg: 8 - 150 YEARS\n              \"BPXSY2\", # Systolic: Blood pres (2nd rdg) mm Hg\n              \"BPXSY3\", # Systolic: Blood pres (3rd rdg) mm Hg\n              \"BPXSY4\", # Systolic: Blood pres (4th rdg) mm Hg\n              \"BPXDI1\", # Diastolic Blood pres (1st rdg) mmHg: 8 - 150 YEARS\n              \"BPXDI2\", # Diastolic: Blood pres (2nd rdg) mm Hg\n              \"BPXDI3\", # Diastolic: Blood pres (3rd rdg) mm Hg\n              \"BPXDI4\")] # Diastolic: Blood pres (4th rdg) mm Hg\nbpx_vars <- names(bpx1)\nbpx2 <- nhanesTranslate('BPX_I', bpx_vars, data = bpx1)\n#> No translation table is available for SEQN\n#> Warning in nhanesTranslate(\"BPX_I\", bpx_vars, data = bpx1): No columns were\n#> translated\nsaveRDS(bpx2, file = \"data/components/bpx15.RData\")\n\nBlood Pressure & Cholesterol (BPQ_H): Although these data were collected as part of the household questionnaire, if they are merged with the MEC exam data, exam sample weights should be used for the analyses.\n\nbpq <- nhanes('BPQ_I')\nbpq1 <- bpq[c(\"SEQN\", # Respondent sequence number\n              \"BPQ080\")] # high cholesterol\nbpq_vars <- names(bpq1)\nbpq2 <- nhanesTranslate('BPQ_I', bpq_vars, data = bpq1)\n#> No translation table is available for SEQN\n#> Translated columns: BPQ080\nsaveRDS(bpq2, file = \"data/components/bpq15.RData\")\n\n\n\n21.1.9 Sleep\nSleep Disorders (SLQ_H):\n\nslq <- nhanes('SLQ_I')\nslq1 <- slq[c(\"SEQN\", # Respondent sequence number\n              \"SLD012\")] # Sleep hours - weekdays or workdays\nslq_vars <- names(slq1)\nslq2 <- nhanesTranslate('SLQ_I', slq_vars, data = slq1)\n#> No translation table is available for SEQN\n#> Warning in nhanesTranslate(\"SLQ_I\", slq_vars, data = slq1): No columns were\n#> translated\nsaveRDS(slq2, file = \"data/components/slq15.RData\")\n\n\n\n21.1.10 Laboratory data\nStandard Biochemistry Profile (BIOPRO_H): Exam sample weights should be used for analyses.\n\n# Standard Biochemistry Profile\nbiopro <- nhanes('BIOPRO_I') # 12 YEARS - 150 YEARS\nbiopro1 <- biopro[c(\"SEQN\", # Respondent sequence number\n                    #\"LBXSTR\", # Triglycerides, refrigerated (mg/dL)\n                    \"LBXSUA\", # Uric acid (mg/dL)\n                    \"LBXSTP\", # Total protein (g/dL)\n                    \"LBXSTB\", # Total bilirubin (mg/dL)\n                    \"LBXSPH\", # Phosphorus (mg/dL)\n                    \"LBXSNASI\", # Sodium (mmol/L)\n                    \"LBXSKSI\", # Potassium (mmol/L)\n                    \"LBXSGB\", # Globulin (g/dL)\n                    \"LBXSCA\")] # Total Calcium (mg/dL)\nbiopro_vars <- names(biopro1) \nbiopro2 <- nhanesTranslate('BIOPRO_I', biopro_vars, data = biopro1)\n#> No translation table is available for SEQN\n#> Warning in nhanesTranslate(\"BIOPRO_I\", biopro_vars, data = biopro1): No columns\n#> were translated\nsaveRDS(biopro2, file = \"data/components/biopro15.RData\")\n\n\n\n21.1.11 ICD-10-CM codes\nPrescription Medications (RXQ_RX_H): The Prescription Medications subsection provides personal interview data on use of prescription medications during a one-month period prior to the participant’s interview date. During the household SP interview, survey participants are asked if they have taken medications in the past 30 days for which they needed a prescription. Those who answer “yes” are asked to show the interviewer the medication containers of all the products used.\n\nrxq <- nhanes('RXQ_RX_I')\nrxq10 <- rxq[c(\"SEQN\", # Respondent sequence number\n               \"RXDRSC1\")] # ICD-10-CM code 1\nrxq11 <- names(rxq10) \nrxq12 <- nhanesTranslate('RXQ_RX_I', rxq11, data = rxq10)\n#> No translation table is available for SEQN\n#> Translated columns: RXDRSC1\n\nrxq20 <- rxq[c(\"SEQN\", # Respondent sequence number\n               \"RXDRSC2\")] # ICD-10-CM code 2\nrxq21 <- names(rxq20) \nrxq22 <- nhanesTranslate('RXQ_RX_I', rxq21, data = rxq20)\n#> No translation table is available for SEQN\n#> Translated columns: RXDRSC2\n\nrxq30 <- rxq[c(\"SEQN\", # Respondent sequence number\n               \"RXDRSC3\")] # ICD-10-CM code 3\nrxq31 <- names(rxq30) \nrxq32 <- nhanesTranslate('RXQ_RX_I', rxq31, data = rxq30)\n#> No translation table is available for SEQN\n#> Translated columns: RXDRSC3\n\nsaveRDS(rxq12, file = \"data/components/rxq1215.RData\")\nsaveRDS(rxq22, file = \"data/components/rxq2215.RData\")\nsaveRDS(rxq32, file = \"data/components/rxq3215.RData\")"
  },
  {
    "objectID": "index15.html#merging-all-the-datasets---except-for-icd-10-codes",
    "href": "index15.html#merging-all-the-datasets---except-for-icd-10-codes",
    "title": "21  Download cycle 9",
    "section": "21.2 Merging all the datasets - except for ICD-10 codes",
    "text": "21.2 Merging all the datasets - except for ICD-10 codes\n\ndat <- join_all(list(demo2, bmx2, diq2, smq2, dbq2, paq2, \n                     huq2, bpx2, bpq2, slq2, biopro2),\n                by = \"SEQN\", type='full')\nnhanes15 <- dat \n\n\n21.2.1 Save dataset for later use\n\ndim(nhanes15)\n#> [1] 9971   42\nsave(nhanes15, rxq12, rxq22, rxq32, file = \"data/analytic15.RData\")"
  },
  {
    "objectID": "index17.html#download-and-subsetting-to-retain-only-the-useful-variables",
    "href": "index17.html#download-and-subsetting-to-retain-only-the-useful-variables",
    "title": "22  Download cycle 10",
    "section": "22.1 Download and Subsetting to retain only the useful variables",
    "text": "22.1 Download and Subsetting to retain only the useful variables\n\n22.1.1 Demographic\nDemographic Variables and Sample Weights (DEMO_H): The 2-year sample weights (WTINT2YR, WTMEC2YR) should be used. 15 masked variance strata and 30 masked primary sampling units (PSUs) are included in the demographics file. Each stratum has 2 PSUs.\n\ndemo <- nhanes('DEMO_J')    # Both males and females 0 YEARS - 150 YEARS\ndemo1 <- demo[c(\"SEQN\",     # Respondent sequence number\n                \"RIDAGEYR\", # Age in years at screening\n                \"RIAGENDR\", # gender\n                \"DMDEDUC2\", # Education level - Adults 20+\n                \"RIDRETH1\", # race/ethnicity\n                \"DMDMARTL\", # marital status    \n                \"INDHHIN2\", # Annual household income\n                \"DMDBORN4\", # where born\n                \"RIDEXPRG\", # Pregnancy status at exam (released for 20-44 yrs)\n                \"SDDSRVYR\", # survey cycle\n                \"WTINT2YR\", # Full sample 2 year weights\n                \"WTMEC2YR\", # Full sample 2 year MEC exam weight\n                \"SDMVPSU\",  # Masked variance pseudo-PSU\n                \"SDMVSTRA\")]# Masked variance pseudo-stratum\ndemo_vars <- names(demo1) \ndemo2 <- nhanesTranslate('DEMO_J', demo_vars, data = demo1)\n#> No translation table is available for SEQN\n#> Translated columns: RIAGENDR DMDEDUC2 RIDRETH1 DMDMARTL INDHHIN2 DMDBORN4 RIDEXPRG\nsaveRDS(demo2, file = \"data/components/demo17.RData\")\n\n\n\n22.1.2 BMI\nBody Measures (BMX_H): The NHANES examination sample weights should be used to analyze the body measurement data. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\nbmx <- nhanes('BMX_J')\nbmx1 <- bmx[c(\"SEQN\", # Respondent sequence number\n              \"BMXBMI\")] # Body Mass Index (kg/m**2): 2 YEARS - 150 YEARS\nbmx_vars <- names(bmx1)\nbmx2 <- nhanesTranslate('BMX_J', bmx_vars, data = bmx1)\n#> No translation table is available for SEQN\n#> Warning in nhanesTranslate(\"BMX_J\", bmx_vars, data = bmx1): No columns were\n#> translated\nsaveRDS(bmx2, file = \"data/components/bmx17.RData\")\n\n\n\n22.1.3 Diabetes\nDiabetes (DIQ_H): diabetes questionnaire data must be conducted using the appropriate survey design variables, sample weights, and the basic demographic variables. Interview weights should only be used if questionnaire data are analyzed by themselves. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\ndiq <- nhanes('DIQ_J')\ndiq1 <- diq[c(\"SEQN\", # Respondent sequence number\n              \"DIQ010\", # Doctor told you have diabetes\n              \"DIQ050\", # Taking insulin now\n              \"DIQ070\", # Take diabetic pills to lower blood sugar\n              \"DIQ175A\")] # Family history\ndiq_vars <- names(diq1)\ndiq2 <- nhanesTranslate('DIQ_J', diq_vars, data = diq1)\n#> No translation table is available for SEQN\n#> Translated columns: DIQ010 DIQ050 DIQ070 DIQ175A\nsaveRDS(diq2, file = \"data/components/diq17.RData\")\n\n\n\n22.1.4 Smoking\nSmoking - Cigarette Use (SMQ_H): Interview weights should only be used if questionnaire data are analyzed by themselves. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\nsmq <- nhanes('SMQ_J')\nsmq1 <- smq[c(\"SEQN\", # Respondent sequence number\n              \"SMQ020\", # Smoked at least 100 cigarettes in life\n              \"SMQ040\")] # Do you now smoke cigarettes?: 18 YEARS - 150 YEARS\nsmq_vars <- names(smq1)\nsmq2 <- nhanesTranslate('SMQ_J', smq_vars, data = smq1)\n#> No translation table is available for SEQN\n#> Translated columns: SMQ020 SMQ040\nsaveRDS(smq2, file = \"data/components/smq17.RData\")\n\n\n\n22.1.5 Diet\nDiet Behavior & Nutrition (DBQ_H): interview sample weights may be used in their analysis. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\ndbq <- nhanes('DBQ_J')\ndbq1 <- dbq[c(\"SEQN\", # Respondent sequence number\n              \"DBQ700\")] # How healthy is the diet: 16 YEARS - 150 YEARS\ndbq_vars <- names(dbq1)\ndbq2 <- nhanesTranslate('DBQ_J', dbq_vars, data = dbq1)\n#> No translation table is available for SEQN\n#> Translated columns: DBQ700\nsaveRDS(dbq2, file = \"data/components/dbq17.RData\")\n\n\n\n22.1.6 Physical activity\nPhysical Activity (PAQ_H): the interview sample weights should be used in their analysis. However, if the data is joined with data from the MEC, the MEC sample weights should be used.\n\npaq <- nhanes('PAQ_J')\npaq1 <- paq[c(\"SEQN\", # Respondent sequence number\n              \"PAQ605\")] # Vigorous work activity: 18 YEARS150 YEARS\npaq_vars <- names(paq1)\npaq2 <- nhanesTranslate('PAQ_J', paq_vars, data = paq1)\n#> No translation table is available for SEQN\n#> Translated columns: PAQ605\nsaveRDS(paq2, file = \"data/components/paq17.RData\")\n\n\n\n22.1.7 Access to healthcare\nHospital Utilization & Access to Care (HUQ_H): Although these data were collected as part of the household questionnaire, if they are merged with the MEC exam data, exam sample weights should be used for the analyses.\n\nhuq <- nhanes('HUQ_J')\nhuq1 <- huq[c(\"SEQN\", # Respondent sequence number\n              \"HUQ030\")] # Routine place to go for healthcare\nhuq_vars <- names(huq1)\nhuq2 <- nhanesTranslate('HUQ_J', huq_vars, data = huq1)\n#> No translation table is available for SEQN\n#> Translated columns: HUQ030\nsaveRDS(huq2, file = \"data/components/huq17.RData\")\n\n\n\n22.1.8 Blood pressure\nBlood Pressure (BPX_H): Exam sample weights should be used for analyses.\n\nSystolic blood pressure and maximum inflation level cannot be greater than 300 mmHg;\nSystolic and diastolic blood pressure measurements and the maximum inflation level can be even numbers only;\nSystolic blood pressure must be greater than diastolic blood pressure;\nIf there is no systolic blood pressure, there can be no diastolic blood pressure. (There can be a systolic measurement without a diastolic measurement.); and\nDiastolic blood pressure can be zero.\n\n\nbpx <- nhanes('BPX_J')\nbpx1 <- bpx[c(\"SEQN\", # Respondent sequence number\n              \"BPXSY1\", # Systolic Blood pres (1st rdg) mmHg: 8 - 150 YEARS\n              \"BPXSY2\", # Systolic: Blood pres (2nd rdg) mm Hg\n              \"BPXSY3\", # Systolic: Blood pres (3rd rdg) mm Hg\n              \"BPXSY4\", # Systolic: Blood pres (4th rdg) mm Hg\n              \"BPXDI1\", # Diastolic Blood pres (1st rdg) mmHg: 8 - 150 YEARS\n              \"BPXDI2\", # Diastolic: Blood pres (2nd rdg) mm Hg\n              \"BPXDI3\", # Diastolic: Blood pres (3rd rdg) mm Hg\n              \"BPXDI4\")] # Diastolic: Blood pres (4th rdg) mm Hg\nbpx_vars <- names(bpx1)\nbpx2 <- nhanesTranslate('BPX_J', bpx_vars, data = bpx1)\n#> No translation table is available for SEQN\n#> Warning in nhanesTranslate(\"BPX_J\", bpx_vars, data = bpx1): No columns were\n#> translated\nsaveRDS(bpx2, file = \"data/components/bpx17.RData\")\n\nBlood Pressure & Cholesterol (BPQ_H): Although these data were collected as part of the household questionnaire, if they are merged with the MEC exam data, exam sample weights should be used for the analyses.\n\nbpq <- nhanes('BPQ_J')\nbpq1 <- bpq[c(\"SEQN\", # Respondent sequence number\n              \"BPQ080\")] # high cholesterol\nbpq_vars <- names(bpq1)\nbpq2 <- nhanesTranslate('BPQ_J', bpq_vars, data = bpq1)\n#> No translation table is available for SEQN\n#> Translated columns: BPQ080\nsaveRDS(bpq2, file = \"data/components/bpq17.RData\")\n\n\n\n22.1.9 Sleep\nSleep Disorders (SLQ_H):\n\nslq <- nhanes('SLQ_J')\nslq1 <- slq[c(\"SEQN\", # Respondent sequence number\n              \"SLD012\")] # Sleep hours - weekdays or workdays\nslq_vars <- names(slq1)\nslq2 <- nhanesTranslate('SLQ_J', slq_vars, data = slq1)\n#> No translation table is available for SEQN\n#> Warning in nhanesTranslate(\"SLQ_J\", slq_vars, data = slq1): No columns were\n#> translated\nsaveRDS(slq2, file = \"data/components/slq17.RData\")\n\n\n\n22.1.10 Laboratory data\nStandard Biochemistry Profile (BIOPRO_H): Exam sample weights should be used for analyses.\n\n# Standard Biochemistry Profile\nbiopro <- nhanes('BIOPRO_J') # 12 YEARS - 150 YEARS\nbiopro1 <- biopro[c(\"SEQN\", # Respondent sequence number\n                    #\"LBXSTR\", # Triglycerides, refrigerated (mg/dL)\n                    \"LBXSUA\", # Uric acid (mg/dL)\n                    \"LBXSTP\", # Total protein (g/dL)\n                    \"LBXSTB\", # Total bilirubin (mg/dL)\n                    \"LBXSPH\", # Phosphorus (mg/dL)\n                    \"LBXSNASI\", # Sodium (mmol/L)\n                    \"LBXSKSI\", # Potassium (mmol/L)\n                    \"LBXSGB\", # Globulin (g/dL)\n                    \"LBXSCA\")] # Total Calcium (mg/dL)\nbiopro_vars <- names(biopro1) \nbiopro2 <- nhanesTranslate('BIOPRO_J', biopro_vars, data = biopro1)\n#> No translation table is available for SEQN\n#> Warning in nhanesTranslate(\"BIOPRO_J\", biopro_vars, data = biopro1): No columns\n#> were translated\nsaveRDS(biopro2, file = \"data/components/biopro17.RData\")\n\n\n\n22.1.11 ICD-10-CM codes\nPrescription Medications (RXQ_RX_H): The Prescription Medications subsection provides personal interview data on use of prescription medications during a one-month period prior to the participant’s interview date. During the household SP interview, survey participants are asked if they have taken medications in the past 30 days for which they needed a prescription. Those who answer “yes” are asked to show the interviewer the medication containers of all the products used.\n\nrxq <- nhanes('RXQ_RX_J')\nrxq10 <- rxq[c(\"SEQN\", # Respondent sequence number\n               \"RXDRSC1\")] # ICD-10-CM code 1\nrxq11 <- names(rxq10) \nrxq12 <- nhanesTranslate('RXQ_RX_J', rxq11, data = rxq10)\n#> No translation table is available for SEQN\n#> Translated columns: RXDRSC1\n\nrxq20 <- rxq[c(\"SEQN\", # Respondent sequence number\n               \"RXDRSC2\")] # ICD-10-CM code 2\nrxq21 <- names(rxq20) \nrxq22 <- nhanesTranslate('RXQ_RX_J', rxq21, data = rxq20)\n#> No translation table is available for SEQN\n#> Translated columns: RXDRSC2\n\nrxq30 <- rxq[c(\"SEQN\", # Respondent sequence number\n               \"RXDRSC3\")] # ICD-10-CM code 3\nrxq31 <- names(rxq30) \nrxq32 <- nhanesTranslate('RXQ_RX_J', rxq31, data = rxq30)\n#> No translation table is available for SEQN\n#> Translated columns: RXDRSC3\n\nsaveRDS(rxq12, file = \"data/components/rxq1217.RData\")\nsaveRDS(rxq22, file = \"data/components/rxq2217.RData\")\nsaveRDS(rxq32, file = \"data/components/rxq3217.RData\")"
  },
  {
    "objectID": "index17.html#merging-all-the-datasets---except-for-icd-10-codes",
    "href": "index17.html#merging-all-the-datasets---except-for-icd-10-codes",
    "title": "22  Download cycle 10",
    "section": "22.2 Merging all the datasets - except for ICD-10 codes",
    "text": "22.2 Merging all the datasets - except for ICD-10 codes\n\ndat <- join_all(list(demo2, bmx2, diq2, smq2, dbq2, paq2, \n                     huq2, bpx2, bpq2, slq2, biopro2),\n                by = \"SEQN\", type='full')\nnhanes17 <- dat\n\n\n22.2.1 Save dataset for later use\n\ndim(nhanes17)\n#> [1] 9254   42\nsave(nhanes17, rxq12, rxq22, rxq32, file = \"data/analytic17.RData\")"
  },
  {
    "objectID": "analytic13.html#load-downloaded-dataset",
    "href": "analytic13.html#load-downloaded-dataset",
    "title": "23  Recoding cycle 8",
    "section": "23.1 Load downloaded dataset",
    "text": "23.1 Load downloaded dataset\n\nload(file = \"data/analytic13.RData\")"
  },
  {
    "objectID": "analytic13.html#recoding",
    "href": "analytic13.html#recoding",
    "title": "23  Recoding cycle 8",
    "section": "23.2 Recoding",
    "text": "23.2 Recoding\n\n23.2.1 ID\n\ndat2 <- nhanes13\ndat2$id <- dat2$SEQN\n\n\n\n23.2.2 Demographic\n\n23.2.2.1 Age\n\ndat2$age <- dat2$RIDAGEYR\ndat2$age.cat <- car::recode(dat2$age, \" 0:19 = '<20'; 20:49 = '20-49'; 50:64 = '50-64'; \n                            65:80 = '65+'; else = NA \")\ndat2$age.cat <- factor(dat2$age.cat, levels = c(\"<20\", \"20-49\", \"50-64\", \"65+\"))\ntable(dat2$age.cat, useNA = \"always\")\n#> \n#>   <20 20-49 50-64   65+  <NA> \n#>  4406  2989  1474  1306     0\n\n\n\n23.2.2.2 Sex\n\ndat2$sex <- dat2$RIAGENDR\ntable(dat2$sex, useNA = \"always\")\n#> \n#>   Male Female   <NA> \n#>   5003   5172      0\n\n\n\n23.2.2.3 Education\n\ndat2$education <- dat2$DMDEDUC2\ndat2$education <- as.factor(dat2$education)\ndat2$education <- car::recode(dat2$education, recodes = \" c('College graduate or above') = \n'College graduate or above'; c('Some college or AA degree', 'High school graduate/GED or equi') = \n'High school'; c('Less than 9th grade', '9-11th grade (Includes 12th grad') = \n'Less than high school'; else = NA \")\ndat2$education <- factor(dat2$education, \n                         levels = c(\"Less than high school\", \"High school\", \n                                    \"College graduate or above\"))\ntable(dat2$education, useNA = \"always\")\n#> \n#>     Less than high school               High school College graduate or above \n#>                      1246                      3073                      1443 \n#>                      <NA> \n#>                      4413\n\n\n\n23.2.2.4 Race/ethnicity\n\ndat2$race <- dat2$RIDRETH1\ndat2$race <- car::recode(dat2$race, recodes = \" 'Non-Hispanic White'='White';\n                    'Non-Hispanic Black'='Black'; c('Mexican American',\n                    'Other Hispanic')= 'Hispanic'; else='Others' \")\ndat2$race <- factor(dat2$race, levels = c(\"White\", \"Black\", \"Hispanic\", \"Others\"))\ntable(dat2$race, useNA = \"always\")\n#> \n#>    White    Black Hispanic   Others     <NA> \n#>     3674     2267     2690     1544        0\n\n\n\n23.2.2.5 Marital status\n\ndat2$marital <- dat2$DMDMARTL\ndat2$marital <- car::recode(dat2$marital, recodes = \" 'Never married'='Never married';\nc('Married', 'Living with partner') = 'Married/with partner'; \n                            c('Widowed', 'Divorced', 'Separated')='Other'; else=NA \")\ndat2$marital <- factor(dat2$marital, levels = c(\"Never married\", \"Married/with partner\",\n                                                \"Other\"))\ntable(dat2$marital, useNA = \"always\")\n#> \n#>        Never married Married/with partner                Other \n#>                 1112                 3382                 1272 \n#>                 <NA> \n#>                 4409\n\n\n\n23.2.2.6 Income\n\ndat2$income <- dat2$INDHHIN2\ndat2$income  <- car::recode(dat2$income, recodes = \" c('$ 0 to $ 4,999', '$ 5,000 to $ 9,999',\n'$10,000 to $14,999', '$15,000 to $19,999', 'Under $20,000')='less than $20,000';\n                       c('Over $20,000','$20,000 and Over', '$20,000 to $24,999', \n                       '$25,000 to $34,999', '$35,000 to $44,999', '$45,000 to $54,999', \n                       '$55,000 to $64,999', '$65,000 to $74,999')='$20,000 to $74,999';\n                       c('$75,000 to $99,999','$100,000 and Over')='$75,000 and Over'; \n                            else=NA \")\ndat2$income  <- factor(dat2$income , levels=c(\"less than $20,000\", \"$20,000 to $74,999\", \n                                              \"$75,000 and Over\"))\ntable(dat2$income, useNA = \"always\")\n#> \n#>  less than $20,000 $20,000 to $74,999   $75,000 and Over               <NA> \n#>               2110               4964               2641                460\n\n\n\n23.2.2.7 Where born / citizenship\n\ndat2$born <- dat2$DMDBORN4\ndat2$born <- car::recode(dat2$born, recodes = \" 'Others'='Other place';\n                       'Born in 50 US states or Washingt'= 'Born in US'; else=NA\")\ndat2$born <- factor(dat2$born, levels = c(\"Born in US\", \"Other place\"))\ntable(dat2$born, useNA = \"always\") \n#> \n#>  Born in US Other place        <NA> \n#>        8262        1908           5\n\n\n\n23.2.2.8 Pregnancy\n\ndat2$pregnancy <- dat2$RIDEXPRG\ndat2$pregnancy <- car::recode(dat2$pregnancy, \n                      recodes = \" 'Yes, positive lab pregnancy test' = 'Yes';\n                       'The participant was not pregnant' = 'No'; \n                       'Cannot ascertain if the particip' = 'inconclusive';\n                       else= 'outside of target population'  \")\ntable(dat2$pregnancy, useNA = \"always\") \n#> \n#>                 inconclusive                           No \n#>                           94                         1150 \n#> outside of target population                          Yes \n#>                         8866                           65 \n#>                         <NA> \n#>                            0\n\n\n\n\n23.2.3 BMI\n\n23.2.3.1 BMI and Obesity\n\ndat2$bmi <- dat2$BMXBMI\nsummary(dat2$bmi)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   12.10   19.70   24.70   25.68   30.20   82.90    1120\ndat2$obese <- ifelse(dat2$BMXBMI >= 30, \"Yes\", \"No\")\ndat2$obese <- factor(dat2$obese, levels = c(\"No\", \"Yes\"))\ntable(dat2$obese, useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 6708 2347 1120\n\n\n\n\n23.2.4 Diabetes\n\ndat2$diabetes <- dat2$DIQ010\ndat2$diabetes <- car::recode(dat2$diabetes, \" 'Yes'='Yes'; c('No','Borderline')='No';\n                             else=NA \")\n\n# Taking insulin now or diabetic pills to lower blood sugar - they have diabetes\ndat2$diabetes[dat2$DIQ050 == \"Yes\"] <- \"Yes\"\ndat2$diabetes[dat2$DIQ070 == \"Yes\"] <- \"Yes\"\ntable(dat2$diabetes, useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 8981  782  412\n\n\n\n23.2.5 Family history of diabetes\n\ndat2$diabetes.family.history <- car::recode(dat2$DIQ175A, \" 'Family history' = 'Yes'; \n                             else = 'No' \")\ndat2$diabetes.family.history <- factor(dat2$diabetes.family.history, levels = c(\"No\", \"Yes\"))\ndat2$diabetes.family.history[dat2$DIQ175A==\"Don't know\"] <- NA\ntable(dat2$diabetes.family.history, useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 8837 1337    1\n\n\n\n23.2.6 Smoking\n\ndat2$smoking <- dat2$SMQ020\ndat2$smoking <- car::recode(dat2$smoking, \" 'Yes' = 'Current smoker'; 'No' = 'Never smoker'; else=NA  \")\ndat2$smoking <- factor(dat2$smoking, levels = c(\"Never smoker\", \"Previous smoker\", \"Current smoker\"))\ndat2$smoking[dat2$SMQ040 == \"Not at all\"] <- \"Previous smoker\"\ntable(dat2$smoking, useNA = \"always\")\n#> \n#>    Never smoker Previous smoker  Current smoker            <NA> \n#>            3532            1347            1232            4064\n\n\n\n23.2.7 Diet\n\n23.2.7.1 How healthy is the diet\n\ndat2$diet.healthy <- dat2$DBQ700\ndat2$diet.healthy <- car::recode(dat2$diet.healthy, recodes = \" c('Excellent', 'Very good')=\n                    'Very good or excellent'; 'Good'='Good'; c('Fair', 'Poor')=\n                    'Poor or fair'; else = NA \")\ndat2$diet.healthy <- factor(dat2$diet.healthy, levels = c(\"Poor or fair\", \"Good\", \n                                                          \"Very good or excellent\"))\ntable(dat2$diet.healthy, useNA = \"always\")\n#> \n#>           Poor or fair                   Good Very good or excellent \n#>                   1824                   2743                   1896 \n#>                   <NA> \n#>                   3712\n\n\n\n\n23.2.8 Vigorous physical activity\n\ndat2$physical.activity <- dat2$PAQ605\ndat2$physical.activity <- car::recode(dat2$physical.activity, recodes = \" 'No' = 'No'; \n                                      'Yes' = 'Yes'; else=NA\")\ndat2$physical.activity <- factor(dat2$physical.activity, levels = c(\"No\", \"Yes\"))\ntable(dat2$physical.activity, useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 5975 1172 3028\n\n\n\n23.2.9 Access to medical services\n\ndat2$medical.access <- dat2$HUQ030\ndat2$medical.access <- car::recode(dat2$medical.access, recodes = \" c('Yes',\n                              'There is more than one place')='Yes'; 'There is no place'=\n                              'No'; else=NA\")\ntable(dat2$medical.access, useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 1194 8981    0\n\n\n\n23.2.10 Hypertension/high blood pressure\n\n23.2.10.1 Systolic BP\n\ndat2$systolic1 <- dat2$BPXSY1\ndat2$systolic2 <- dat2$BPXSY2\ndat2$systolic3 <- dat2$BPXSY3\ndat2$systolic4 <- dat2$BPXSY4\n\ndat2 <- dat2 %>% \n  mutate(systolicBP = rowMeans(dat2[, c(\"systolic1\", \"systolic2\", \n                                        \"systolic3\", \"systolic4\")], \n                             na.rm = TRUE))\nsummary(dat2$systolicBP)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   64.67  106.00  115.33  118.31  128.00  228.67    2644\n\n\n\n23.2.10.2 Diastolic BP\n\ndat2$diastolic1 <- dat2$BPXDI1\ndat2$diastolic2 <- dat2$BPXDI2\ndat2$diastolic3 <- dat2$BPXDI3\ndat2$diastolic4 <- dat2$BPXDI4\ndatX <- dat2[, c(\"diastolic1\", \"diastolic2\", \n                 \"diastolic3\", \"diastolic4\")]\ndatX[datX ==0] <- NA\ndat2$diastolicBP <- rowMeans(datX[, c(\"diastolic1\", \"diastolic2\", \n                                      \"diastolic3\", \"diastolic4\")], \n                             na.rm = TRUE)\nsummary(dat2$diastolicBP)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   3.333  58.000  66.667  66.329  74.667 128.000    2688\n\n\n\n\n23.2.11 Sleep (daily in hours)\n\ndat2$sleep <- dat2$SLD010H\ndat2$sleep[dat2$sleep == 99] <- NA\nsummary(dat2$sleep)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   2.000   6.000   7.000   6.951   8.000  12.000    3721\n\n\n\n23.2.12 Laboratory data\n\n23.2.12.1 Uric acid (mg/dL)\n\ndat2$uric.acid <- dat2$LBXSUA\nsummary(dat2$uric.acid)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>    0.70    4.30    5.20    5.35    6.20   13.30    3624\n\n\n\n23.2.12.2 Total protein (g/dL)\n\ndat2$protein.total <- dat2$LBXSTP\nsummary(dat2$protein.total)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   4.700   6.800   7.100   7.108   7.400  10.200    3631\n\n\n\n23.2.12.3 Total bilirubin (mg/dL)\n\ndat2$bilirubin.total <- dat2$LBXSTB\nsummary(dat2$bilirubin.total)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   0.100   0.400   0.600   0.639   0.800   7.100    3626\n\n\n\n23.2.12.4 Phosphorus (mg/dL)\n\ndat2$phosphorus <- dat2$LBXSPH\nsummary(dat2$phosphorus)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   1.800   3.500   3.900   3.929   4.300  10.900    3623\n\n\n\n23.2.12.5 Sodium (mmol/L)\n\ndat2$sodium <- dat2$LBXSNASI\nsummary(dat2$sodium)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   119.0   139.0   140.0   139.8   141.0   154.0    3622\n\n\n\n23.2.12.6 Potassium (mmol/L)\n\ndat2$potassium <- dat2$LBXSKSI\nsummary(dat2$potassium)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   2.800   3.800   4.000   4.027   4.200   5.800    3623\n\n\n\n23.2.12.7 Globulin (g/dL)\n\ndat2$globulin <- dat2$LBXSGB\nsummary(dat2$globulin)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   1.400   2.500   2.800   2.826   3.100   6.500    3631\n\n\n\n23.2.12.8 Total calcium (mg/dL)\n\ndat2$calcium.total <- dat2$LBXSCA\nsummary(dat2$calcium.total)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   7.600   9.200   9.500   9.486   9.700  14.800    3664\n\n\n\n23.2.12.9 High cholesterol\n\ndat2$high.cholesterol <- dat2$BPQ080\ndat2$high.cholesterol <- car::recode(dat2$high.cholesterol, recodes = \" 'Yes'='Yes';\n                                     'No'='No'; else = NA\")\ntable(dat2$high.cholesterol, useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 4391 2037 3747\n\n\n\n\n23.2.13 Survey features\n\n23.2.13.1 Weight\n\ndat2$survey.weight <- dat2$WTINT2YR\nsummary(dat2$survey.weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    3698   12754   20233   30585   36280  167885\ndat2$survey.weight.mec <- dat2$WTMEC2YR\nsummary(dat2$survey.weight.mec)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>       0   12562   20175   30585   36748  171395\n\n\n\n23.2.13.2 PSU\n\ndat2$psu <- as.factor(dat2$SDMVPSU)\ntable(dat2$psu)\n#> \n#>    1    2 \n#> 5249 4926\n\n\n\n23.2.13.3 Strata\n\ndat2$strata <- as.factor(dat2$SDMVSTRA)\ntable(dat2$strata)\n#> \n#> 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 \n#> 674 646 671 732 674 752 664 663 723 665 741 681 700 500 689\n\n\n\n\n23.2.14 Survey year\n\ndat2$year <- dat2$SDDSRVYR\ntable(dat2$year, useNA = \"always\") \n#> \n#>     8  <NA> \n#> 10175     0\n\n\n\n23.2.15 ICD-10-CM codes\n\ncolnames(rxq12) <- c(\"id\", \"icd10\")\ncolnames(rxq22) <- c(\"id\", \"icd10\")\ncolnames(rxq32) <- c(\"id\", \"icd10\")\n\nrx2013 <- rbind(rxq12, rxq22, rxq32)\nrx2013 <- rx2013[order(rx2013$id),]\n\nrx2013$icd10[rx2013$icd10 == \"Unknown\"] <- NA\nrx2013$icd10[rx2013$icd10 == \"Refused\"] <- NA\nrx2013$icd10[rx2013$icd10 == \"Don't know\"] <- NA\nrx2013$icd10[rx2013$icd10 == \"\"] <- NA\nrx2013$icd10.new <- substr(rx2013$icd10, start = 1, stop = 3)\n\nrx2013 <- na.omit(rx2013)"
  },
  {
    "objectID": "analytic13.html#analytic-data",
    "href": "analytic13.html#analytic-data",
    "title": "23  Recoding cycle 8",
    "section": "23.3 Analytic data",
    "text": "23.3 Analytic data\n\n23.3.1 Full dataset\n\nnhanes13r <- dat2\n\n\n\n23.3.2 Analytic datset - adults 20 years of more\n\nvars <- c(\n  # ID\n  \"id\",\n  \n  # Demographic\n  \"age\", \"age.cat\", \"sex\", \"education\", \"race\", \n  \"marital\", \"income\", \"born\", \"pregnancy\",\n  \n  # obesity\n  \"obese\", \n  \n  # Diabetes\n  \"diabetes\", \"diabetes.family.history\",\n  \n  # Smoking\n  \"smoking\", \n  \n  # Diet\n  \"diet.healthy\", \n\n  # Physical activity\n  \"physical.activity\", \n  \n  # Access to routine healthcare\n  \"medical.access\",\n  \n  # Blood pressure and Hypertension\n  \"systolicBP\", \"diastolicBP\", \n  \n  # Sleep \n  \"sleep\",\n\n  # Laboratory \n  \"uric.acid\", \"protein.total\", \"bilirubin.total\", \"phosphorus\",\n  \"sodium\", \"potassium\", \"globulin\", \"calcium.total\", \n  \"high.cholesterol\",\n  \n  # Survey features\n  \"survey.weight\", \"survey.weight.mec\", \"psu\", \"strata\", \n  \n  # Survey year\n  \"year\"\n)\n\nnhanes13r.sel <- nhanes13r[, vars]\n\n\n# Adults 20 years of more and not pregnant\ndim(nhanes13r.sel)\n#> [1] 10175    34\nanalytic13 <- subset(nhanes13r.sel, age >= 20 & \n                       pregnancy != 'yes')\ndim(analytic13)\n#> [1] 5769   34\n\n\n\n23.3.3 Save dataset for later use\n\ndim(analytic13)\n#> [1] 5769   34\ndim(rx2013)\n#> [1] 14474     3\nsave(analytic13, rx2013, file = \"data/analytic13recoded.RData\")"
  },
  {
    "objectID": "analytic15.html#load-downloaded-dataset",
    "href": "analytic15.html#load-downloaded-dataset",
    "title": "24  Recoding cycle 9",
    "section": "24.1 Load downloaded dataset",
    "text": "24.1 Load downloaded dataset\n\nload(file = \"data/analytic15.RData\")"
  },
  {
    "objectID": "analytic15.html#recoding",
    "href": "analytic15.html#recoding",
    "title": "24  Recoding cycle 9",
    "section": "24.2 Recoding",
    "text": "24.2 Recoding\n\n24.2.1 ID\n\ndat2 <- nhanes15\ndat2$id <- dat2$SEQN\n\n\n\n24.2.2 Demographic\n\n24.2.2.1 Age\n\ndat2$age <- dat2$RIDAGEYR\ndat2$age.cat <- car::recode(dat2$age, \" 0:19 = '<20'; 20:49 = '20-49'; 50:64 = '50-64'; \n                            65:80 = '65+'; else = NA \")\ndat2$age.cat <- factor(dat2$age.cat, levels = c(\"<20\", \"20-49\", \"50-64\", \"65+\"))\ntable(dat2$age.cat, useNA = \"always\")\n#> \n#>   <20 20-49 50-64   65+  <NA> \n#>  4252  2894  1447  1378     0\n\n\n\n24.2.2.2 Sex\n\ndat2$sex <- dat2$RIAGENDR\ntable(dat2$sex, useNA = \"always\")\n#> \n#>   Male Female   <NA> \n#>   4892   5079      0\n\n\n\n24.2.2.3 Education\n\ndat2$education <- dat2$DMDEDUC2\ndat2$education <- as.factor(dat2$education)\ndat2$education <- car::recode(dat2$education, recodes = \" c('College graduate or above') = \n'College graduate or above'; c('Some college or AA degree', 'High school graduate/GED or equi') = \n'High school'; c('Less than 9th grade', '9-11th grade (Includes 12th grad') = \n'Less than high school'; else = NA \")\ndat2$education <- factor(dat2$education, \n                         levels = c(\"Less than high school\", \"High school\", \n                                    \"College graduate or above\"))\ntable(dat2$education, useNA = \"always\")\n#> \n#>     Less than high school               High school College graduate or above \n#>                      1364                      2928                      1422 \n#>                      <NA> \n#>                      4257\n\n\n\n24.2.2.4 Race/ethnicity\n\ndat2$race <- dat2$RIDRETH1\ndat2$race <- car::recode(dat2$race, recodes = \" 'Non-Hispanic White'='White';\n                    'Non-Hispanic Black'='Black'; c('Mexican American',\n                    'Other Hispanic')= 'Hispanic'; else='Others' \")\ndat2$race <- factor(dat2$race, levels = c(\"White\", \"Black\", \"Hispanic\", \"Others\"))\ntable(dat2$race, useNA = \"always\")\n#> \n#>    White    Black Hispanic   Others     <NA> \n#>     3066     2129     3229     1547        0\n\n\n\n24.2.2.5 Marital status\n\ndat2$marital <- dat2$DMDMARTL\ndat2$marital <- car::recode(dat2$marital, recodes = \" 'Never married'='Never married';\nc('Married', 'Living with partner') = 'Married/with partner'; \n                            c('Widowed', 'Divorced', 'Separated')='Other'; else=NA \")\ndat2$marital <- factor(dat2$marital, levels = c(\"Never married\", \"Married/with partner\",\n                                                \"Other\"))\ntable(dat2$marital, useNA = \"always\")\n#> \n#>        Never married Married/with partner                Other \n#>                 1048                 3441                 1227 \n#>                 <NA> \n#>                 4255\n\n\n\n24.2.2.6 Income\n\ndat2$income <- dat2$INDHHIN2\ndat2$income  <- car::recode(dat2$income, recodes = \" c('$ 0 to $ 4,999', '$ 5,000 to $ 9,999',\n'$10,000 to $14,999', '$15,000 to $19,999', 'Under $20,000')='less than $20,000';\n                       c('Over $20,000','$20,000 and Over', '$20,000 to $24,999', \n                       '$25,000 to $34,999', '$35,000 to $44,999', '$45,000 to $54,999', \n                       '$55,000 to $64,999', '$65,000 to $74,999')='$20,000 to $74,999';\n                       c('$75,000 to $99,999','$100,000 and Over')='$75,000 and Over'; \n                            else=NA \")\ndat2$income  <- factor(dat2$income , levels=c(\"less than $20,000\", \"$20,000 to $74,999\", \n                                              \"$75,000 and Over\"))\ntable(dat2$income, useNA = \"always\")\n#> \n#>  less than $20,000 $20,000 to $74,999   $75,000 and Over               <NA> \n#>               1906               4812               2554                699\n\n\n\n24.2.2.7 Where born / citizenship\n\ndat2$born <- dat2$DMDBORN4\ndat2$born <- car::recode(dat2$born, recodes = \" 'Others'='Other place';\n                       'Born in 50 US states or Washingt'= 'Born in US'; else=NA\")\ndat2$born <- factor(dat2$born, levels = c(\"Born in US\", \"Other place\"))\ntable(dat2$born, useNA = \"always\") \n#> \n#>  Born in US Other place        <NA> \n#>        7733        2236           2\n\n\n\n24.2.2.8 Pregnancy\n\ndat2$pregnancy <- dat2$RIDEXPRG\ndat2$pregnancy <- car::recode(dat2$pregnancy, \n                      recodes = \" 'Yes, positive lab pregnancy test' = 'Yes';\n                       'The participant was not pregnant' = 'No'; \n                       'Cannot ascertain if the particip' = 'inconclusive';\n                       else= 'outside of target population'  \")\ntable(dat2$pregnancy, useNA = \"always\") \n#> \n#>                 inconclusive                           No \n#>                           93                         1125 \n#> outside of target population                          Yes \n#>                         8683                           70 \n#>                         <NA> \n#>                            0\n\n\n\n\n24.2.3 BMI\n\n24.2.3.1 BMI and Obesity\n\ndat2$bmi <- dat2$BMXBMI\nsummary(dat2$bmi)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   11.50   19.90   25.20   26.02   30.60   67.30    1215\ndat2$obese <- ifelse(dat2$BMXBMI >= 30, \"Yes\", \"No\")\ndat2$obese <- factor(dat2$obese, levels = c(\"No\", \"Yes\"))\ntable(dat2$obese, useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 6346 2410 1215\n\n\n\n\n24.2.4 Diabetes\n\ndat2$diabetes <- dat2$DIQ010\ndat2$diabetes <- car::recode(dat2$diabetes, \" 'Yes'='Yes'; c('No','Borderline')='No';\n                             else=NA \")\n\n# Taking insulin now or diabetic pills to lower blood sugar - they have diabetes\ndat2$diabetes[dat2$DIQ050 == \"Yes\"] <- \"Yes\"\ndat2$diabetes[dat2$DIQ070 == \"Yes\"] <- \"Yes\"\ntable(dat2$diabetes, useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 8648  923  400\n\n\n\n24.2.5 Family history of diabetes\n\ntable(dat2$DIQ175A, useNA = \"always\")\n#> \n#>   10 <NA> \n#> 1186 8785\ndat2$diabetes.family.history <- dat2$DIQ175A\ndat2$diabetes.family.history <- car::recode(dat2$diabetes.family.history, \" 10 = 'Yes'; \n                             else = 'No' \")\ndat2$diabetes.family.history <- factor(dat2$diabetes.family.history, levels = c(\"No\", \"Yes\"))\ndat2$diabetes.family.history[dat2$DIQ175A==\"Don't know\"] <- NA\ntable(dat2$diabetes.family.history, useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 8785 1186    0\n\n\n\n24.2.6 Smoking\n\ndat2$smoking <- dat2$SMQ020\ndat2$smoking <- car::recode(dat2$smoking, \" 'Yes' = 'Current smoker'; 'No' = 'Never smoker'; else=NA  \")\ndat2$smoking <- factor(dat2$smoking, levels = c(\"Never smoker\", \"Previous smoker\", \"Current smoker\"))\ndat2$smoking[dat2$SMQ040 == \"Not at all\"] <- \"Previous smoker\"\ntable(dat2$smoking, useNA = \"always\")\n#> \n#>    Never smoker Previous smoker  Current smoker            <NA> \n#>            3559            1322            1100            3990\n\n\n\n24.2.7 Diet\n\n24.2.7.1 How healthy is the diet\n\ndat2$diet.healthy <- dat2$DBQ700\ndat2$diet.healthy <- car::recode(dat2$diet.healthy, recodes = \" c('Excellent', 'Very good')=\n                    'Very good or excellent'; 'Good'='Good'; c('Fair', 'Poor')=\n                    'Poor or fair'; else = NA \")\ndat2$diet.healthy <- factor(dat2$diet.healthy, levels = c(\"Poor or fair\", \"Good\", \n                                                          \"Very good or excellent\"))\ntable(dat2$diet.healthy, useNA = \"always\")\n#> \n#>           Poor or fair                   Good Very good or excellent \n#>                   2105                   2524                   1697 \n#>                   <NA> \n#>                   3645\n\n\n\n\n24.2.8 Vigorous physical activity\n\ndat2$physical.activity <- dat2$PAQ605\ndat2$physical.activity <- car::recode(dat2$physical.activity, recodes = \" 'No' = 'No'; \n                                      'Yes' = 'Yes'; else=NA\")\ndat2$physical.activity <- factor(dat2$physical.activity, levels = c(\"No\", \"Yes\"))\ntable(dat2$physical.activity, useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 5596 1366 3009\n\n\n\n24.2.9 Access to medical services\n\ndat2$medical.access <- dat2$HUQ030\ndat2$medical.access <- car::recode(dat2$medical.access, recodes = \" c('Yes',\n                              'There is more than one place')='Yes'; 'There is no place'=\n                              'No'; else=NA\")\ntable(dat2$medical.access, useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 1340 8631    0\n\n\n\n24.2.10 Hypertension/high blood pressure\n\n24.2.10.1 Systolic BP\n\ndat2$systolic1 <- dat2$BPXSY1\ndat2$systolic2 <- dat2$BPXSY2\ndat2$systolic3 <- dat2$BPXSY3\ndat2$systolic4 <- dat2$BPXSY4\n\ndat2 <- dat2 %>% \n  mutate(systolicBP = rowMeans(dat2[, c(\"systolic1\", \"systolic2\", \n                                        \"systolic3\", \"systolic4\")], \n                             na.rm = TRUE))\nsummary(dat2$systolicBP)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>    74.0   107.3   117.3   120.4   130.0   231.3    2608\n\n\n\n24.2.10.2 Diastolic BP\n\ndat2$diastolic1 <- dat2$BPXDI1\ndat2$diastolic2 <- dat2$BPXDI2\ndat2$diastolic3 <- dat2$BPXDI3\ndat2$diastolic4 <- dat2$BPXDI4\ndatX <- dat2[, c(\"diastolic1\", \"diastolic2\", \n                 \"diastolic3\", \"diastolic4\")]\ndatX[datX ==0] <- NA\ndat2$diastolicBP <- rowMeans(datX[, c(\"diastolic1\", \"diastolic2\", \n                                      \"diastolic3\", \"diastolic4\")], \n                             na.rm = TRUE)\nsummary(dat2$diastolicBP)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>    2.00   58.00   66.67   66.64   74.67  138.67    2636\n\n\n\n\n24.2.11 Sleep (daily in hours)\n\ndat2$sleep <- dat2$SLD012\nsummary(dat2$sleep)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   2.000   7.000   8.000   7.753   8.500  14.500    3677\n\n\n\n24.2.12 Laboratory data\n\n24.2.12.1 Uric acid (mg/dL)\n\ndat2$uric.acid <- dat2$LBXSUA\nsummary(dat2$uric.acid)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   1.600   4.300   5.200   5.335   6.200  18.000    3717\n\n\n\n24.2.12.2 Total protein (g/dL)\n\ndat2$protein.total <- dat2$LBXSTP\nsummary(dat2$protein.total)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   5.200   6.900   7.200   7.201   7.500  10.100    3718\n\n\n\n24.2.12.3 Total bilirubin (mg/dL)\n\ndat2$bilirubin.total <- dat2$LBXSTB\nsummary(dat2$bilirubin.total)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   0.000   0.400   0.500   0.552   0.700   3.500    3717\n\n\n\n24.2.12.4 Phosphorus (mg/dL)\n\ndat2$phosphorus <- dat2$LBXSPH\nsummary(dat2$phosphorus)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   1.000   3.400   3.800   3.796   4.200   9.700    3715\n\n\n\n24.2.12.5 Sodium (mmol/L)\n\ndat2$sodium <- dat2$LBXSNASI\nsummary(dat2$sodium)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   124.0   137.0   139.0   138.7   140.0   161.0    3714\n\n\n\n24.2.12.6 Potassium (mmol/L)\n\ndat2$potassium <- dat2$LBXSKSI\nsummary(dat2$potassium)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   2.600   3.740   3.930   3.952   4.150   5.860    3714\n\n\n\n24.2.12.7 Globulin (g/dL)\n\ndat2$globulin <- dat2$LBXSGB\nsummary(dat2$globulin)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   0.600   2.600   2.800   2.857   3.100   7.000    3719\n\n\n\n24.2.12.8 Total calcium (mg/dL)\n\ndat2$calcium.total <- dat2$LBXSCA\nsummary(dat2$calcium.total)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   7.300   9.100   9.400   9.375   9.600  11.500    3714\n\n\n\n24.2.12.9 High cholesterol\n\ndat2$high.cholesterol <- dat2$BPQ080\ndat2$high.cholesterol <- car::recode(dat2$high.cholesterol, recodes = \" 'Yes'='Yes';\n                                     'No'='No'; else = NA\")\ntable(dat2$high.cholesterol, useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 4323 1960 3688\n\n\n\n\n24.2.13 Survey features\n\n24.2.13.1 Weight\n\ndat2$survey.weight <- dat2$WTINT2YR\nsummary(dat2$survey.weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    3294   12878   20160   31740   33257  233756\ndat2$survey.weight.mec <- dat2$WTMEC2YR\nsummary(dat2$survey.weight.mec)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>       0   12551   20281   31740   33708  242387\n\n\n\n24.2.13.2 PSU\n\ndat2$psu <- as.factor(dat2$SDMVPSU)\ntable(dat2$psu)\n#> \n#>    1    2 \n#> 5127 4844\n\n\n\n24.2.13.3 Strata\n\ndat2$strata <- as.factor(dat2$SDMVSTRA)\ntable(dat2$strata)\n#> \n#> 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 \n#> 462 685 694 629 612 571 673 723 665 688 681 759 805 773 551\n\n\n\n\n24.2.14 Survey year\n\ndat2$year <- dat2$SDDSRVYR\ntable(dat2$year, useNA = \"always\") \n#> \n#>    9 <NA> \n#> 9971    0\n\n\n\n24.2.15 ICD-10-CM codes\n\ncolnames(rxq12) <- c(\"id\", \"icd10\")\ncolnames(rxq22) <- c(\"id\", \"icd10\")\ncolnames(rxq32) <- c(\"id\", \"icd10\")\n\nrx2015 <- rbind(rxq12, rxq22, rxq32)\nrx2015 <- rx2015[order(rx2015$id),]\n\nrx2015$icd10[rx2015$icd10 == \"Unknown\"] <- NA\nrx2015$icd10[rx2015$icd10 == \"Refused\"] <- NA\nrx2015$icd10[rx2015$icd10 == \"Don't know\"] <- NA\nrx2015$icd10[rx2015$icd10 == \"\"] <- NA\nrx2015$icd10.new <- substr(rx2015$icd10, start = 1, stop = 3)\n\nrx2015 <- na.omit(rx2015)"
  },
  {
    "objectID": "analytic15.html#analytic-data",
    "href": "analytic15.html#analytic-data",
    "title": "24  Recoding cycle 9",
    "section": "24.3 Analytic data",
    "text": "24.3 Analytic data\n\n24.3.1 Full dataset\n\nnhanes15r <- dat2\n\n\n\n24.3.2 Analytic datset - adults 20 years of more\n\nvars <- c(\n  # ID\n  \"id\",\n  \n  # Demographic\n  \"age\", \"age.cat\", \"sex\", \"education\", \"race\", \n  \"marital\", \"income\", \"born\", \"pregnancy\",\n  \n  # obesity\n  \"obese\", \n  \n  # Diabetes\n  \"diabetes\", \"diabetes.family.history\",\n  \n  # Smoking\n  \"smoking\", \n  \n  # Diet\n  \"diet.healthy\", \n\n  # Physical activity\n  \"physical.activity\", \n  \n  # Access to routine healthcare\n  \"medical.access\",\n  \n  # Blood pressure and Hypertension\n  \"systolicBP\", \"diastolicBP\", \n  \n  # Sleep \n  \"sleep\",\n\n  # Laboratory \n  \"uric.acid\", \"protein.total\", \"bilirubin.total\", \"phosphorus\",\n  \"sodium\", \"potassium\", \"globulin\", \"calcium.total\", \n  \"high.cholesterol\",\n  \n  # Survey features\n  \"survey.weight\", \"survey.weight.mec\", \"psu\", \"strata\", \n  \n  # Survey year\n  \"year\"\n)\n\nnhanes15r.sel <- nhanes15r[, vars]\n\n\n# Adults 20 years of more and not pregnant\ndim(nhanes15r.sel)\n#> [1] 9971   34\nanalytic15 <- subset(nhanes15r.sel, age >= 20 & \n                       pregnancy != 'yes')\ndim(analytic15)\n#> [1] 5719   34\n\n\n\n24.3.3 Save dataset for later use\n\ndim(analytic15)\n#> [1] 5719   34\ndim(rx2015)\n#> [1] 14084     3\nsave(analytic15, rx2015, file = \"data/analytic15recoded.RData\")"
  },
  {
    "objectID": "analytic17.html#load-downloaded-dataset",
    "href": "analytic17.html#load-downloaded-dataset",
    "title": "25  Recoding cycle 10",
    "section": "25.1 Load downloaded dataset",
    "text": "25.1 Load downloaded dataset\n\nload(file = \"data/analytic17.RData\")"
  },
  {
    "objectID": "analytic17.html#recoding",
    "href": "analytic17.html#recoding",
    "title": "25  Recoding cycle 10",
    "section": "25.2 Recoding",
    "text": "25.2 Recoding\n\n25.2.1 ID\n\ndat2 <- nhanes17\ndat2$id <- dat2$SEQN\n\n\n\n25.2.2 Demographic\n\n25.2.2.1 Age\n\ndat2$age <- dat2$RIDAGEYR\ndat2$age.cat <- car::recode(dat2$age, \" 0:19 = '<20'; 20:49 = '20-49'; 50:64 = '50-64'; \n                            65:80 = '65+'; else = NA \")\ndat2$age.cat <- factor(dat2$age.cat, levels = c(\"<20\", \"20-49\", \"50-64\", \"65+\"))\ntable(dat2$age.cat, useNA = \"always\")\n#> \n#>   <20 20-49 50-64   65+  <NA> \n#>  3685  2500  1569  1500     0\n\n\n\n25.2.2.2 Sex\n\ndat2$sex <- dat2$RIAGENDR\ntable(dat2$sex, useNA = \"always\")\n#> \n#>   Male Female   <NA> \n#>   4557   4697      0\n\n\n\n25.2.2.3 Education\n\ndat2$education <- dat2$DMDEDUC2\ndat2$education <- as.factor(dat2$education)\ndat2$education <- car::recode(dat2$education, recodes = \" c('College graduate or above') = \n'College graduate or above'; c('Some college or AA degree', 'High school graduate/GED or equi') = \n'High school'; c('Less than 9th grade', '9-11th grade (Includes 12th grad') = \n'Less than high school'; else = NA \")\ndat2$education <- factor(dat2$education, \n                         levels = c(\"Less than high school\", \"High school\", \n                                    \"College graduate or above\"))\ntable(dat2$education, useNA = \"always\")\n#> \n#>     Less than high school               High school College graduate or above \n#>                      1117                      3103                      1336 \n#>                      <NA> \n#>                      3698\n\n\n\n25.2.2.4 Race/ethnicity\n\ndat2$race <- dat2$RIDRETH1\ndat2$race <- car::recode(dat2$race, recodes = \" 'Non-Hispanic White'='White';\n                    'Non-Hispanic Black'='Black'; c('Mexican American',\n                    'Other Hispanic')= 'Hispanic'; else='Others' \")\ndat2$race <- factor(dat2$race, levels = c(\"White\", \"Black\", \"Hispanic\", \"Others\"))\ntable(dat2$race, useNA = \"always\")\n#> \n#>    White    Black Hispanic   Others     <NA> \n#>     3150     2115     2187     1802        0\n\n\n\n25.2.2.5 Marital status\n\ndat2$marital <- dat2$DMDMARTL\ndat2$marital <- car::recode(dat2$marital, recodes = \" 'Never married'='Never married';\nc('Married', 'Living with partner') = 'Married/with partner'; \n                            c('Widowed', 'Divorced', 'Separated')='Other'; else=NA \")\ndat2$marital <- factor(dat2$marital, levels = c(\"Never married\", \"Married/with partner\",\n                                                \"Other\"))\ntable(dat2$marital, useNA = \"always\")\n#> \n#>        Never married Married/with partner                Other \n#>                 1006                 3252                 1305 \n#>                 <NA> \n#>                 3691\n\n\n\n25.2.2.6 Income\n\ndat2$income <- dat2$INDHHIN2\ndat2$income  <- car::recode(dat2$income, recodes = \" c('$ 0 to $ 4,999', '$ 5,000 to $ 9,999',\n'$10,000 to $14,999', '$15,000 to $19,999', 'Under $20,000')='less than $20,000';\n                       c('Over $20,000','$20,000 and Over', '$20,000 to $24,999', \n                       '$25,000 to $34,999', '$35,000 to $44,999', '$45,000 to $54,999', \n                       '$55,000 to $64,999', '$65,000 to $74,999')='$20,000 to $74,999';\n                       c('$75,000 to $99,999','$100,000 and Over')='$75,000 and Over'; \n                            else=NA \")\ndat2$income  <- factor(dat2$income , levels=c(\"less than $20,000\", \"$20,000 to $74,999\", \n                                              \"$75,000 and Over\"))\ntable(dat2$income, useNA = \"always\")\n#> \n#>  less than $20,000 $20,000 to $74,999   $75,000 and Over               <NA> \n#>               1589               4331               2453                881\n\n\n\n25.2.2.7 Where born / citizenship\n\ndat2$born <- dat2$DMDBORN4\ndat2$born <- car::recode(dat2$born, recodes = \" 'Others'='Other place';\n                       'Born in 50 US states or Washingt'= 'Born in US'; else=NA\")\ndat2$born <- factor(dat2$born, levels = c(\"Born in US\", \"Other place\"))\ntable(dat2$born, useNA = \"always\") \n#> \n#>  Born in US Other place        <NA> \n#>        7303        1948           3\n\n\n\n25.2.2.8 Pregnancy\n\ndat2$pregnancy <- dat2$RIDEXPRG\ndat2$pregnancy <- car::recode(dat2$pregnancy, \n                      recodes = \" 'Yes, positive lab pregnancy test' = 'Yes';\n                       'The participant was not pregnant' = 'No'; \n                       'Cannot ascertain if the particip' = 'inconclusive';\n                       else= 'outside of target population'  \")\ntable(dat2$pregnancy, useNA = \"always\") \n#> \n#>                 inconclusive                           No \n#>                           89                          966 \n#> outside of target population                          Yes \n#>                         8144                           55 \n#>                         <NA> \n#>                            0\n\n\n\n\n25.2.3 BMI\n\n25.2.3.1 BMI and Obesity\n\ndat2$bmi <- dat2$BMXBMI\nsummary(dat2$bmi)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   12.30   20.40   25.80   26.58   31.30   86.20    1249\ndat2$obese <- ifelse(dat2$BMXBMI >= 30, \"Yes\", \"No\")\ndat2$obese <- factor(dat2$obese, levels = c(\"No\", \"Yes\"))\ntable(dat2$obese, useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 5597 2408 1249\n\n\n\n\n25.2.4 Diabetes\n\ndat2$diabetes <- dat2$DIQ010\ndat2$diabetes <- car::recode(dat2$diabetes, \" 'Yes'='Yes'; c('No','Borderline')='No';\n                             else=NA \")\n\n# Taking insulin now or diabetic pills to lower blood sugar - they have diabetes\ndat2$diabetes[dat2$DIQ050 == \"Yes\"] <- \"Yes\"\ndat2$diabetes[dat2$DIQ070 == \"Yes\"] <- \"Yes\"\ntable(dat2$diabetes, useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 7927  966  361\n\n\n\n25.2.5 Family history of diabetes\n\ntable(dat2$DIQ175A, useNA = \"always\")\n#> \n#> Family history     Don't know           <NA> \n#>           1143              2           8109\ndat2$diabetes.family.history <- dat2$DIQ175A\ndat2$diabetes.family.history <- car::recode(dat2$diabetes.family.history, \" 'Family history' = 'Yes'; \n                             else = 'No' \")\ndat2$diabetes.family.history <- factor(dat2$diabetes.family.history, levels = c(\"No\", \"Yes\"))\ndat2$diabetes.family.history[dat2$DIQ175A==\"Don't know\"] <- NA\ntable(dat2$diabetes.family.history, useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 8109 1143    2\n\n\n\n25.2.6 Smoking\n\ndat2$smoking <- dat2$SMQ020\ndat2$smoking <- car::recode(dat2$smoking, \" 'Yes' = 'Current smoker'; 'No' = 'Never smoker'; else=NA  \")\ndat2$smoking <- factor(dat2$smoking, levels = c(\"Never smoker\", \"Previous smoker\", \"Current smoker\"))\ndat2$smoking[dat2$SMQ040 == \"Not at all\"] <- \"Previous smoker\"\ntable(dat2$smoking, useNA = \"always\")\n#> \n#>    Never smoker Previous smoker  Current smoker            <NA> \n#>            3497            1338            1021            3398\n\n\n\n25.2.7 Diet\n\n25.2.7.1 How healthy is the diet\n\ndat2$diet.healthy <- dat2$DBQ700\ndat2$diet.healthy <- car::recode(dat2$diet.healthy, recodes = \" c('Excellent', 'Very good')=\n                    'Very good or excellent'; 'Good'='Good'; c('Fair', 'Poor')=\n                    'Poor or fair'; else = NA \")\ndat2$diet.healthy <- factor(dat2$diet.healthy, levels = c(\"Poor or fair\", \"Good\", \n                                                          \"Very good or excellent\"))\ntable(dat2$diet.healthy, useNA = \"always\")\n#> \n#>           Poor or fair                   Good Very good or excellent \n#>                   2036                   2411                   1712 \n#>                   <NA> \n#>                   3095\n\n\n\n\n25.2.8 Vigorous physical activity\n\ndat2$physical.activity <- dat2$PAQ605\ndat2$physical.activity <- car::recode(dat2$physical.activity, recodes = \" 'No' = 'No'; \n                                      'Yes' = 'Yes'; else=NA\")\ndat2$physical.activity <- factor(dat2$physical.activity, levels = c(\"No\", \"Yes\"))\ntable(dat2$physical.activity, useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 4461 1389 3404\n\n\n\n25.2.9 Access to medical services\n\ndat2$medical.access <- dat2$HUQ030\ndat2$medical.access <- car::recode(dat2$medical.access, recodes = \" c('Yes',\n                              'There is more than one place')='Yes'; 'There is no place'=\n                              'No'; else=NA\")\ntable(dat2$medical.access, useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 1398 7854    2\n\n\n\n25.2.10 Hypertension/high blood pressure\n\n25.2.10.1 Systolic BP\n\ndat2$systolic1 <- dat2$BPXSY1\ndat2$systolic2 <- dat2$BPXSY2\ndat2$systolic3 <- dat2$BPXSY3\ndat2$systolic4 <- dat2$BPXSY4\n\ndat2 <- dat2 %>% \n  mutate(systolicBP = rowMeans(dat2[, c(\"systolic1\", \"systolic2\", \n                                        \"systolic3\", \"systolic4\")], \n                             na.rm = TRUE))\nsummary(dat2$systolicBP)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   72.67  106.67  118.00  121.68  132.67  238.00    2537\n\n\n\n25.2.10.2 Diastolic BP\n\ndat2$diastolic1 <- dat2$BPXDI1\ndat2$diastolic2 <- dat2$BPXDI2\ndat2$diastolic3 <- dat2$BPXDI3\ndat2$diastolic4 <- dat2$BPXDI4\ndatX <- dat2[, c(\"diastolic1\", \"diastolic2\", \n                 \"diastolic3\", \"diastolic4\")]\ndatX[datX ==0] <- NA\ndat2$diastolicBP <- rowMeans(datX[, c(\"diastolic1\", \"diastolic2\", \n                                      \"diastolic3\", \"diastolic4\")], \n                             na.rm = TRUE)\nsummary(dat2$diastolicBP)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>    8.00   61.33   70.00   69.54   77.33  135.33    2618\n\n\n\n\n25.2.11 Sleep (daily in hours)\n\ndat2$sleep <- dat2$SLD012\nsummary(dat2$sleep)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   2.000   7.000   8.000   7.659   8.500  14.000    3141\n\n\n\n25.2.12 Laboratory data\n\n25.2.12.1 Uric acid (mg/dL)\n\ndat2$uric.acid <- dat2$LBXSUA\nsummary(dat2$uric.acid)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   0.800   4.300   5.300   5.402   6.300  15.100    3353\n\n\n\n25.2.12.2 Total protein (g/dL)\n\ndat2$protein.total <- dat2$LBXSTP\nsummary(dat2$protein.total)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   5.300   6.900   7.200   7.166   7.400  10.000    3353\n\n\n\n25.2.12.3 Total bilirubin (mg/dL)\n\ndat2$bilirubin.total <- dat2$LBXSTB\nsummary(dat2$bilirubin.total)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>    0.10    0.30    0.40    0.46    0.60    3.70    3351\n\n\n\n25.2.12.4 Phosphorus (mg/dL)\n\ndat2$phosphorus <- dat2$LBXSPH\nsummary(dat2$phosphorus)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   1.900   3.300   3.600   3.665   4.000   9.600    3353\n\n\n\n25.2.12.5 Sodium (mmol/L)\n\ndat2$sodium <- dat2$LBXSNASI\nsummary(dat2$sodium)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   121.0   138.0   140.0   140.3   142.0   151.0    3350\n\n\n\n25.2.12.6 Potassium (mmol/L)\n\ndat2$potassium <- dat2$LBXSKSI\nsummary(dat2$potassium)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   2.800   3.900   4.100   4.094   4.300   6.600    3355\n\n\n\n25.2.12.7 Globulin (g/dL)\n\ndat2$globulin <- dat2$LBXSGB\nsummary(dat2$globulin)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>   1.800   2.800   3.100   3.087   3.300   6.000    3353\n\n\n\n25.2.12.8 Total calcium (mg/dL)\n\ndat2$calcium.total <- dat2$LBXSCA\nsummary(dat2$calcium.total)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>    6.40    9.10    9.30    9.32    9.60   11.70    3353\n\n\n\n25.2.12.9 High cholesterol\n\ndat2$high.cholesterol <- dat2$BPQ080\ndat2$high.cholesterol <- car::recode(dat2$high.cholesterol, recodes = \" 'Yes'='Yes';\n                                     'No'='No'; else = NA\")\ntable(dat2$high.cholesterol, useNA = \"always\")\n#> \n#>   No  Yes <NA> \n#> 4153 1968 3133\n\n\n\n\n25.2.13 Survey features\n\n25.2.13.1 Weight\n\ndat2$survey.weight <- dat2$WTINT2YR\nsummary(dat2$survey.weight)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    2571   13074   21099   34671   36923  433085\ndat2$survey.weight.mec <- dat2$WTMEC2YR\nsummary(dat2$survey.weight.mec)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>       0   12347   21060   34671   37562  419763\n\n\n\n25.2.13.2 PSU\n\ndat2$psu <- as.factor(dat2$SDMVPSU)\ntable(dat2$psu)\n#> \n#>    1    2 \n#> 4464 4790\n\n\n\n25.2.13.3 Strata\n\ndat2$strata <- as.factor(dat2$SDMVSTRA)\ntable(dat2$strata)\n#> \n#> 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 \n#> 510 638 695 554 605 653 612 693 735 551 689 609 604 596 510\n\n\n\n\n25.2.14 Survey year\n\ndat2$year <- dat2$SDDSRVYR\ntable(dat2$year, useNA = \"always\") \n#> \n#>   10 <NA> \n#> 9254    0\n\n\n\n25.2.15 ICD-10-CM codes\n\ncolnames(rxq12) <- c(\"id\", \"icd10\")\ncolnames(rxq22) <- c(\"id\", \"icd10\")\ncolnames(rxq32) <- c(\"id\", \"icd10\")\n\nrx2017 <- rbind(rxq12, rxq22, rxq32)\nrx2017 <- rx2017[order(rx2017$id),]\n\nrx2017$icd10[rx2017$icd10 == \"Unknown\"] <- NA\nrx2017$icd10[rx2017$icd10 == \"Refused\"] <- NA\nrx2017$icd10[rx2017$icd10 == \"Don't know\"] <- NA\nrx2017$icd10[rx2017$icd10 == \"\"] <- NA\nrx2017$icd10.new <- substr(rx2017$icd10, start = 1, stop = 3)\n\nrx2017 <- na.omit(rx2017)"
  },
  {
    "objectID": "analytic17.html#analytic-data",
    "href": "analytic17.html#analytic-data",
    "title": "25  Recoding cycle 10",
    "section": "25.3 Analytic data",
    "text": "25.3 Analytic data\n\n25.3.1 Full dataset\n\nnhanes17r <- dat2\n\n\n\n25.3.2 Analytic datset - adults 20 years of more\n\nvars <- c(\n  # ID\n  \"id\",\n  \n  # Demographic\n  \"age\", \"age.cat\", \"sex\", \"education\", \"race\", \n  \"marital\", \"income\", \"born\", \"pregnancy\",\n  \n  # obesity\n  \"obese\", \n  \n  # Diabetes\n  \"diabetes\", \"diabetes.family.history\",\n  \n  # Smoking\n  \"smoking\", \n  \n  # Diet\n  \"diet.healthy\", \n\n  # Physical activity\n  \"physical.activity\", \n  \n  # Access to routine healthcare\n  \"medical.access\",\n  \n  # Blood pressure and Hypertension\n  \"systolicBP\", \"diastolicBP\", \n  \n  # Sleep \n  \"sleep\",\n\n  # Laboratory \n  \"uric.acid\", \"protein.total\", \"bilirubin.total\", \"phosphorus\",\n  \"sodium\", \"potassium\", \"globulin\", \"calcium.total\", \n  \"high.cholesterol\",\n  \n  # Survey features\n  \"survey.weight\", \"survey.weight.mec\", \"psu\", \"strata\", \n  \n  # Survey year\n  \"year\"\n)\n\nnhanes17r.sel <- nhanes17r[, vars]\n\n\n# Adults 20 years of more and not pregnant\ndim(nhanes17r.sel)\n#> [1] 9254   34\nanalytic17 <- subset(nhanes17r.sel, age >= 20 & \n                       pregnancy != 'yes')\ndim(analytic17)\n#> [1] 5569   34\n\n\n\n25.3.3 Save dataset for later use\n\ndim(analytic17)\n#> [1] 5569   34\ndim(rx2017)\n#> [1] 15025     3\nsave(analytic17, rx2017, file = \"data/analytic17recoded.RData\")"
  },
  {
    "objectID": "merge13to17.html#analytic-dataset",
    "href": "merge13to17.html#analytic-dataset",
    "title": "26  Merge three cycles",
    "section": "26.1 Analytic dataset",
    "text": "26.1 Analytic dataset\n\n26.1.1 Load 2013-18 datasets\n\nload(\"data/analytic13recoded.RData\")\nload(\"data/analytic15recoded.RData\")\nload(\"data/analytic17recoded.RData\")\n\n\n\n26.1.2 Merge 2013-18 datasets\n\n# adults aged 20 years or more\ndata.merged0 <- rbind(analytic13, analytic15, analytic17)\ndim(data.merged0)\n#> [1] 17057    34\ndata.merged <- droplevels(data.merged0)\n\n\n\n26.1.3 Check missingness\n\nplot_missing(data.merged)\n\n\n\n# profile_missing(data.merged)\ndim(data.merged)\n#> [1] 17057    34\n\n\n\nThe data contants variables with some missing information.\n\ndata.complete <- na.omit(data.merged)\ndim(data.complete)\n#> [1] 13505    34\n\n\n\n\nOnly complete cases retained, and survey features/weights were ignored for simplicity.\nIn a realistic analysis, we would consider the missingness pattern before deleting or imputing such information."
  },
  {
    "objectID": "merge13to17.html#summary-statistics",
    "href": "merge13to17.html#summary-statistics",
    "title": "26  Merge three cycles",
    "section": "26.2 Summary statistics",
    "text": "26.2 Summary statistics\n\n\n\n\n\n\n\nNo(N=8087)\nYes(N=5418)\nOverall(N=13505)\n\n\n\n\nage.cat\n\n\n\n\n\n20-49\n4076 (50.4%)\n2649 (48.9%)\n6725 (49.8%)\n\n\n50-64\n2048 (25.3%)\n1558 (28.8%)\n3606 (26.7%)\n\n\n65+\n1963 (24.3%)\n1211 (22.4%)\n3174 (23.5%)\n\n\nsex\n\n\n\n\n\nMale\n4125 (51.0%)\n2352 (43.4%)\n6477 (48.0%)\n\n\nFemale\n3962 (49.0%)\n3066 (56.6%)\n7028 (52.0%)\n\n\neducation\n\n\n\n\n\nLess than high school\n1609 (19.9%)\n1132 (20.9%)\n2741 (20.3%)\n\n\nHigh school\n4111 (50.8%)\n3237 (59.7%)\n7348 (54.4%)\n\n\nCollege graduate or above\n2367 (29.3%)\n1049 (19.4%)\n3416 (25.3%)\n\n\nrace\n\n\n\n\n\nWhite\n3159 (39.1%)\n2086 (38.5%)\n5245 (38.8%)\n\n\nBlack\n1420 (17.6%)\n1338 (24.7%)\n2758 (20.4%)\n\n\nHispanic\n1780 (22.0%)\n1540 (28.4%)\n3320 (24.6%)\n\n\nOthers\n1728 (21.4%)\n454 (8.4%)\n2182 (16.2%)\n\n\nmarital\n\n\n\n\n\nNever married\n1519 (18.8%)\n916 (16.9%)\n2435 (18.0%)\n\n\nMarried/with partner\n4924 (60.9%)\n3227 (59.6%)\n8151 (60.4%)\n\n\nOther\n1644 (20.3%)\n1275 (23.5%)\n2919 (21.6%)\n\n\nincome\n\n\n\n\n\nless than $20,000\n1524 (18.8%)\n1091 (20.1%)\n2615 (19.4%)\n\n\n$20,000 to $74,999\n4058 (50.2%)\n2966 (54.7%)\n7024 (52.0%)\n\n\n$75,000 and Over\n2505 (31.0%)\n1361 (25.1%)\n3866 (28.6%)\n\n\nborn\n\n\n\n\n\nBorn in US\n5247 (64.9%)\n4193 (77.4%)\n9440 (69.9%)\n\n\nOther place\n2840 (35.1%)\n1225 (22.6%)\n4065 (30.1%)\n\n\nyear\n\n\n\n\n\nMean (SD)\n8.93 (0.812)\n8.99 (0.809)\n8.95 (0.812)\n\n\nMedian [Min, Max]\n9.00 [8.00, 10.0]\n9.00 [8.00, 10.0]\n9.00 [8.00, 10.0]\n\n\ndiabetes.family.history\n\n\n\n\n\nNo\n6739 (83.3%)\n4087 (75.4%)\n10826 (80.2%)\n\n\nYes\n1348 (16.7%)\n1331 (24.6%)\n2679 (19.8%)\n\n\nsmoking\n\n\n\n\n\nNever smoker\n4654 (57.5%)\n3074 (56.7%)\n7728 (57.2%)\n\n\nPrevious smoker\n1795 (22.2%)\n1435 (26.5%)\n3230 (23.9%)\n\n\nCurrent smoker\n1638 (20.3%)\n909 (16.8%)\n2547 (18.9%)\n\n\ndiet.healthy\n\n\n\n\n\nPoor or fair\n1981 (24.5%)\n2261 (41.7%)\n4242 (31.4%)\n\n\nGood\n3346 (41.4%)\n2153 (39.7%)\n5499 (40.7%)\n\n\nVery good or excellent\n2760 (34.1%)\n1004 (18.5%)\n3764 (27.9%)\n\n\nphysical.activity\n\n\n\n\n\nNo\n6445 (79.7%)\n4167 (76.9%)\n10612 (78.6%)\n\n\nYes\n1642 (20.3%)\n1251 (23.1%)\n2893 (21.4%)\n\n\nmedical.access\n\n\n\n\n\nNo\n1546 (19.1%)\n762 (14.1%)\n2308 (17.1%)\n\n\nYes\n6541 (80.9%)\n4656 (85.9%)\n11197 (82.9%)\n\n\nsleep\n\n\n\n\n\nMean (SD)\n7.41 (1.53)\n7.32 (1.61)\n7.37 (1.56)\n\n\nMedian [Min, Max]\n7.50 [2.00, 14.5]\n7.00 [2.00, 14.0]\n7.50 [2.00, 14.5]\n\n\nsystolicBP\n\n\n\n\n\nMean (SD)\n123 (18.8)\n127 (17.3)\n125 (18.4)\n\n\nMedian [Min, Max]\n119 [64.7, 229]\n125 [74.0, 234]\n122 [64.7, 234]\n\n\ndiastolicBP\n\n\n\n\n\nMean (SD)\n70.0 (11.3)\n72.2 (11.8)\n70.9 (11.6)\n\n\nMedian [Min, Max]\n70.0 [8.00, 123]\n72.0 [26.0, 125]\n70.7 [8.00, 125]\n\n\nuric.acid\n\n\n\n\n\nMean (SD)\n5.20 (1.37)\n5.77 (1.49)\n5.43 (1.45)\n\n\nMedian [Min, Max]\n5.10 [0.700, 12.3]\n5.70 [1.60, 18.0]\n5.30 [0.700, 18.0]\n\n\nprotein.total\n\n\n\n\n\nMean (SD)\n7.14 (0.466)\n7.11 (0.449)\n7.13 (0.459)\n\n\nMedian [Min, Max]\n7.10 [4.70, 10.2]\n7.10 [5.20, 9.40]\n7.10 [4.70, 10.2]\n\n\nbilirubin.total\n\n\n\n\n\nMean (SD)\n0.581 (0.302)\n0.511 (0.288)\n0.553 (0.298)\n\n\nMedian [Min, Max]\n0.500 [0, 3.30]\n0.500 [0, 7.10]\n0.500 [0, 7.10]\n\n\nphosphorus\n\n\n\n\n\nMean (SD)\n3.72 (0.560)\n3.65 (0.575)\n3.69 (0.567)\n\n\nMedian [Min, Max]\n3.70 [1.00, 9.60]\n3.60 [1.70, 8.90]\n3.70 [1.00, 9.60]\n\n\nsodium\n\n\n\n\n\nMean (SD)\n140 (2.47)\n139 (2.52)\n140 (2.49)\n\n\nMedian [Min, Max]\n140 [124, 161]\n139 [119, 154]\n140 [119, 161]\n\n\npotassium\n\n\n\n\n\nMean (SD)\n4.00 (0.361)\n4.02 (0.356)\n4.01 (0.359)\n\n\nMedian [Min, Max]\n4.00 [2.63, 6.00]\n4.00 [2.60, 6.60]\n4.00 [2.60, 6.60]\n\n\nglobulin\n\n\n\n\n\nMean (SD)\n2.87 (0.449)\n3.00 (0.452)\n2.92 (0.455)\n\n\nMedian [Min, Max]\n2.80 [1.60, 6.50]\n3.00 [1.40, 5.70]\n2.90 [1.40, 6.50]\n\n\ncalcium.total\n\n\n\n\n\nMean (SD)\n9.39 (0.366)\n9.32 (0.370)\n9.36 (0.369)\n\n\nMedian [Min, Max]\n9.40 [6.40, 14.8]\n9.30 [6.60, 12.0]\n9.40 [6.40, 14.8]\n\n\nhigh.cholesterol\n\n\n\n\n\nNo\n5436 (67.2%)\n3271 (60.4%)\n8707 (64.5%)\n\n\nYes\n2651 (32.8%)\n2147 (39.6%)\n4798 (35.5%)\n\n\n\n\n\n\n\n\n\nInvestigator specified covariates stratified by the exposure (obesity)\nThis Table includes information about participants with and without ICD-10-CM proxy information. Therefore, the sample is is larger than the original analysis."
  },
  {
    "objectID": "merge13to17.html#proxy-data-from-icd10-codes",
    "href": "merge13to17.html#proxy-data-from-icd10-codes",
    "title": "26  Merge three cycles",
    "section": "26.3 Proxy data from ICD10 codes",
    "text": "26.3 Proxy data from ICD10 codes\n\ndat.proxy.long <- rbind(rx2013, rx2015, rx2017) \ndat.proxy.long$icd10 <- NULL\n# Rename 3 digits ICD-10 codes as icd10\ncolnames(dat.proxy.long)[names(dat.proxy.long)==\"icd10.new\"] <- \"icd10\"\n\n\n\nWe combine all of the ICD-10-CM information form all 3 cycles."
  },
  {
    "objectID": "merge13to17.html#save-dataset-for-later-use",
    "href": "merge13to17.html#save-dataset-for-later-use",
    "title": "26  Merge three cycles",
    "section": "26.4 Save dataset for later use",
    "text": "26.4 Save dataset for later use\n\nsave(data.merged, \n     data.complete, \n     dat.proxy.long, \n     file = \"data/analytic3cycles.RData\")"
  }
]